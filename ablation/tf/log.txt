[GEN] tf.raw_ops.MaxPoolGradGradWithArgmax -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input.
  grad: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the
    input of `max_pool`.
  argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The indices of the maximum values chosen for each output of `max_pool`.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input.
  grad: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the
    input of `max_pool`.
  argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The indices of the maximum values chosen for each output of `max_pool`.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPoolGradGradWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradWithArgmax -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPoolGradGradWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradWithArgmax -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input.
  grad: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the
    input of `max_pool`.
  argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The indices of the maximum values chosen for each output of `max_pool`.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input.
  grad: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the
    input of `max_pool`.
  argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The indices of the maximum values chosen for each output of `max_pool`.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPoolGradGradWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradWithArgmax -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPoolGradGradWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.BiasAdd -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            Adds `bias` to `value`.

This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Any number of dimensions.
  bias: A `Tensor`. Must have the same type as `value`.
    1-D with size the last dimension of `value`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `value`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BiasAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BiasAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            Adds `bias` to `value`.

This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Any number of dimensions.
  bias: A `Tensor`. Must have the same type as `value`.
    1-D with size the last dimension of `value`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `value`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BiasAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BiasAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.BiasAdd/fuzz.cpp
[GEN] tf.raw_ops.BiasAdd -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BiasAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BiasAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BiasAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BiasAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.BiasAdd/fuzz.cpp
[GEN] tf.raw_ops.BiasAdd -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            Adds `bias` to `value`.

This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Any number of dimensions.
  bias: A `Tensor`. Must have the same type as `value`.
    1-D with size the last dimension of `value`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `value`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            Adds `bias` to `value`.

This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Any number of dimensions.
  bias: A `Tensor`. Must have the same type as `value`.
    1-D with size the last dimension of `value`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `value`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.BiasAdd/fuzz.cpp
[GEN] tf.raw_ops.BiasAdd -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BiasAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BiasAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.BiasAdd/fuzz.cpp
[GEN] tf.raw_ops.SparseToDense -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            Converts a sparse representation into a dense tensor.

Builds an array `dense` with shape `output_shape` such that

```
# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
```

All other values in `dense` are set to `default_value`.  If `sparse_values` is a
scalar, all sparse indices are set to this single value.

Indices should be sorted in lexicographic order, and indices must not
contain any repeats. If `validate_indices` is true, these properties
are checked during execution.

Args:
  sparse_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
    index where `sparse_values[i]` will be placed.
  output_shape: A `Tensor`. Must have the same type as `sparse_indices`.
    1-D.  Shape of the dense output tensor.
  sparse_values: A `Tensor`.
    1-D.  Values corresponding to each row of `sparse_indices`,
    or a scalar value to be used for all sparse indices.
  default_value: A `Tensor`. Must have the same type as `sparse_values`.
    Scalar value to set for indices not specified in
    `sparse_indices`.
  validate_indices: An optional `bool`. Defaults to `True`.
    If true, indices are checked to make sure they are sorted in
    lexicographic order and that there are no repeats.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `sparse_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseToDense`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseToDense` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            Converts a sparse representation into a dense tensor.

Builds an array `dense` with shape `output_shape` such that

```
# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
```

All other values in `dense` are set to `default_value`.  If `sparse_values` is a
scalar, all sparse indices are set to this single value.

Indices should be sorted in lexicographic order, and indices must not
contain any repeats. If `validate_indices` is true, these properties
are checked during execution.

Args:
  sparse_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
    index where `sparse_values[i]` will be placed.
  output_shape: A `Tensor`. Must have the same type as `sparse_indices`.
    1-D.  Shape of the dense output tensor.
  sparse_values: A `Tensor`.
    1-D.  Values corresponding to each row of `sparse_indices`,
    or a scalar value to be used for all sparse indices.
  default_value: A `Tensor`. Must have the same type as `sparse_values`.
    Scalar value to set for indices not specified in
    `sparse_indices`.
  validate_indices: An optional `bool`. Defaults to `True`.
    If true, indices are checked to make sure they are sorted in
    lexicographic order and that there are no repeats.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `sparse_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseToDense`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseToDense` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseToDense/fuzz.cpp
[GEN] tf.raw_ops.SparseToDense -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseToDense`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseToDense` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseToDense`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseToDense` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseToDense/fuzz.cpp
[GEN] tf.raw_ops.SparseToDense -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            Converts a sparse representation into a dense tensor.

Builds an array `dense` with shape `output_shape` such that

```
# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
```

All other values in `dense` are set to `default_value`.  If `sparse_values` is a
scalar, all sparse indices are set to this single value.

Indices should be sorted in lexicographic order, and indices must not
contain any repeats. If `validate_indices` is true, these properties
are checked during execution.

Args:
  sparse_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
    index where `sparse_values[i]` will be placed.
  output_shape: A `Tensor`. Must have the same type as `sparse_indices`.
    1-D.  Shape of the dense output tensor.
  sparse_values: A `Tensor`.
    1-D.  Values corresponding to each row of `sparse_indices`,
    or a scalar value to be used for all sparse indices.
  default_value: A `Tensor`. Must have the same type as `sparse_values`.
    Scalar value to set for indices not specified in
    `sparse_indices`.
  validate_indices: An optional `bool`. Defaults to `True`.
    If true, indices are checked to make sure they are sorted in
    lexicographic order and that there are no repeats.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `sparse_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            Converts a sparse representation into a dense tensor.

Builds an array `dense` with shape `output_shape` such that

```
# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
```

All other values in `dense` are set to `default_value`.  If `sparse_values` is a
scalar, all sparse indices are set to this single value.

Indices should be sorted in lexicographic order, and indices must not
contain any repeats. If `validate_indices` is true, these properties
are checked during execution.

Args:
  sparse_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
    index where `sparse_values[i]` will be placed.
  output_shape: A `Tensor`. Must have the same type as `sparse_indices`.
    1-D.  Shape of the dense output tensor.
  sparse_values: A `Tensor`.
    1-D.  Values corresponding to each row of `sparse_indices`,
    or a scalar value to be used for all sparse indices.
  default_value: A `Tensor`. Must have the same type as `sparse_values`.
    Scalar value to set for indices not specified in
    `sparse_indices`.
  validate_indices: An optional `bool`. Defaults to `True`.
    If true, indices are checked to make sure they are sorted in
    lexicographic order and that there are no repeats.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `sparse_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseToDense/fuzz.cpp
[GEN] tf.raw_ops.SparseToDense -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseToDense`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseToDense functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseToDense/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNGradV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSqrtN.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSqrtN op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSqrtN op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSqrtN op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSqrtN op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSqrtN.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSqrtN op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSqrtN op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSqrtN op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSqrtN op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSegmentSqrtNGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNGradV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSegmentSqrtNGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNGradV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSqrtN.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSqrtN op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSqrtN op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSqrtN op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSqrtN op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSqrtN.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSqrtN op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSqrtN op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSqrtN op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSqrtN op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSegmentSqrtNGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNGradV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSegmentSqrtNGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSumGradV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSum.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSum op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSum op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSum op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSum op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSumGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSumGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSum.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSum op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSum op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSum op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSum op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSumGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSumGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSegmentSumGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSumGradV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSumGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSumGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSumGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSumGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSegmentSumGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSumGradV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSum.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSum op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSum op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSum op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSum op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            Computes gradients for SparseSegmentSum.

Returns tensor "output" with same shape as grad, except for dimension 0 whose
value is the number of unique indexes in "indices". Also returns vector
"sorted_unique_indices" containing the corresponding indexes from "indices".

Args:
  grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
    gradient propagated to the SparseSegmentSum op.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    indices passed to the corresponding SparseSegmentSum op.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    segment_ids passed to the corresponding SparseSegmentSum op.
  dense_output_dim0: A `Tensor` of type `int32`.
    dimension 0 of "data" passed to SparseSegmentSum op.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, sorted_unique_indices).

  output: A `Tensor`. Has the same type as `grad`.
  sorted_unique_indices: A `Tensor`. Has the same type as `indices`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSegmentSumGradV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSumGradV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSumGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSumGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSegmentSumGradV2/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMax -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            Apply a sparse update to a tensor taking the element-wise maximum.

Returns a new tensor copied from `tensor` whose values are element-wise maximum between
tensor and updates according to the indices.

>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]
>>> indices = [[1], [4], [5]]
>>> updates = [1, -1, 1]
>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)

Refer to `tf.tensor_scatter_nd_update` for more details.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            Apply a sparse update to a tensor taking the element-wise maximum.

Returns a new tensor copied from `tensor` whose values are element-wise maximum between
tensor and updates according to the indices.

>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]
>>> indices = [[1], [4], [5]]
>>> updates = [1, -1, 1]
>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)

Refer to `tf.tensor_scatter_nd_update` for more details.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.TensorScatterMax/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMax -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.TensorScatterMax/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMax -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            Apply a sparse update to a tensor taking the element-wise maximum.

Returns a new tensor copied from `tensor` whose values are element-wise maximum between
tensor and updates according to the indices.

>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]
>>> indices = [[1], [4], [5]]
>>> updates = [1, -1, 1]
>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)

Refer to `tf.tensor_scatter_nd_update` for more details.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            Apply a sparse update to a tensor taking the element-wise maximum.

Returns a new tensor copied from `tensor` whose values are element-wise maximum between
tensor and updates according to the indices.

>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]
>>> indices = [[1], [4], [5]]
>>> updates = [1, -1, 1]
>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)

Refer to `tf.tensor_scatter_nd_update` for more details.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.TensorScatterMax/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMax -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.TensorScatterMax/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMin -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.TensorScatterMin/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMin -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorScatterMin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorScatterMin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.TensorScatterMin/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMin -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  tensor: A `Tensor`. Tensor to update.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Index tensor.
  updates: A `Tensor`. Must have the same type as `tensor`.
    Updates to scatter into output.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `tensor`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.TensorScatterMin/fuzz.cpp
[GEN] tf.raw_ops.TensorScatterMin -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorScatterMin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorScatterMin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.TensorScatterMin/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyMomentum -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            Update relevant entries in '*var' and '*accum' according to the momentum scheme.

Set use_nesterov = True if you want to use Nesterov momentum.

That is for rows we have grad for, we update var and accum as follows:

accum = accum * momentum + grad
var -= lr * accum

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  momentum: A `Tensor`. Must have the same type as `lr`.
    Momentum. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  use_nesterov: An optional `bool`. Defaults to `False`.
    If `True`, the tensor passed to compute grad will be
    var - lr * momentum * accum, so in the end, the var you get is actually
    var - lr * momentum * accum.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyMomentum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyMomentum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            Update relevant entries in '*var' and '*accum' according to the momentum scheme.

Set use_nesterov = True if you want to use Nesterov momentum.

That is for rows we have grad for, we update var and accum as follows:

accum = accum * momentum + grad
var -= lr * accum

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  momentum: A `Tensor`. Must have the same type as `lr`.
    Momentum. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  use_nesterov: An optional `bool`. Defaults to `False`.
    If `True`, the tensor passed to compute grad will be
    var - lr * momentum * accum, so in the end, the var you get is actually
    var - lr * momentum * accum.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyMomentum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyMomentum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ResourceSparseApplyMomentum/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyMomentum -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyMomentum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyMomentum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyMomentum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyMomentum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ResourceSparseApplyMomentum/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyMomentum -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            Update relevant entries in '*var' and '*accum' according to the momentum scheme.

Set use_nesterov = True if you want to use Nesterov momentum.

That is for rows we have grad for, we update var and accum as follows:

accum = accum * momentum + grad
var -= lr * accum

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  momentum: A `Tensor`. Must have the same type as `lr`.
    Momentum. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  use_nesterov: An optional `bool`. Defaults to `False`.
    If `True`, the tensor passed to compute grad will be
    var - lr * momentum * accum, so in the end, the var you get is actually
    var - lr * momentum * accum.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            Update relevant entries in '*var' and '*accum' according to the momentum scheme.

Set use_nesterov = True if you want to use Nesterov momentum.

That is for rows we have grad for, we update var and accum as follows:

accum = accum * momentum + grad
var -= lr * accum

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  momentum: A `Tensor`. Must have the same type as `lr`.
    Momentum. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  use_nesterov: An optional `bool`. Defaults to `False`.
    If `True`, the tensor passed to compute grad will be
    var - lr * momentum * accum, so in the end, the var you get is actually
    var - lr * momentum * accum.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ResourceSparseApplyMomentum/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyMomentum -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyMomentum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyMomentum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ResourceSparseApplyMomentum/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyAdagradDA -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  gradient_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  gradient_squared_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  grad: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `grad`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `grad`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `grad`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  gradient_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  gradient_squared_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  grad: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `grad`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `grad`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `grad`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ResourceSparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyAdagradDA -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ResourceSparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyAdagradDA -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  gradient_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  gradient_squared_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  grad: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `grad`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `grad`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `grad`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  gradient_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  gradient_squared_accumulator: A `Tensor` of type `resource`.
    Should be from a Variable().
  grad: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `grad`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `grad`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `grad`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ResourceSparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyAdagradDA -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ResourceSparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.HistogramFixedWidth -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            Return histogram of values.

Given the tensor `values`, this operation returns a rank 1 histogram counting
the number of entries in `values` that fall into every bin.  The bins are
equal width and determined by the arguments `value_range` and `nbins`.

```python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

with tf.get_default_session() as sess:
  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
  variables.global_variables_initializer().run()
  sess.run(hist) => [2, 1, 1, 0, 2]
```

Args:
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    Numeric `Tensor`.
  value_range: A `Tensor`. Must have the same type as `values`.
    Shape [2] `Tensor` of same `dtype` as `values`.
    values <= value_range[0] will be mapped to hist[0],
    values >= value_range[1] will be mapped to hist[-1].
  nbins: A `Tensor` of type `int32`.
    Scalar `int32 Tensor`.  Number of histogram bins.
  dtype: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.HistogramFixedWidth`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.HistogramFixedWidth` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            Return histogram of values.

Given the tensor `values`, this operation returns a rank 1 histogram counting
the number of entries in `values` that fall into every bin.  The bins are
equal width and determined by the arguments `value_range` and `nbins`.

```python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

with tf.get_default_session() as sess:
  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
  variables.global_variables_initializer().run()
  sess.run(hist) => [2, 1, 1, 0, 2]
```

Args:
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    Numeric `Tensor`.
  value_range: A `Tensor`. Must have the same type as `values`.
    Shape [2] `Tensor` of same `dtype` as `values`.
    values <= value_range[0] will be mapped to hist[0],
    values >= value_range[1] will be mapped to hist[-1].
  nbins: A `Tensor` of type `int32`.
    Scalar `int32 Tensor`.  Number of histogram bins.
  dtype: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.HistogramFixedWidth`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.HistogramFixedWidth` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.HistogramFixedWidth/fuzz.cpp
[GEN] tf.raw_ops.HistogramFixedWidth -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.HistogramFixedWidth`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.HistogramFixedWidth` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.HistogramFixedWidth`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.HistogramFixedWidth` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.HistogramFixedWidth/fuzz.cpp
[GEN] tf.raw_ops.HistogramFixedWidth -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            Return histogram of values.

Given the tensor `values`, this operation returns a rank 1 histogram counting
the number of entries in `values` that fall into every bin.  The bins are
equal width and determined by the arguments `value_range` and `nbins`.

```python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

with tf.get_default_session() as sess:
  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
  variables.global_variables_initializer().run()
  sess.run(hist) => [2, 1, 1, 0, 2]
```

Args:
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    Numeric `Tensor`.
  value_range: A `Tensor`. Must have the same type as `values`.
    Shape [2] `Tensor` of same `dtype` as `values`.
    values <= value_range[0] will be mapped to hist[0],
    values >= value_range[1] will be mapped to hist[-1].
  nbins: A `Tensor` of type `int32`.
    Scalar `int32 Tensor`.  Number of histogram bins.
  dtype: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            Return histogram of values.

Given the tensor `values`, this operation returns a rank 1 histogram counting
the number of entries in `values` that fall into every bin.  The bins are
equal width and determined by the arguments `value_range` and `nbins`.

```python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

with tf.get_default_session() as sess:
  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
  variables.global_variables_initializer().run()
  sess.run(hist) => [2, 1, 1, 0, 2]
```

Args:
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    Numeric `Tensor`.
  value_range: A `Tensor`. Must have the same type as `values`.
    Shape [2] `Tensor` of same `dtype` as `values`.
    values <= value_range[0] will be mapped to hist[0],
    values >= value_range[1] will be mapped to hist[-1].
  nbins: A `Tensor` of type `int32`.
    Scalar `int32 Tensor`.  Number of histogram bins.
  dtype: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.HistogramFixedWidth/fuzz.cpp
[GEN] tf.raw_ops.HistogramFixedWidth -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.HistogramFixedWidth`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.HistogramFixedWidth functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.HistogramFixedWidth/fuzz.cpp
[GEN] tf.raw_ops.CropAndResizeGradBoxes -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            Computes the gradient of the crop_and_resize op wrt the input boxes tensor.

Args:
  grads: A `Tensor` of type `float32`.
    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`.
  image: A `Tensor`. Must be one of the following types: `uint8`, `uint16`, `int8`, `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
    A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
    Both `image_height` and `image_width` need to be positive.
  boxes: A `Tensor` of type `float32`.
    A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor
    specifies the coordinates of a box in the `box_ind[i]` image and is specified
    in normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value of
    `y` is mapped to the image coordinate at `y * (image_height - 1)`, so as the
    `[0, 1]` interval of normalized image height is mapped to
    `[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in
    which case the sampled crop is an up-down flipped version of the original
    image. The width dimension is treated similarly. Normalized coordinates
    outside the `[0, 1]` range are allowed, in which case we use
    `extrapolation_value` to extrapolate the input image values.
  box_ind: A `Tensor` of type `int32`.
    A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.
    The value of `box_ind[i]` specifies the image that the `i`-th box refers to.
  method: An optional `string` from: `"bilinear"`. Defaults to `"bilinear"`.
    A string specifying the interpolation method. Only 'bilinear' is
    supported for now.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CropAndResizeGradBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CropAndResizeGradBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            Computes the gradient of the crop_and_resize op wrt the input boxes tensor.

Args:
  grads: A `Tensor` of type `float32`.
    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`.
  image: A `Tensor`. Must be one of the following types: `uint8`, `uint16`, `int8`, `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
    A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
    Both `image_height` and `image_width` need to be positive.
  boxes: A `Tensor` of type `float32`.
    A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor
    specifies the coordinates of a box in the `box_ind[i]` image and is specified
    in normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value of
    `y` is mapped to the image coordinate at `y * (image_height - 1)`, so as the
    `[0, 1]` interval of normalized image height is mapped to
    `[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in
    which case the sampled crop is an up-down flipped version of the original
    image. The width dimension is treated similarly. Normalized coordinates
    outside the `[0, 1]` range are allowed, in which case we use
    `extrapolation_value` to extrapolate the input image values.
  box_ind: A `Tensor` of type `int32`.
    A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.
    The value of `box_ind[i]` specifies the image that the `i`-th box refers to.
  method: An optional `string` from: `"bilinear"`. Defaults to `"bilinear"`.
    A string specifying the interpolation method. Only 'bilinear' is
    supported for now.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CropAndResizeGradBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CropAndResizeGradBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.CropAndResizeGradBoxes/fuzz.cpp
[GEN] tf.raw_ops.CropAndResizeGradBoxes -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CropAndResizeGradBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CropAndResizeGradBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CropAndResizeGradBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CropAndResizeGradBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.CropAndResizeGradBoxes/fuzz.cpp
[GEN] tf.raw_ops.CropAndResizeGradBoxes -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            Computes the gradient of the crop_and_resize op wrt the input boxes tensor.

Args:
  grads: A `Tensor` of type `float32`.
    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`.
  image: A `Tensor`. Must be one of the following types: `uint8`, `uint16`, `int8`, `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
    A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
    Both `image_height` and `image_width` need to be positive.
  boxes: A `Tensor` of type `float32`.
    A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor
    specifies the coordinates of a box in the `box_ind[i]` image and is specified
    in normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value of
    `y` is mapped to the image coordinate at `y * (image_height - 1)`, so as the
    `[0, 1]` interval of normalized image height is mapped to
    `[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in
    which case the sampled crop is an up-down flipped version of the original
    image. The width dimension is treated similarly. Normalized coordinates
    outside the `[0, 1]` range are allowed, in which case we use
    `extrapolation_value` to extrapolate the input image values.
  box_ind: A `Tensor` of type `int32`.
    A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.
    The value of `box_ind[i]` specifies the image that the `i`-th box refers to.
  method: An optional `string` from: `"bilinear"`. Defaults to `"bilinear"`.
    A string specifying the interpolation method. Only 'bilinear' is
    supported for now.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            Computes the gradient of the crop_and_resize op wrt the input boxes tensor.

Args:
  grads: A `Tensor` of type `float32`.
    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`.
  image: A `Tensor`. Must be one of the following types: `uint8`, `uint16`, `int8`, `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
    A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
    Both `image_height` and `image_width` need to be positive.
  boxes: A `Tensor` of type `float32`.
    A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor
    specifies the coordinates of a box in the `box_ind[i]` image and is specified
    in normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value of
    `y` is mapped to the image coordinate at `y * (image_height - 1)`, so as the
    `[0, 1]` interval of normalized image height is mapped to
    `[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in
    which case the sampled crop is an up-down flipped version of the original
    image. The width dimension is treated similarly. Normalized coordinates
    outside the `[0, 1]` range are allowed, in which case we use
    `extrapolation_value` to extrapolate the input image values.
  box_ind: A `Tensor` of type `int32`.
    A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.
    The value of `box_ind[i]` specifies the image that the `i`-th box refers to.
  method: An optional `string` from: `"bilinear"`. Defaults to `"bilinear"`.
    A string specifying the interpolation method. Only 'bilinear' is
    supported for now.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.CropAndResizeGradBoxes/fuzz.cpp
[GEN] tf.raw_ops.CropAndResizeGradBoxes -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CropAndResizeGradBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CropAndResizeGradBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.CropAndResizeGradBoxes/fuzz.cpp
[GEN] tf.raw_ops.MaxPool3D -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            Performs 3D max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    Shape `[batch, depth, rows, cols, channels]` tensor to pool over.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool3D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool3D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            Performs 3D max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    Shape `[batch, depth, rows, cols, channels]` tensor to pool over.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool3D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool3D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPool3D/fuzz.cpp
[GEN] tf.raw_ops.MaxPool3D -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool3D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool3D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool3D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool3D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPool3D/fuzz.cpp
[GEN] tf.raw_ops.MaxPool3D -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            Performs 3D max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    Shape `[batch, depth, rows, cols, channels]` tensor to pool over.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            Performs 3D max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    Shape `[batch, depth, rows, cols, channels]` tensor to pool over.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPool3D/fuzz.cpp
[GEN] tf.raw_ops.MaxPool3D -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool3D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool3D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPool3D/fuzz.cpp
[GEN] tf.raw_ops.AvgPool3DGrad -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            Computes gradients of average pooling function.

Args:
  orig_input_shape: A `Tensor` of type `int32`.
    The original input dimensions.
  grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Output backprop of shape `[batch, depth, rows, cols, channels]`.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `grad`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AvgPool3DGrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AvgPool3DGrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            Computes gradients of average pooling function.

Args:
  orig_input_shape: A `Tensor` of type `int32`.
    The original input dimensions.
  grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Output backprop of shape `[batch, depth, rows, cols, channels]`.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `grad`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AvgPool3DGrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AvgPool3DGrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.AvgPool3DGrad/fuzz.cpp
[GEN] tf.raw_ops.AvgPool3DGrad -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AvgPool3DGrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AvgPool3DGrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AvgPool3DGrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AvgPool3DGrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.AvgPool3DGrad/fuzz.cpp
[GEN] tf.raw_ops.AvgPool3DGrad -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            Computes gradients of average pooling function.

Args:
  orig_input_shape: A `Tensor` of type `int32`.
    The original input dimensions.
  grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Output backprop of shape `[batch, depth, rows, cols, channels]`.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `grad`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            Computes gradients of average pooling function.

Args:
  orig_input_shape: A `Tensor` of type `int32`.
    The original input dimensions.
  grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Output backprop of shape `[batch, depth, rows, cols, channels]`.
  ksize: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The size of the window for each dimension of
    the input tensor. Must have `ksize[0] = ksize[4] = 1`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `grad`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.AvgPool3DGrad/fuzz.cpp
[GEN] tf.raw_ops.AvgPool3DGrad -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AvgPool3DGrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AvgPool3DGrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.AvgPool3DGrad/fuzz.cpp
[GEN] tf.raw_ops.SparseBincount -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            Counts the number of occurrences of each value in an integer array.

Outputs a vector with length `size` and the same dtype as `weights`. If
`weights` are empty, then index `i` stores the number of times the value `i` is
counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of
the value in `weights` at each index where the corresponding value in `arr` is
`i`.

Values in `arr` outside of the range [0, size) are ignored.

Args:
  indices: A `Tensor` of type `int64`. 2D int64 `Tensor`.
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1D int `Tensor`.
  dense_shape: A `Tensor` of type `int64`. 1D int64 `Tensor`.
  size: A `Tensor`. Must have the same type as `values`.
    non-negative int scalar `Tensor`.
  weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    is an int32, int64, float32, or float64 `Tensor` with the same
    shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights
    equal to 1.
  binary_output: An optional `bool`. Defaults to `False`.
    bool; Whether the kernel should count the appearance or number of occurrences.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `weights`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseBincount`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseBincount` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            Counts the number of occurrences of each value in an integer array.

Outputs a vector with length `size` and the same dtype as `weights`. If
`weights` are empty, then index `i` stores the number of times the value `i` is
counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of
the value in `weights` at each index where the corresponding value in `arr` is
`i`.

Values in `arr` outside of the range [0, size) are ignored.

Args:
  indices: A `Tensor` of type `int64`. 2D int64 `Tensor`.
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1D int `Tensor`.
  dense_shape: A `Tensor` of type `int64`. 1D int64 `Tensor`.
  size: A `Tensor`. Must have the same type as `values`.
    non-negative int scalar `Tensor`.
  weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    is an int32, int64, float32, or float64 `Tensor` with the same
    shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights
    equal to 1.
  binary_output: An optional `bool`. Defaults to `False`.
    bool; Whether the kernel should count the appearance or number of occurrences.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `weights`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseBincount`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseBincount` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseBincount/fuzz.cpp
[GEN] tf.raw_ops.SparseBincount -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseBincount`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseBincount` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseBincount`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseBincount` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseBincount/fuzz.cpp
[GEN] tf.raw_ops.SparseBincount -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            Counts the number of occurrences of each value in an integer array.

Outputs a vector with length `size` and the same dtype as `weights`. If
`weights` are empty, then index `i` stores the number of times the value `i` is
counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of
the value in `weights` at each index where the corresponding value in `arr` is
`i`.

Values in `arr` outside of the range [0, size) are ignored.

Args:
  indices: A `Tensor` of type `int64`. 2D int64 `Tensor`.
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1D int `Tensor`.
  dense_shape: A `Tensor` of type `int64`. 1D int64 `Tensor`.
  size: A `Tensor`. Must have the same type as `values`.
    non-negative int scalar `Tensor`.
  weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    is an int32, int64, float32, or float64 `Tensor` with the same
    shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights
    equal to 1.
  binary_output: An optional `bool`. Defaults to `False`.
    bool; Whether the kernel should count the appearance or number of occurrences.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `weights`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            Counts the number of occurrences of each value in an integer array.

Outputs a vector with length `size` and the same dtype as `weights`. If
`weights` are empty, then index `i` stores the number of times the value `i` is
counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of
the value in `weights` at each index where the corresponding value in `arr` is
`i`.

Values in `arr` outside of the range [0, size) are ignored.

Args:
  indices: A `Tensor` of type `int64`. 2D int64 `Tensor`.
  values: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1D int `Tensor`.
  dense_shape: A `Tensor` of type `int64`. 1D int64 `Tensor`.
  size: A `Tensor`. Must have the same type as `values`.
    non-negative int scalar `Tensor`.
  weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.
    is an int32, int64, float32, or float64 `Tensor` with the same
    shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights
    equal to 1.
  binary_output: An optional `bool`. Defaults to `False`.
    bool; Whether the kernel should count the appearance or number of occurrences.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `weights`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseBincount/fuzz.cpp
[GEN] tf.raw_ops.SparseBincount -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseBincount`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseBincount functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseBincount/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyProximalAdagrad -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.

That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
prox_v = var
prox_v -= lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `lr`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `lr`.
    L2 regularization. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyProximalAdagrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyProximalAdagrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.

That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
prox_v = var
prox_v -= lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `lr`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `lr`.
    L2 regularization. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyProximalAdagrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyProximalAdagrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ResourceSparseApplyProximalAdagrad/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyProximalAdagrad -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyProximalAdagrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyProximalAdagrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ResourceSparseApplyProximalAdagrad`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ResourceSparseApplyProximalAdagrad` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ResourceSparseApplyProximalAdagrad/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyProximalAdagrad -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.

That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
prox_v = var
prox_v -= lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `lr`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `lr`.
    L2 regularization. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.

That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
prox_v = var
prox_v -= lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}

Args:
  var: A `Tensor` of type `resource`. Should be from a Variable().
  accum: A `Tensor` of type `resource`. Should be from a Variable().
  lr: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `lr`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `lr`.
    L2 regularization. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `lr`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ResourceSparseApplyProximalAdagrad/fuzz.cpp
[GEN] tf.raw_ops.ResourceSparseApplyProximalAdagrad -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ResourceSparseApplyProximalAdagrad`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ResourceSparseApplyProximalAdagrad functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ResourceSparseApplyProximalAdagrad/fuzz.cpp
[GEN] tf.raw_ops.QuantizeAndDequantizeV3 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            Quantizes then dequantizes a tensor.

This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a
tensor, so its value can change during training.

Args:
  input: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  input_min: A `Tensor`. Must have the same type as `input`.
  input_max: A `Tensor`. Must have the same type as `input`.
  num_bits: A `Tensor` of type `int32`.
  signed_input: An optional `bool`. Defaults to `True`.
  range_given: An optional `bool`. Defaults to `True`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeAndDequantizeV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeAndDequantizeV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            Quantizes then dequantizes a tensor.

This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a
tensor, so its value can change during training.

Args:
  input: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  input_min: A `Tensor`. Must have the same type as `input`.
  input_max: A `Tensor`. Must have the same type as `input`.
  num_bits: A `Tensor` of type `int32`.
  signed_input: An optional `bool`. Defaults to `True`.
  range_given: An optional `bool`. Defaults to `True`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeAndDequantizeV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeAndDequantizeV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.QuantizeAndDequantizeV3/fuzz.cpp
[GEN] tf.raw_ops.QuantizeAndDequantizeV3 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeAndDequantizeV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeAndDequantizeV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeAndDequantizeV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeAndDequantizeV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.QuantizeAndDequantizeV3/fuzz.cpp
[GEN] tf.raw_ops.QuantizeAndDequantizeV3 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            Quantizes then dequantizes a tensor.

This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a
tensor, so its value can change during training.

Args:
  input: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  input_min: A `Tensor`. Must have the same type as `input`.
  input_max: A `Tensor`. Must have the same type as `input`.
  num_bits: A `Tensor` of type `int32`.
  signed_input: An optional `bool`. Defaults to `True`.
  range_given: An optional `bool`. Defaults to `True`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            Quantizes then dequantizes a tensor.

This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a
tensor, so its value can change during training.

Args:
  input: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  input_min: A `Tensor`. Must have the same type as `input`.
  input_max: A `Tensor`. Must have the same type as `input`.
  num_bits: A `Tensor` of type `int32`.
  signed_input: An optional `bool`. Defaults to `True`.
  range_given: An optional `bool`. Defaults to `True`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.QuantizeAndDequantizeV3/fuzz.cpp
[GEN] tf.raw_ops.QuantizeAndDequantizeV3 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeAndDequantizeV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeAndDequantizeV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.QuantizeAndDequantizeV3/fuzz.cpp
[GEN] tf.raw_ops.MaxPool -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPool/fuzz.cpp
[GEN] tf.raw_ops.MaxPool -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPool/fuzz.cpp
[GEN] tf.raw_ops.MaxPool -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPool/fuzz.cpp
[GEN] tf.raw_ops.MaxPool -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPool/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInputV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input_sizes: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    An integer vector representing the tensor shape of `input`,
    where `input` is a 5-D
    `[batch, depth, rows, cols, in_channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInputV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInputV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input_sizes: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    An integer vector representing the tensor shape of `input`,
    where `input` is a 5-D
    `[batch, depth, rows, cols, in_channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInputV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInputV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Conv3DBackpropInputV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInputV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInputV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInputV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInputV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInputV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Conv3DBackpropInputV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInputV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input_sizes: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    An integer vector representing the tensor shape of `input`,
    where `input` is a 5-D
    `[batch, depth, rows, cols, in_channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input_sizes: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    An integer vector representing the tensor shape of `input`,
    where `input` is a 5-D
    `[batch, depth, rows, cols, in_channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Conv3DBackpropInputV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInputV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInputV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInputV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Conv3DBackpropInputV2/fuzz.cpp
[GEN] tf.raw_ops.Mfcc -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            Transforms a spectrogram into a form that's useful for speech recognition.

Mel Frequency Cepstral Coefficients are a way of representing audio data that's
been effective as an input feature for machine learning. They are created by
taking the spectrum of a spectrogram (a 'cepstrum'), and discarding some of the
higher frequencies that are less significant to the human ear. They have a long
history in the speech recognition world, and https://en.wikipedia.org/wiki/Mel-frequency_cepstrum
is a good resource to learn more.

Args:
  spectrogram: A `Tensor` of type `float32`.
    Typically produced by the Spectrogram op, with magnitude_squared
    set to true.
  sample_rate: A `Tensor` of type `int32`.
    How many samples per second the source audio used.
  upper_frequency_limit: An optional `float`. Defaults to `4000`.
    The highest frequency to use when calculating the
    ceptstrum.
  lower_frequency_limit: An optional `float`. Defaults to `20`.
    The lowest frequency to use when calculating the
    ceptstrum.
  filterbank_channel_count: An optional `int`. Defaults to `40`.
    Resolution of the Mel bank used internally.
  dct_coefficient_count: An optional `int`. Defaults to `13`.
    How many output channels to produce per time slice.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Mfcc`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Mfcc` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            Transforms a spectrogram into a form that's useful for speech recognition.

Mel Frequency Cepstral Coefficients are a way of representing audio data that's
been effective as an input feature for machine learning. They are created by
taking the spectrum of a spectrogram (a 'cepstrum'), and discarding some of the
higher frequencies that are less significant to the human ear. They have a long
history in the speech recognition world, and https://en.wikipedia.org/wiki/Mel-frequency_cepstrum
is a good resource to learn more.

Args:
  spectrogram: A `Tensor` of type `float32`.
    Typically produced by the Spectrogram op, with magnitude_squared
    set to true.
  sample_rate: A `Tensor` of type `int32`.
    How many samples per second the source audio used.
  upper_frequency_limit: An optional `float`. Defaults to `4000`.
    The highest frequency to use when calculating the
    ceptstrum.
  lower_frequency_limit: An optional `float`. Defaults to `20`.
    The lowest frequency to use when calculating the
    ceptstrum.
  filterbank_channel_count: An optional `int`. Defaults to `40`.
    Resolution of the Mel bank used internally.
  dct_coefficient_count: An optional `int`. Defaults to `13`.
    How many output channels to produce per time slice.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Mfcc`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Mfcc` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Mfcc/fuzz.cpp
[GEN] tf.raw_ops.Mfcc -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Mfcc`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Mfcc` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Mfcc`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Mfcc` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Mfcc/fuzz.cpp
[GEN] tf.raw_ops.Mfcc -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            Transforms a spectrogram into a form that's useful for speech recognition.

Mel Frequency Cepstral Coefficients are a way of representing audio data that's
been effective as an input feature for machine learning. They are created by
taking the spectrum of a spectrogram (a 'cepstrum'), and discarding some of the
higher frequencies that are less significant to the human ear. They have a long
history in the speech recognition world, and https://en.wikipedia.org/wiki/Mel-frequency_cepstrum
is a good resource to learn more.

Args:
  spectrogram: A `Tensor` of type `float32`.
    Typically produced by the Spectrogram op, with magnitude_squared
    set to true.
  sample_rate: A `Tensor` of type `int32`.
    How many samples per second the source audio used.
  upper_frequency_limit: An optional `float`. Defaults to `4000`.
    The highest frequency to use when calculating the
    ceptstrum.
  lower_frequency_limit: An optional `float`. Defaults to `20`.
    The lowest frequency to use when calculating the
    ceptstrum.
  filterbank_channel_count: An optional `int`. Defaults to `40`.
    Resolution of the Mel bank used internally.
  dct_coefficient_count: An optional `int`. Defaults to `13`.
    How many output channels to produce per time slice.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            Transforms a spectrogram into a form that's useful for speech recognition.

Mel Frequency Cepstral Coefficients are a way of representing audio data that's
been effective as an input feature for machine learning. They are created by
taking the spectrum of a spectrogram (a 'cepstrum'), and discarding some of the
higher frequencies that are less significant to the human ear. They have a long
history in the speech recognition world, and https://en.wikipedia.org/wiki/Mel-frequency_cepstrum
is a good resource to learn more.

Args:
  spectrogram: A `Tensor` of type `float32`.
    Typically produced by the Spectrogram op, with magnitude_squared
    set to true.
  sample_rate: A `Tensor` of type `int32`.
    How many samples per second the source audio used.
  upper_frequency_limit: An optional `float`. Defaults to `4000`.
    The highest frequency to use when calculating the
    ceptstrum.
  lower_frequency_limit: An optional `float`. Defaults to `20`.
    The lowest frequency to use when calculating the
    ceptstrum.
  filterbank_channel_count: An optional `int`. Defaults to `40`.
    Resolution of the Mel bank used internally.
  dct_coefficient_count: An optional `int`. Defaults to `13`.
    How many output channels to produce per time slice.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Mfcc/fuzz.cpp
[GEN] tf.raw_ops.Mfcc -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Mfcc`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Mfcc functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Mfcc/fuzz.cpp
[GEN] tf.raw_ops.QuantizedConv2D -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            Computes a 2D convolution given quantized 4D input and filter tensors.

The inputs are quantized tensors where the lowest value represents the real
number of the associated minimum, and the highest represents the maximum.
This means that you can only interpret the quantized output in the same way, by
taking the returned minimum and maximum values into account.

Args:
  input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  filter: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
    filter's input_depth dimension must match input's depth dimensions.
  min_input: A `Tensor` of type `float32`.
    The float value that the lowest quantized input value represents.
  max_input: A `Tensor` of type `float32`.
    The float value that the highest quantized input value represents.
  min_filter: A `Tensor` of type `float32`.
    The float value that the lowest quantized filter value represents.
  max_filter: A `Tensor` of type `float32`.
    The float value that the highest quantized filter value represents.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, min_output, max_output).

  output: A `Tensor` of type `out_type`.
  min_output: A `Tensor` of type `float32`.
  max_output: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            Computes a 2D convolution given quantized 4D input and filter tensors.

The inputs are quantized tensors where the lowest value represents the real
number of the associated minimum, and the highest represents the maximum.
This means that you can only interpret the quantized output in the same way, by
taking the returned minimum and maximum values into account.

Args:
  input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  filter: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
    filter's input_depth dimension must match input's depth dimensions.
  min_input: A `Tensor` of type `float32`.
    The float value that the lowest quantized input value represents.
  max_input: A `Tensor` of type `float32`.
    The float value that the highest quantized input value represents.
  min_filter: A `Tensor` of type `float32`.
    The float value that the lowest quantized filter value represents.
  max_filter: A `Tensor` of type `float32`.
    The float value that the highest quantized filter value represents.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, min_output, max_output).

  output: A `Tensor` of type `out_type`.
  min_output: A `Tensor` of type `float32`.
  max_output: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.QuantizedConv2D/fuzz.cpp
[GEN] tf.raw_ops.QuantizedConv2D -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.QuantizedConv2D/fuzz.cpp
[GEN] tf.raw_ops.QuantizedConv2D -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            Computes a 2D convolution given quantized 4D input and filter tensors.

The inputs are quantized tensors where the lowest value represents the real
number of the associated minimum, and the highest represents the maximum.
This means that you can only interpret the quantized output in the same way, by
taking the returned minimum and maximum values into account.

Args:
  input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  filter: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
    filter's input_depth dimension must match input's depth dimensions.
  min_input: A `Tensor` of type `float32`.
    The float value that the lowest quantized input value represents.
  max_input: A `Tensor` of type `float32`.
    The float value that the highest quantized input value represents.
  min_filter: A `Tensor` of type `float32`.
    The float value that the lowest quantized filter value represents.
  max_filter: A `Tensor` of type `float32`.
    The float value that the highest quantized filter value represents.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, min_output, max_output).

  output: A `Tensor` of type `out_type`.
  min_output: A `Tensor` of type `float32`.
  max_output: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            Computes a 2D convolution given quantized 4D input and filter tensors.

The inputs are quantized tensors where the lowest value represents the real
number of the associated minimum, and the highest represents the maximum.
This means that you can only interpret the quantized output in the same way, by
taking the returned minimum and maximum values into account.

Args:
  input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  filter: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
    filter's input_depth dimension must match input's depth dimensions.
  min_input: A `Tensor` of type `float32`.
    The float value that the lowest quantized input value represents.
  max_input: A `Tensor` of type `float32`.
    The float value that the highest quantized input value represents.
  min_filter: A `Tensor` of type `float32`.
    The float value that the lowest quantized filter value represents.
  max_filter: A `Tensor` of type `float32`.
    The float value that the highest quantized filter value represents.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, min_output, max_output).

  output: A `Tensor` of type `out_type`.
  min_output: A `Tensor` of type `float32`.
  max_output: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.QuantizedConv2D/fuzz.cpp
[GEN] tf.raw_ops.QuantizedConv2D -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.QuantizedConv2D/fuzz.cpp
[GEN] tf.conv2d_backprop_filter_v2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[filter_height, filter_width, in_channels, out_channels]`.
    Only shape of tensor is used.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.conv2d_backprop_filter_v2`. 
    3.  **Operation Application:** Apply the `tf.conv2d_backprop_filter_v2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[filter_height, filter_width, in_channels, out_channels]`.
    Only shape of tensor is used.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.conv2d_backprop_filter_v2`. 
    3.  **Operation Application:** Apply the `tf.conv2d_backprop_filter_v2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.conv2d_backprop_filter_v2/fuzz.cpp
[GEN] tf.conv2d_backprop_filter_v2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.conv2d_backprop_filter_v2`. 
    3.  **Operation Application:** Apply the `tf.conv2d_backprop_filter_v2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.conv2d_backprop_filter_v2`. 
    3.  **Operation Application:** Apply the `tf.conv2d_backprop_filter_v2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.conv2d_backprop_filter_v2/fuzz.cpp
[GEN] tf.conv2d_backprop_filter_v2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[filter_height, filter_width, in_channels, out_channels]`.
    Only shape of tensor is used.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[filter_height, filter_width, in_channels, out_channels]`.
    Only shape of tensor is used.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.conv2d_backprop_filter_v2/fuzz.cpp
[GEN] tf.conv2d_backprop_filter_v2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.conv2d_backprop_filter_v2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.conv2d_backprop_filter_v2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.conv2d_backprop_filter_v2/fuzz.cpp
[GEN] tf.raw_ops.Conv2DBackpropFilter -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, out_channels]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv2DBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv2DBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, out_channels]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv2DBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv2DBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Conv2DBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.Conv2DBackpropFilter -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv2DBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv2DBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv2DBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv2DBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Conv2DBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.Conv2DBackpropFilter -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, out_channels]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, out_channels]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
    If `padding` is `"EXPLICIT"`, the list of explicit padding amounts. For the ith
    dimension, the amount of padding inserted before and after the dimension is
    `explicit_paddings[2 * i]` and `explicit_paddings[2 * i + 1]`, respectively. If
    `padding` is not `"EXPLICIT"`, `explicit_paddings` must be empty.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Conv2DBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.Conv2DBackpropFilter -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv2DBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv2DBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Conv2DBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropFilterV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 5-D
    `[filter_depth, filter_height, filter_width, in_channels, out_channels]`
    tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropFilterV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropFilterV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 5-D
    `[filter_depth, filter_height, filter_width, in_channels, out_channels]`
    tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropFilterV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropFilterV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Conv3DBackpropFilterV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropFilterV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropFilterV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropFilterV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropFilterV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropFilterV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Conv3DBackpropFilterV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropFilterV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 5-D
    `[filter_depth, filter_height, filter_width, in_channels, out_channels]`
    tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 5-D
    `[filter_depth, filter_height, filter_width, in_channels, out_channels]`
    tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NDHWC", "NCDHW"`. Defaults to `"NDHWC"`.
    The data format of the input and output data. With the
    default format "NDHWC", the data is stored in the order of:
        [batch, in_depth, in_height, in_width, in_channels].
    Alternatively, the format could be "NCDHW", the data storage order is:
        [batch, in_channels, in_depth, in_height, in_width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
    1-D tensor of length 5.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each
    filter element on that dimension. The dimension order is determined by the
    value of `data_format`, see above for details. Dilations in the batch and
    depth dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Conv3DBackpropFilterV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropFilterV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropFilterV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropFilterV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Conv3DBackpropFilterV2/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInput -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Conv3DBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInput -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Conv3DBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Conv3DBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Conv3DBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInput -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of 3-D convolution with respect to the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    Shape `[batch, depth, rows, cols, in_channels]`.
  filter: A `Tensor`. Must have the same type as `input`.
    Shape `[depth, rows, cols, in_channels, out_channels]`.
    `in_channels` must match between `input` and `filter`.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    Backprop signal of shape `[batch, out_depth, out_rows, out_cols,
    out_channels]`.
  strides: A list of `ints` that has length `>= 5`.
    1-D tensor of length 5. The stride of the sliding window for each
    dimension of `input`. Must have `strides[0] = strides[4] = 1`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Conv3DBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.Conv3DBackpropInput -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Conv3DBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Conv3DBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Conv3DBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input tensor.
  orig_output: A `Tensor`. Must have the same type as `orig_input`.
    The original output tensor.
  grad: A `Tensor`. Must have the same type as `orig_input`.
    4-D.  Gradients of gradients w.r.t. the input of `max_pool`.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `orig_input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input tensor.
  orig_output: A `Tensor`. Must have the same type as `orig_input`.
    The original output tensor.
  grad: A `Tensor`. Must have the same type as `orig_input`.
    4-D.  Gradients of gradients w.r.t. the input of `max_pool`.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `orig_input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPoolGradGradV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolGradGradV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolGradGradV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPoolGradGradV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input tensor.
  orig_output: A `Tensor`. Must have the same type as `orig_input`.
    The original output tensor.
  grad: A `Tensor`. Must have the same type as `orig_input`.
    4-D.  Gradients of gradients w.r.t. the input of `max_pool`.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `orig_input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            Computes second-order gradients of the maxpooling function.

Args:
  orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    The original input tensor.
  orig_output: A `Tensor`. Must have the same type as `orig_input`.
    The original output tensor.
  grad: A `Tensor`. Must have the same type as `orig_input`.
    4-D.  Gradients of gradients w.r.t. the input of `max_pool`.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `orig_input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPoolGradGradV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolGradGradV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolGradGradV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolGradGradV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPoolGradGradV2/fuzz.cpp
[GEN] tf.raw_ops.Exp -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            Computes exponential of x element-wise.  \\(y = e^x\\).

  This function computes the exponential of every element in the input tensor.
  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.
  `e` denotes Euler's number and is approximately equal to 2.718281.
  Output is positive for any real input.

  ```python
  x = tf.constant(2.0)
  tf.math.exp(x) ==> 7.389056

  x = tf.constant([2.0, 8.0])
  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)
  ```

  For complex numbers, the exponential value is calculated as follows:

  ```
  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)
  ```

  Let's consider complex number 1+1j as an example.
  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)

  ```python
  x = tf.constant(1 + 1j)
  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Exp`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Exp` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            Computes exponential of x element-wise.  \\(y = e^x\\).

  This function computes the exponential of every element in the input tensor.
  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.
  `e` denotes Euler's number and is approximately equal to 2.718281.
  Output is positive for any real input.

  ```python
  x = tf.constant(2.0)
  tf.math.exp(x) ==> 7.389056

  x = tf.constant([2.0, 8.0])
  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)
  ```

  For complex numbers, the exponential value is calculated as follows:

  ```
  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)
  ```

  Let's consider complex number 1+1j as an example.
  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)

  ```python
  x = tf.constant(1 + 1j)
  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Exp`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Exp` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Exp/fuzz.cpp
[GEN] tf.raw_ops.Exp -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Exp`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Exp` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Exp`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Exp` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Exp/fuzz.cpp
[GEN] tf.raw_ops.Exp -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            Computes exponential of x element-wise.  \\(y = e^x\\).

  This function computes the exponential of every element in the input tensor.
  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.
  `e` denotes Euler's number and is approximately equal to 2.718281.
  Output is positive for any real input.

  ```python
  x = tf.constant(2.0)
  tf.math.exp(x) ==> 7.389056

  x = tf.constant([2.0, 8.0])
  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)
  ```

  For complex numbers, the exponential value is calculated as follows:

  ```
  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)
  ```

  Let's consider complex number 1+1j as an example.
  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)

  ```python
  x = tf.constant(1 + 1j)
  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            Computes exponential of x element-wise.  \\(y = e^x\\).

  This function computes the exponential of every element in the input tensor.
  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.
  `e` denotes Euler's number and is approximately equal to 2.718281.
  Output is positive for any real input.

  ```python
  x = tf.constant(2.0)
  tf.math.exp(x) ==> 7.389056

  x = tf.constant([2.0, 8.0])
  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)
  ```

  For complex numbers, the exponential value is calculated as follows:

  ```
  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)
  ```

  Let's consider complex number 1+1j as an example.
  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)

  ```python
  x = tf.constant(1 + 1j)
  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Exp/fuzz.cpp
[GEN] tf.raw_ops.Exp -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Exp`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Exp functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Exp/fuzz.cpp
[GEN] tf.raw_ops.RandomGamma -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            Outputs random values from the Gamma distribution(s) described by alpha.

This op uses the algorithm by Marsaglia et al. to acquire samples via
transformation-rejection from pairs of uniform and normal random variables.
See http://dl.acm.org/citation.cfm?id=358414

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in alpha.
  alpha: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    A tensor in which each scalar is a "shape" parameter describing the
    associated gamma distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `alpha`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomGamma`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomGamma` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            Outputs random values from the Gamma distribution(s) described by alpha.

This op uses the algorithm by Marsaglia et al. to acquire samples via
transformation-rejection from pairs of uniform and normal random variables.
See http://dl.acm.org/citation.cfm?id=358414

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in alpha.
  alpha: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    A tensor in which each scalar is a "shape" parameter describing the
    associated gamma distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `alpha`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomGamma`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomGamma` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.RandomGamma/fuzz.cpp
[GEN] tf.raw_ops.RandomGamma -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomGamma`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomGamma` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomGamma`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomGamma` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.RandomGamma/fuzz.cpp
[GEN] tf.raw_ops.RandomGamma -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            Outputs random values from the Gamma distribution(s) described by alpha.

This op uses the algorithm by Marsaglia et al. to acquire samples via
transformation-rejection from pairs of uniform and normal random variables.
See http://dl.acm.org/citation.cfm?id=358414

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in alpha.
  alpha: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    A tensor in which each scalar is a "shape" parameter describing the
    associated gamma distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `alpha`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            Outputs random values from the Gamma distribution(s) described by alpha.

This op uses the algorithm by Marsaglia et al. to acquire samples via
transformation-rejection from pairs of uniform and normal random variables.
See http://dl.acm.org/citation.cfm?id=358414

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in alpha.
  alpha: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    A tensor in which each scalar is a "shape" parameter describing the
    associated gamma distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `alpha`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.RandomGamma/fuzz.cpp
[GEN] tf.raw_ops.RandomGamma -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomGamma`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomGamma functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.RandomGamma/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolWithArgmax -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            Performs max pooling on the input and outputs both max values and indices.

The indices in `argmax` are flattened, so that a maximum value at position
`[b, y, x, c]` becomes flattened index:
`(y * width + x) * channels + c` if `include_batch_in_index` is False;
`((b * height + y) * width + x) * channels + c` if `include_batch_in_index` is True.

The indices returned are always in `[0, height) x [0, width)` before flattening,
even if padding is involved and the mathematically correct answer is outside
(either negative or too large).  This is a bug, but fixing it is difficult to do
in a safe backwards compatible way, especially due to flattening.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, height, width, channels]`.  Input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  Targmax: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, argmax).

  output: A `Tensor`. Has the same type as `input`.
  argmax: A `Tensor` of type `Targmax`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            Performs max pooling on the input and outputs both max values and indices.

The indices in `argmax` are flattened, so that a maximum value at position
`[b, y, x, c]` becomes flattened index:
`(y * width + x) * channels + c` if `include_batch_in_index` is False;
`((b * height + y) * width + x) * channels + c` if `include_batch_in_index` is True.

The indices returned are always in `[0, height) x [0, width)` before flattening,
even if padding is involved and the mathematically correct answer is outside
(either negative or too large).  This is a bug, but fixing it is difficult to do
in a safe backwards compatible way, especially due to flattening.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, height, width, channels]`.  Input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  Targmax: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, argmax).

  output: A `Tensor`. Has the same type as `input`.
  argmax: A `Tensor` of type `Targmax`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPoolWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolWithArgmax -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolWithArgmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolWithArgmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPoolWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolWithArgmax -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            Performs max pooling on the input and outputs both max values and indices.

The indices in `argmax` are flattened, so that a maximum value at position
`[b, y, x, c]` becomes flattened index:
`(y * width + x) * channels + c` if `include_batch_in_index` is False;
`((b * height + y) * width + x) * channels + c` if `include_batch_in_index` is True.

The indices returned are always in `[0, height) x [0, width)` before flattening,
even if padding is involved and the mathematically correct answer is outside
(either negative or too large).  This is a bug, but fixing it is difficult to do
in a safe backwards compatible way, especially due to flattening.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, height, width, channels]`.  Input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  Targmax: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, argmax).

  output: A `Tensor`. Has the same type as `input`.
  argmax: A `Tensor` of type `Targmax`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            Performs max pooling on the input and outputs both max values and indices.

The indices in `argmax` are flattened, so that a maximum value at position
`[b, y, x, c]` becomes flattened index:
`(y * width + x) * channels + c` if `include_batch_in_index` is False;
`((b * height + y) * width + x) * channels + c` if `include_batch_in_index` is True.

The indices returned are always in `[0, height) x [0, width)` before flattening,
even if padding is involved and the mathematically correct answer is outside
(either negative or too large).  This is a bug, but fixing it is difficult to do
in a safe backwards compatible way, especially due to flattening.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, height, width, channels]`.  Input to pool over.
  ksize: A list of `ints` that has length `>= 4`.
    The size of the window for each dimension of the input tensor.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  Targmax: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.
  include_batch_in_index: An optional `bool`. Defaults to `False`.
    Whether to include batch dimension in flattened index of `argmax`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, argmax).

  output: A `Tensor`. Has the same type as `input`.
  argmax: A `Tensor` of type `Targmax`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPoolWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolWithArgmax -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolWithArgmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolWithArgmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPoolWithArgmax/fuzz.cpp
[GEN] tf.raw_ops.RefSwitch -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            Forwards the ref tensor `data` to the output port determined by `pred`.

If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `Switch` and `Merge`.

Args:
  data: A mutable `Tensor`.
    The ref tensor to be forwarded to the appropriate output.
  pred: A `Tensor` of type `bool`.
    A scalar that specifies which output port will receive data.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_false, output_true).

  output_false: A mutable `Tensor`. Has the same type as `data`.
  output_true: A mutable `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RefSwitch`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RefSwitch` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            Forwards the ref tensor `data` to the output port determined by `pred`.

If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `Switch` and `Merge`.

Args:
  data: A mutable `Tensor`.
    The ref tensor to be forwarded to the appropriate output.
  pred: A `Tensor` of type `bool`.
    A scalar that specifies which output port will receive data.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_false, output_true).

  output_false: A mutable `Tensor`. Has the same type as `data`.
  output_true: A mutable `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RefSwitch`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RefSwitch` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.RefSwitch/fuzz.cpp
[GEN] tf.raw_ops.RefSwitch -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RefSwitch`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RefSwitch` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RefSwitch`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RefSwitch` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.RefSwitch/fuzz.cpp
[GEN] tf.raw_ops.RefSwitch -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            Forwards the ref tensor `data` to the output port determined by `pred`.

If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `Switch` and `Merge`.

Args:
  data: A mutable `Tensor`.
    The ref tensor to be forwarded to the appropriate output.
  pred: A `Tensor` of type `bool`.
    A scalar that specifies which output port will receive data.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_false, output_true).

  output_false: A mutable `Tensor`. Has the same type as `data`.
  output_true: A mutable `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            Forwards the ref tensor `data` to the output port determined by `pred`.

If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `Switch` and `Merge`.

Args:
  data: A mutable `Tensor`.
    The ref tensor to be forwarded to the appropriate output.
  pred: A `Tensor` of type `bool`.
    A scalar that specifies which output port will receive data.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_false, output_true).

  output_false: A mutable `Tensor`. Has the same type as `data`.
  output_true: A mutable `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.RefSwitch/fuzz.cpp
[GEN] tf.raw_ops.RefSwitch -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RefSwitch`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RefSwitch functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.RefSwitch/fuzz.cpp
[GEN] tf.raw_ops.Rsqrt -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            Computes reciprocal of square root of x element-wise.

I.e., \\(y = 1 / \sqrt{x}\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Rsqrt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Rsqrt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            Computes reciprocal of square root of x element-wise.

I.e., \\(y = 1 / \sqrt{x}\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Rsqrt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Rsqrt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Rsqrt/fuzz.cpp
[GEN] tf.raw_ops.Rsqrt -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Rsqrt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Rsqrt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Rsqrt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Rsqrt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Rsqrt/fuzz.cpp
[GEN] tf.raw_ops.Rsqrt -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            Computes reciprocal of square root of x element-wise.

I.e., \\(y = 1 / \sqrt{x}\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            Computes reciprocal of square root of x element-wise.

I.e., \\(y = 1 / \sqrt{x}\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Rsqrt/fuzz.cpp
[GEN] tf.raw_ops.Rsqrt -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Rsqrt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Rsqrt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Rsqrt/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdadelta -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            var: Should be from a Variable().

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  accum_update: A mutable `Tensor`. Must have the same type as `var`.
    : Should be from a Variable().
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  rho: A `Tensor`. Must have the same type as `var`.
    Decay factor. Must be a scalar.
  epsilon: A `Tensor`. Must have the same type as `var`.
    Constant factor. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdadelta`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdadelta` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            var: Should be from a Variable().

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  accum_update: A mutable `Tensor`. Must have the same type as `var`.
    : Should be from a Variable().
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  rho: A `Tensor`. Must have the same type as `var`.
    Decay factor. Must be a scalar.
  epsilon: A `Tensor`. Must have the same type as `var`.
    Constant factor. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdadelta`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdadelta` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseApplyAdadelta/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdadelta -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdadelta`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdadelta` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdadelta`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdadelta` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseApplyAdadelta/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdadelta -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            var: Should be from a Variable().

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  accum_update: A mutable `Tensor`. Must have the same type as `var`.
    : Should be from a Variable().
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  rho: A `Tensor`. Must have the same type as `var`.
    Decay factor. Must be a scalar.
  epsilon: A `Tensor`. Must have the same type as `var`.
    Constant factor. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            var: Should be from a Variable().

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  accum_update: A mutable `Tensor`. Must have the same type as `var`.
    : Should be from a Variable().
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  rho: A `Tensor`. Must have the same type as `var`.
    Decay factor. Must be a scalar.
  epsilon: A `Tensor`. Must have the same type as `var`.
    Constant factor. Must be a scalar.
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseApplyAdadelta/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdadelta -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdadelta`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdadelta functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseApplyAdadelta/fuzz.cpp
[GEN] tf.raw_ops.ApplyFtrlV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            Update '*var' according to the Ftrl-proximal scheme.

grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            Update '*var' according to the Ftrl-proximal scheme.

grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.ApplyFtrlV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.ApplyFtrlV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            Update '*var' according to the Ftrl-proximal scheme.

grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            Update '*var' according to the Ftrl-proximal scheme.

grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.ApplyFtrlV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyFtrlV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            Update relevant entries in '*var' according to the Ftrl-proximal scheme.

That is for rows we have grad for, we update var, accum and linear as follows:
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            Update relevant entries in '*var' according to the Ftrl-proximal scheme.

That is for rows we have grad for, we update var, accum and linear as follows:
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyFtrlV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyFtrlV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyFtrlV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyFtrlV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            Update relevant entries in '*var' according to the Ftrl-proximal scheme.

That is for rows we have grad for, we update var, accum and linear as follows:
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            Update relevant entries in '*var' according to the Ftrl-proximal scheme.

That is for rows we have grad for, we update var, accum and linear as follows:
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad * grad
linear += grad_with_shrinkage -
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  accum: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  linear: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 shrinkage regularization. Must be a scalar.
  l2_shrinkage: A `Tensor`. Must have the same type as `var`.
  lr_power: A `Tensor`. Must have the same type as `var`.
    Scaling factor. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If `True`, updating of the var and accum tensors will be protected
    by a lock; otherwise the behavior is undefined, but may exhibit less
    contention.
  multiply_linear_by_lr: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyFtrlV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyFtrlV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyFtrlV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseApplyFtrlV2/fuzz.cpp
[GEN] tf.raw_ops.AsString -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            Converts each entry in the given tensor to strings.

Supports many numeric types and boolean.

For Unicode, see the
[https://www.tensorflow.org/tutorials/representation/unicode](Working with Unicode text)
tutorial.

Examples:

>>> tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `complex64`, `complex128`, `bool`, `variant`, `string`.
  precision: An optional `int`. Defaults to `-1`.
    The post-decimal precision to use for floating point numbers.
    Only used if precision > -1.
  scientific: An optional `bool`. Defaults to `False`.
    Use scientific notation for floating point numbers.
  shortest: An optional `bool`. Defaults to `False`.
    Use shortest representation (either scientific or standard) for
    floating point numbers.
  width: An optional `int`. Defaults to `-1`.
    Pad pre-decimal numbers to this width.
    Applies to both floating point and integer numbers.
    Only used if width > -1.
  fill: An optional `string`. Defaults to `""`.
    The value to pad if width > -1.  If empty, pads with spaces.
    Another typical value is '0'.  String cannot be longer than 1 character.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AsString`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AsString` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            Converts each entry in the given tensor to strings.

Supports many numeric types and boolean.

For Unicode, see the
[https://www.tensorflow.org/tutorials/representation/unicode](Working with Unicode text)
tutorial.

Examples:

>>> tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `complex64`, `complex128`, `bool`, `variant`, `string`.
  precision: An optional `int`. Defaults to `-1`.
    The post-decimal precision to use for floating point numbers.
    Only used if precision > -1.
  scientific: An optional `bool`. Defaults to `False`.
    Use scientific notation for floating point numbers.
  shortest: An optional `bool`. Defaults to `False`.
    Use shortest representation (either scientific or standard) for
    floating point numbers.
  width: An optional `int`. Defaults to `-1`.
    Pad pre-decimal numbers to this width.
    Applies to both floating point and integer numbers.
    Only used if width > -1.
  fill: An optional `string`. Defaults to `""`.
    The value to pad if width > -1.  If empty, pads with spaces.
    Another typical value is '0'.  String cannot be longer than 1 character.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AsString`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AsString` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.AsString/fuzz.cpp
[GEN] tf.raw_ops.AsString -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AsString`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AsString` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AsString`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AsString` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.AsString/fuzz.cpp
[GEN] tf.raw_ops.AsString -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            Converts each entry in the given tensor to strings.

Supports many numeric types and boolean.

For Unicode, see the
[https://www.tensorflow.org/tutorials/representation/unicode](Working with Unicode text)
tutorial.

Examples:

>>> tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `complex64`, `complex128`, `bool`, `variant`, `string`.
  precision: An optional `int`. Defaults to `-1`.
    The post-decimal precision to use for floating point numbers.
    Only used if precision > -1.
  scientific: An optional `bool`. Defaults to `False`.
    Use scientific notation for floating point numbers.
  shortest: An optional `bool`. Defaults to `False`.
    Use shortest representation (either scientific or standard) for
    floating point numbers.
  width: An optional `int`. Defaults to `-1`.
    Pad pre-decimal numbers to this width.
    Applies to both floating point and integer numbers.
    Only used if width > -1.
  fill: An optional `string`. Defaults to `""`.
    The value to pad if width > -1.  If empty, pads with spaces.
    Another typical value is '0'.  String cannot be longer than 1 character.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            Converts each entry in the given tensor to strings.

Supports many numeric types and boolean.

For Unicode, see the
[https://www.tensorflow.org/tutorials/representation/unicode](Working with Unicode text)
tutorial.

Examples:

>>> tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `complex64`, `complex128`, `bool`, `variant`, `string`.
  precision: An optional `int`. Defaults to `-1`.
    The post-decimal precision to use for floating point numbers.
    Only used if precision > -1.
  scientific: An optional `bool`. Defaults to `False`.
    Use scientific notation for floating point numbers.
  shortest: An optional `bool`. Defaults to `False`.
    Use shortest representation (either scientific or standard) for
    floating point numbers.
  width: An optional `int`. Defaults to `-1`.
    Pad pre-decimal numbers to this width.
    Applies to both floating point and integer numbers.
    Only used if width > -1.
  fill: An optional `string`. Defaults to `""`.
    The value to pad if width > -1.  If empty, pads with spaces.
    Another typical value is '0'.  String cannot be longer than 1 character.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.AsString/fuzz.cpp
[GEN] tf.raw_ops.AsString -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AsString`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AsString functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.AsString/fuzz.cpp
[GEN] tf.raw_ops.RandomUniformInt -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            Outputs random integers from a uniform distribution.

The generated values are uniform integers in the range `[minval, maxval)`.
The lower bound `minval` is included in the range, while the upper bound
`maxval` is excluded.

The random integers are slightly biased unless `maxval - minval` is an exact
power of two.  The bias is small for values of `maxval - minval` significantly
smaller than the range of the output (either `2^32` or `2^64`).

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The shape of the output tensor.
  minval: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D.  Inclusive lower bound on the generated integers.
  maxval: A `Tensor`. Must have the same type as `minval`.
    0-D.  Exclusive upper bound on the generated integers.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `minval`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomUniformInt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomUniformInt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            Outputs random integers from a uniform distribution.

The generated values are uniform integers in the range `[minval, maxval)`.
The lower bound `minval` is included in the range, while the upper bound
`maxval` is excluded.

The random integers are slightly biased unless `maxval - minval` is an exact
power of two.  The bias is small for values of `maxval - minval` significantly
smaller than the range of the output (either `2^32` or `2^64`).

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The shape of the output tensor.
  minval: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D.  Inclusive lower bound on the generated integers.
  maxval: A `Tensor`. Must have the same type as `minval`.
    0-D.  Exclusive upper bound on the generated integers.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `minval`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomUniformInt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomUniformInt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.RandomUniformInt/fuzz.cpp
[GEN] tf.raw_ops.RandomUniformInt -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomUniformInt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomUniformInt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomUniformInt`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomUniformInt` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.RandomUniformInt/fuzz.cpp
[GEN] tf.raw_ops.RandomUniformInt -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            Outputs random integers from a uniform distribution.

The generated values are uniform integers in the range `[minval, maxval)`.
The lower bound `minval` is included in the range, while the upper bound
`maxval` is excluded.

The random integers are slightly biased unless `maxval - minval` is an exact
power of two.  The bias is small for values of `maxval - minval` significantly
smaller than the range of the output (either `2^32` or `2^64`).

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The shape of the output tensor.
  minval: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D.  Inclusive lower bound on the generated integers.
  maxval: A `Tensor`. Must have the same type as `minval`.
    0-D.  Exclusive upper bound on the generated integers.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `minval`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            Outputs random integers from a uniform distribution.

The generated values are uniform integers in the range `[minval, maxval)`.
The lower bound `minval` is included in the range, while the upper bound
`maxval` is excluded.

The random integers are slightly biased unless `maxval - minval` is an exact
power of two.  The bias is small for values of `maxval - minval` significantly
smaller than the range of the output (either `2^32` or `2^64`).

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The shape of the output tensor.
  minval: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    0-D.  Inclusive lower bound on the generated integers.
  maxval: A `Tensor`. Must have the same type as `minval`.
    0-D.  Exclusive upper bound on the generated integers.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `minval`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.RandomUniformInt/fuzz.cpp
[GEN] tf.raw_ops.RandomUniformInt -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomUniformInt`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomUniformInt functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.RandomUniformInt/fuzz.cpp
[GEN] tf.raw_ops.InvertPermutation -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            Computes the inverse permutation of a tensor.

This operation computes the inverse of an index permutation. It takes a 1-D
integer tensor `x`, which represents the indices of a zero-based array, and
swaps each value with its index position. In other words, for an output tensor
`y` and an input tensor `x`, this operation computes the following:

`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`

The values must include 0. There can be no duplicate values or negative values.

For example:

```
# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
```

Args:
  x: A `Tensor`. Must be one of the following types: `int32`, `int64`. 1-D.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InvertPermutation`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InvertPermutation` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            Computes the inverse permutation of a tensor.

This operation computes the inverse of an index permutation. It takes a 1-D
integer tensor `x`, which represents the indices of a zero-based array, and
swaps each value with its index position. In other words, for an output tensor
`y` and an input tensor `x`, this operation computes the following:

`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`

The values must include 0. There can be no duplicate values or negative values.

For example:

```
# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
```

Args:
  x: A `Tensor`. Must be one of the following types: `int32`, `int64`. 1-D.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InvertPermutation`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InvertPermutation` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.InvertPermutation/fuzz.cpp
[GEN] tf.raw_ops.InvertPermutation -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InvertPermutation`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InvertPermutation` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InvertPermutation`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InvertPermutation` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.InvertPermutation/fuzz.cpp
[GEN] tf.raw_ops.InvertPermutation -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            Computes the inverse permutation of a tensor.

This operation computes the inverse of an index permutation. It takes a 1-D
integer tensor `x`, which represents the indices of a zero-based array, and
swaps each value with its index position. In other words, for an output tensor
`y` and an input tensor `x`, this operation computes the following:

`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`

The values must include 0. There can be no duplicate values or negative values.

For example:

```
# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
```

Args:
  x: A `Tensor`. Must be one of the following types: `int32`, `int64`. 1-D.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            Computes the inverse permutation of a tensor.

This operation computes the inverse of an index permutation. It takes a 1-D
integer tensor `x`, which represents the indices of a zero-based array, and
swaps each value with its index position. In other words, for an output tensor
`y` and an input tensor `x`, this operation computes the following:

`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`

The values must include 0. There can be no duplicate values or negative values.

For example:

```
# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
```

Args:
  x: A `Tensor`. Must be one of the following types: `int32`, `int64`. 1-D.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.InvertPermutation/fuzz.cpp
[GEN] tf.raw_ops.InvertPermutation -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InvertPermutation`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InvertPermutation functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.InvertPermutation/fuzz.cpp
[GEN] tf.raw_ops.Cumsum -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            Compute the cumulative sum of the tensor `x` along `axis`.

By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:

```python
tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]
```

By setting the `exclusive` kwarg to `True`, an exclusive cumsum is
performed instead:

```python
tf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]
```

By setting the `reverse` kwarg to `True`, the cumsum is performed in the
opposite direction:

```python
tf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]
```

This is more efficient than using separate `tf.reverse` ops.

The `reverse` and `exclusive` kwargs can also be combined:

```python
tf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]
```

Args:
  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    A `Tensor`. Must be one of the following types: `float32`, `float64`,
    `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,
    `complex128`, `qint8`, `quint8`, `qint32`, `half`.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A `Tensor` of type `int32` (default: 0). Must be in the range
    `[-rank(x), rank(x))`.
  exclusive: An optional `bool`. Defaults to `False`.
    If `True`, perform exclusive cumsum.
  reverse: An optional `bool`. Defaults to `False`.
    A `bool` (default: False).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cumsum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cumsum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            Compute the cumulative sum of the tensor `x` along `axis`.

By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:

```python
tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]
```

By setting the `exclusive` kwarg to `True`, an exclusive cumsum is
performed instead:

```python
tf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]
```

By setting the `reverse` kwarg to `True`, the cumsum is performed in the
opposite direction:

```python
tf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]
```

This is more efficient than using separate `tf.reverse` ops.

The `reverse` and `exclusive` kwargs can also be combined:

```python
tf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]
```

Args:
  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    A `Tensor`. Must be one of the following types: `float32`, `float64`,
    `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,
    `complex128`, `qint8`, `quint8`, `qint32`, `half`.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A `Tensor` of type `int32` (default: 0). Must be in the range
    `[-rank(x), rank(x))`.
  exclusive: An optional `bool`. Defaults to `False`.
    If `True`, perform exclusive cumsum.
  reverse: An optional `bool`. Defaults to `False`.
    A `bool` (default: False).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cumsum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cumsum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Cumsum/fuzz.cpp
[GEN] tf.raw_ops.Cumsum -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cumsum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cumsum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cumsum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cumsum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Cumsum/fuzz.cpp
[GEN] tf.raw_ops.Cumsum -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            Compute the cumulative sum of the tensor `x` along `axis`.

By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:

```python
tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]
```

By setting the `exclusive` kwarg to `True`, an exclusive cumsum is
performed instead:

```python
tf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]
```

By setting the `reverse` kwarg to `True`, the cumsum is performed in the
opposite direction:

```python
tf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]
```

This is more efficient than using separate `tf.reverse` ops.

The `reverse` and `exclusive` kwargs can also be combined:

```python
tf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]
```

Args:
  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    A `Tensor`. Must be one of the following types: `float32`, `float64`,
    `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,
    `complex128`, `qint8`, `quint8`, `qint32`, `half`.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A `Tensor` of type `int32` (default: 0). Must be in the range
    `[-rank(x), rank(x))`.
  exclusive: An optional `bool`. Defaults to `False`.
    If `True`, perform exclusive cumsum.
  reverse: An optional `bool`. Defaults to `False`.
    A `bool` (default: False).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            Compute the cumulative sum of the tensor `x` along `axis`.

By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:

```python
tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]
```

By setting the `exclusive` kwarg to `True`, an exclusive cumsum is
performed instead:

```python
tf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]
```

By setting the `reverse` kwarg to `True`, the cumsum is performed in the
opposite direction:

```python
tf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]
```

This is more efficient than using separate `tf.reverse` ops.

The `reverse` and `exclusive` kwargs can also be combined:

```python
tf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]
```

Args:
  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    A `Tensor`. Must be one of the following types: `float32`, `float64`,
    `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,
    `complex128`, `qint8`, `quint8`, `qint32`, `half`.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A `Tensor` of type `int32` (default: 0). Must be in the range
    `[-rank(x), rank(x))`.
  exclusive: An optional `bool`. Defaults to `False`.
    If `True`, perform exclusive cumsum.
  reverse: An optional `bool`. Defaults to `False`.
    A `bool` (default: False).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Cumsum/fuzz.cpp
[GEN] tf.raw_ops.Cumsum -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cumsum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cumsum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Cumsum/fuzz.cpp
[GEN] tf.raw_ops.SparseConditionalAccumulator -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            A conditional accumulator for aggregating sparse gradients.

The accumulator accepts gradients marked with local_step greater or
equal to the most recent global_step known to the accumulator. The
average can be extracted from the accumulator, provided sufficient
gradients have been accumulated. Extracting the average automatically
resets the aggregate to 0, and increments the global_step recorded by
the accumulator.

Args:
  dtype: A `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.qint16, tf.quint16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64`.
    The type of the value being accumulated.
  shape: A `tf.TensorShape` or list of `ints`. The shape of the values.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator will be shared under the given name
    across multiple sessions.
  reduction_type: An optional `string` from: `"MEAN", "SUM"`. Defaults to `"MEAN"`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseConditionalAccumulator`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseConditionalAccumulator` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            A conditional accumulator for aggregating sparse gradients.

The accumulator accepts gradients marked with local_step greater or
equal to the most recent global_step known to the accumulator. The
average can be extracted from the accumulator, provided sufficient
gradients have been accumulated. Extracting the average automatically
resets the aggregate to 0, and increments the global_step recorded by
the accumulator.

Args:
  dtype: A `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.qint16, tf.quint16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64`.
    The type of the value being accumulated.
  shape: A `tf.TensorShape` or list of `ints`. The shape of the values.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator will be shared under the given name
    across multiple sessions.
  reduction_type: An optional `string` from: `"MEAN", "SUM"`. Defaults to `"MEAN"`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseConditionalAccumulator`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseConditionalAccumulator` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseConditionalAccumulator/fuzz.cpp
[GEN] tf.raw_ops.SparseConditionalAccumulator -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseConditionalAccumulator`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseConditionalAccumulator` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseConditionalAccumulator`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseConditionalAccumulator` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseConditionalAccumulator/fuzz.cpp
[GEN] tf.raw_ops.SparseConditionalAccumulator -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            A conditional accumulator for aggregating sparse gradients.

The accumulator accepts gradients marked with local_step greater or
equal to the most recent global_step known to the accumulator. The
average can be extracted from the accumulator, provided sufficient
gradients have been accumulated. Extracting the average automatically
resets the aggregate to 0, and increments the global_step recorded by
the accumulator.

Args:
  dtype: A `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.qint16, tf.quint16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64`.
    The type of the value being accumulated.
  shape: A `tf.TensorShape` or list of `ints`. The shape of the values.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator will be shared under the given name
    across multiple sessions.
  reduction_type: An optional `string` from: `"MEAN", "SUM"`. Defaults to `"MEAN"`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            A conditional accumulator for aggregating sparse gradients.

The accumulator accepts gradients marked with local_step greater or
equal to the most recent global_step known to the accumulator. The
average can be extracted from the accumulator, provided sufficient
gradients have been accumulated. Extracting the average automatically
resets the aggregate to 0, and increments the global_step recorded by
the accumulator.

Args:
  dtype: A `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.qint16, tf.quint16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64`.
    The type of the value being accumulated.
  shape: A `tf.TensorShape` or list of `ints`. The shape of the values.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this accumulator will be shared under the given name
    across multiple sessions.
  reduction_type: An optional `string` from: `"MEAN", "SUM"`. Defaults to `"MEAN"`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseConditionalAccumulator/fuzz.cpp
[GEN] tf.raw_ops.SparseConditionalAccumulator -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseConditionalAccumulator`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseConditionalAccumulator functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseConditionalAccumulator/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtN -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

See `tf.sparse.segment_sum` for usage examples.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtN`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtN` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

See `tf.sparse.segment_sum` for usage examples.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtN`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtN` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSegmentSqrtN/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtN -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtN`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtN` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtN`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtN` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSegmentSqrtN/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtN -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

See `tf.sparse.segment_sum` for usage examples.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

See `tf.sparse.segment_sum` for usage examples.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSegmentSqrtN/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtN -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtN`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtN functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSegmentSqrtN/fuzz.cpp
[GEN] tf.raw_ops.GetSessionTensor -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            Get the value of the tensor specified by its handle.

Args:
  handle: A `Tensor` of type `string`.
    The handle for a tensor stored in the session state.
  dtype: A `tf.DType`. The type of the output value.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.GetSessionTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.GetSessionTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            Get the value of the tensor specified by its handle.

Args:
  handle: A `Tensor` of type `string`.
    The handle for a tensor stored in the session state.
  dtype: A `tf.DType`. The type of the output value.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.GetSessionTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.GetSessionTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.GetSessionTensor/fuzz.cpp
[GEN] tf.raw_ops.GetSessionTensor -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.GetSessionTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.GetSessionTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.GetSessionTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.GetSessionTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.GetSessionTensor/fuzz.cpp
[GEN] tf.raw_ops.GetSessionTensor -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            Get the value of the tensor specified by its handle.

Args:
  handle: A `Tensor` of type `string`.
    The handle for a tensor stored in the session state.
  dtype: A `tf.DType`. The type of the output value.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            Get the value of the tensor specified by its handle.

Args:
  handle: A `Tensor` of type `string`.
    The handle for a tensor stored in the session state.
  dtype: A `tf.DType`. The type of the output value.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.GetSessionTensor/fuzz.cpp
[GEN] tf.raw_ops.GetSessionTensor -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.GetSessionTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.GetSessionTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.GetSessionTensor/fuzz.cpp
[GEN] tf.raw_ops.MulNoNan -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.

*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MulNoNan`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MulNoNan` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.

*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MulNoNan`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MulNoNan` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MulNoNan/fuzz.cpp
[GEN] tf.raw_ops.MulNoNan -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MulNoNan`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MulNoNan` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MulNoNan`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MulNoNan` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MulNoNan/fuzz.cpp
[GEN] tf.raw_ops.MulNoNan -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.

*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.

*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MulNoNan/fuzz.cpp
[GEN] tf.raw_ops.MulNoNan -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MulNoNan`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MulNoNan functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MulNoNan/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxesV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(100, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  colors: A `Tensor` of type `float32`.
    2-D. A list of RGBA colors to cycle through for the boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxesV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxesV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(100, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  colors: A `Tensor` of type `float32`.
    2-D. A list of RGBA colors to cycle through for the boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxesV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxesV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.DrawBoundingBoxesV2/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxesV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxesV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxesV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxesV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxesV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.DrawBoundingBoxesV2/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxesV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(100, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  colors: A `Tensor` of type `float32`.
    2-D. A list of RGBA colors to cycle through for the boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(100, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  colors: A `Tensor` of type `float32`.
    2-D. A list of RGBA colors to cycle through for the boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.DrawBoundingBoxesV2/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxesV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxesV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxesV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.DrawBoundingBoxesV2/fuzz.cpp
[GEN] tf.raw_ops.Reciprocal -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Reciprocal`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Reciprocal` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Reciprocal`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Reciprocal` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Reciprocal/fuzz.cpp
[GEN] tf.raw_ops.Reciprocal -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Reciprocal`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Reciprocal` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Reciprocal`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Reciprocal` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Reciprocal/fuzz.cpp
[GEN] tf.raw_ops.Reciprocal -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Reciprocal/fuzz.cpp
[GEN] tf.raw_ops.Reciprocal -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Reciprocal`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Reciprocal functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Reciprocal/fuzz.cpp
[GEN] tf.raw_ops.Fingerprint -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            Generates fingerprint values.

Generates fingerprint values of `data`.

Fingerprint op considers the first dimension of `data` as the batch dimension,
and `output[i]` contains the fingerprint value generated from contents in
`data[i, ...]` for all `i`.

Fingerprint op writes fingerprint values as byte arrays. For example, the
default method `farmhash64` generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an `uint8` array of size 8, in little-endian
order.

For example, suppose that `data` has data type `DT_INT32` and shape (2, 3, 4),
and that the fingerprint method is `farmhash64`. In this case, the output shape
is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the size of
each fingerprint value in bytes. `output[0, :]` is generated from 12 integers in
`data[0, :, :]` and similarly `output[1, :]` is generated from other 12 integers
in `data[1, :, :]`.

Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same:

```
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
```

For string data, one should expect `Fingerprint(data) !=
Fingerprint(ReduceJoin(data))` in general.

Args:
  data: A `Tensor`. Must have rank 1 or higher.
  method: A `Tensor` of type `string`.
    Fingerprint method used by this op. Currently available method is
    `farmhash::fingerprint64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `uint8`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Fingerprint`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Fingerprint` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            Generates fingerprint values.

Generates fingerprint values of `data`.

Fingerprint op considers the first dimension of `data` as the batch dimension,
and `output[i]` contains the fingerprint value generated from contents in
`data[i, ...]` for all `i`.

Fingerprint op writes fingerprint values as byte arrays. For example, the
default method `farmhash64` generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an `uint8` array of size 8, in little-endian
order.

For example, suppose that `data` has data type `DT_INT32` and shape (2, 3, 4),
and that the fingerprint method is `farmhash64`. In this case, the output shape
is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the size of
each fingerprint value in bytes. `output[0, :]` is generated from 12 integers in
`data[0, :, :]` and similarly `output[1, :]` is generated from other 12 integers
in `data[1, :, :]`.

Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same:

```
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
```

For string data, one should expect `Fingerprint(data) !=
Fingerprint(ReduceJoin(data))` in general.

Args:
  data: A `Tensor`. Must have rank 1 or higher.
  method: A `Tensor` of type `string`.
    Fingerprint method used by this op. Currently available method is
    `farmhash::fingerprint64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `uint8`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Fingerprint`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Fingerprint` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Fingerprint/fuzz.cpp
[GEN] tf.raw_ops.Fingerprint -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Fingerprint`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Fingerprint` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Fingerprint`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Fingerprint` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Fingerprint/fuzz.cpp
[GEN] tf.raw_ops.Fingerprint -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            Generates fingerprint values.

Generates fingerprint values of `data`.

Fingerprint op considers the first dimension of `data` as the batch dimension,
and `output[i]` contains the fingerprint value generated from contents in
`data[i, ...]` for all `i`.

Fingerprint op writes fingerprint values as byte arrays. For example, the
default method `farmhash64` generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an `uint8` array of size 8, in little-endian
order.

For example, suppose that `data` has data type `DT_INT32` and shape (2, 3, 4),
and that the fingerprint method is `farmhash64`. In this case, the output shape
is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the size of
each fingerprint value in bytes. `output[0, :]` is generated from 12 integers in
`data[0, :, :]` and similarly `output[1, :]` is generated from other 12 integers
in `data[1, :, :]`.

Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same:

```
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
```

For string data, one should expect `Fingerprint(data) !=
Fingerprint(ReduceJoin(data))` in general.

Args:
  data: A `Tensor`. Must have rank 1 or higher.
  method: A `Tensor` of type `string`.
    Fingerprint method used by this op. Currently available method is
    `farmhash::fingerprint64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `uint8`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            Generates fingerprint values.

Generates fingerprint values of `data`.

Fingerprint op considers the first dimension of `data` as the batch dimension,
and `output[i]` contains the fingerprint value generated from contents in
`data[i, ...]` for all `i`.

Fingerprint op writes fingerprint values as byte arrays. For example, the
default method `farmhash64` generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an `uint8` array of size 8, in little-endian
order.

For example, suppose that `data` has data type `DT_INT32` and shape (2, 3, 4),
and that the fingerprint method is `farmhash64`. In this case, the output shape
is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the size of
each fingerprint value in bytes. `output[0, :]` is generated from 12 integers in
`data[0, :, :]` and similarly `output[1, :]` is generated from other 12 integers
in `data[1, :, :]`.

Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same:

```
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
```

For string data, one should expect `Fingerprint(data) !=
Fingerprint(ReduceJoin(data))` in general.

Args:
  data: A `Tensor`. Must have rank 1 or higher.
  method: A `Tensor` of type `string`.
    Fingerprint method used by this op. Currently available method is
    `farmhash::fingerprint64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `uint8`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Fingerprint/fuzz.cpp
[GEN] tf.raw_ops.Fingerprint -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Fingerprint`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Fingerprint functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Fingerprint/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropInput -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the input.

Args:
  input_sizes: A `Tensor` of type `int32`.
    An integer vector representing the shape of `input`, based
    on `data_format`.  For example, if `data_format` is 'NHWC' then
     `input` is a 4-D `[batch, height, width, channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape
    `[filter_height, filter_width, in_channels, depthwise_multiplier]`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the input.

Args:
  input_sizes: A `Tensor` of type `int32`.
    An integer vector representing the shape of `input`, based
    on `data_format`.  For example, if `data_format` is 'NHWC' then
     `input` is a 4-D `[batch, height, width, channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape
    `[filter_height, filter_width, in_channels, depthwise_multiplier]`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.DepthwiseConv2dNativeBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropInput -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.DepthwiseConv2dNativeBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropInput -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the input.

Args:
  input_sizes: A `Tensor` of type `int32`.
    An integer vector representing the shape of `input`, based
    on `data_format`.  For example, if `data_format` is 'NHWC' then
     `input` is a 4-D `[batch, height, width, channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape
    `[filter_height, filter_width, in_channels, depthwise_multiplier]`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the input.

Args:
  input_sizes: A `Tensor` of type `int32`.
    An integer vector representing the shape of `input`, based
    on `data_format`.  For example, if `data_format` is 'NHWC' then
     `input` is a 4-D `[batch, height, width, channels]` tensor.
  filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape
    `[filter_height, filter_width, in_channels, depthwise_multiplier]`.
  out_backprop: A `Tensor`. Must have the same type as `filter`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `filter`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.DepthwiseConv2dNativeBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropInput -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropInput`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropInput functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.DepthwiseConv2dNativeBackpropInput/fuzz.cpp
[GEN] tf.raw_ops.PriorityQueue -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            A queue that produces elements sorted by the first component value.

Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.

Args:
  shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`).
    The shape of each component in a value. The length of this attr must
    be either 0 or the same as the length of component_types. If the length of
    this attr is 0, the shapes of queue elements are not constrained, and
    only one element may be dequeued at a time.
  component_types: An optional list of `tf.DTypes`. Defaults to `[]`.
    The type of each component in a value.
  capacity: An optional `int`. Defaults to `-1`.
    The upper bound on the number of elements in this queue.
    Negative numbers mean no limit.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this queue is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this queue will be shared under the given name
    across multiple sessions.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.PriorityQueue`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.PriorityQueue` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            A queue that produces elements sorted by the first component value.

Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.

Args:
  shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`).
    The shape of each component in a value. The length of this attr must
    be either 0 or the same as the length of component_types. If the length of
    this attr is 0, the shapes of queue elements are not constrained, and
    only one element may be dequeued at a time.
  component_types: An optional list of `tf.DTypes`. Defaults to `[]`.
    The type of each component in a value.
  capacity: An optional `int`. Defaults to `-1`.
    The upper bound on the number of elements in this queue.
    Negative numbers mean no limit.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this queue is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this queue will be shared under the given name
    across multiple sessions.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.PriorityQueue`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.PriorityQueue` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.PriorityQueue/fuzz.cpp
[GEN] tf.raw_ops.PriorityQueue -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.PriorityQueue`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.PriorityQueue` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.PriorityQueue`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.PriorityQueue` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.PriorityQueue/fuzz.cpp
[GEN] tf.raw_ops.PriorityQueue -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            A queue that produces elements sorted by the first component value.

Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.

Args:
  shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`).
    The shape of each component in a value. The length of this attr must
    be either 0 or the same as the length of component_types. If the length of
    this attr is 0, the shapes of queue elements are not constrained, and
    only one element may be dequeued at a time.
  component_types: An optional list of `tf.DTypes`. Defaults to `[]`.
    The type of each component in a value.
  capacity: An optional `int`. Defaults to `-1`.
    The upper bound on the number of elements in this queue.
    Negative numbers mean no limit.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this queue is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this queue will be shared under the given name
    across multiple sessions.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            A queue that produces elements sorted by the first component value.

Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.

Args:
  shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`).
    The shape of each component in a value. The length of this attr must
    be either 0 or the same as the length of component_types. If the length of
    this attr is 0, the shapes of queue elements are not constrained, and
    only one element may be dequeued at a time.
  component_types: An optional list of `tf.DTypes`. Defaults to `[]`.
    The type of each component in a value.
  capacity: An optional `int`. Defaults to `-1`.
    The upper bound on the number of elements in this queue.
    Negative numbers mean no limit.
  container: An optional `string`. Defaults to `""`.
    If non-empty, this queue is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this queue will be shared under the given name
    across multiple sessions.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.PriorityQueue/fuzz.cpp
[GEN] tf.raw_ops.PriorityQueue -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.PriorityQueue`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.PriorityQueue functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.PriorityQueue/fuzz.cpp
[GEN] tf.raw_ops.LogUniformCandidateSampler -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            Generates labels for candidate sampling with a log-uniform distribution.

See explanations of candidate sampling and the data formats at
go/candidate-sampling.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.

Args:
  true_classes: A `Tensor` of type `int64`.
    A batch_size * num_true matrix, in which each row contains the
    IDs of the num_true target_classes in the corresponding original label.
  num_true: An `int` that is `>= 1`. Number of true labels per context.
  num_sampled: An `int` that is `>= 1`.
    Number of candidates to randomly sample.
  unique: A `bool`.
    If unique is true, we sample with rejection, so that all sampled
    candidates in a batch are unique. This requires some approximation to
    estimate the post-rejection sampling probabilities.
  range_max: An `int` that is `>= 1`.
    The sampler will sample integers from the interval [0, range_max).
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (sampled_candidates, true_expected_count, sampled_expected_count).

  sampled_candidates: A `Tensor` of type `int64`.
  true_expected_count: A `Tensor` of type `float32`.
  sampled_expected_count: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogUniformCandidateSampler`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogUniformCandidateSampler` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            Generates labels for candidate sampling with a log-uniform distribution.

See explanations of candidate sampling and the data formats at
go/candidate-sampling.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.

Args:
  true_classes: A `Tensor` of type `int64`.
    A batch_size * num_true matrix, in which each row contains the
    IDs of the num_true target_classes in the corresponding original label.
  num_true: An `int` that is `>= 1`. Number of true labels per context.
  num_sampled: An `int` that is `>= 1`.
    Number of candidates to randomly sample.
  unique: A `bool`.
    If unique is true, we sample with rejection, so that all sampled
    candidates in a batch are unique. This requires some approximation to
    estimate the post-rejection sampling probabilities.
  range_max: An `int` that is `>= 1`.
    The sampler will sample integers from the interval [0, range_max).
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (sampled_candidates, true_expected_count, sampled_expected_count).

  sampled_candidates: A `Tensor` of type `int64`.
  true_expected_count: A `Tensor` of type `float32`.
  sampled_expected_count: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogUniformCandidateSampler`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogUniformCandidateSampler` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.LogUniformCandidateSampler/fuzz.cpp
[GEN] tf.raw_ops.LogUniformCandidateSampler -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogUniformCandidateSampler`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogUniformCandidateSampler` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogUniformCandidateSampler`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogUniformCandidateSampler` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.LogUniformCandidateSampler/fuzz.cpp
[GEN] tf.raw_ops.LogUniformCandidateSampler -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            Generates labels for candidate sampling with a log-uniform distribution.

See explanations of candidate sampling and the data formats at
go/candidate-sampling.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.

Args:
  true_classes: A `Tensor` of type `int64`.
    A batch_size * num_true matrix, in which each row contains the
    IDs of the num_true target_classes in the corresponding original label.
  num_true: An `int` that is `>= 1`. Number of true labels per context.
  num_sampled: An `int` that is `>= 1`.
    Number of candidates to randomly sample.
  unique: A `bool`.
    If unique is true, we sample with rejection, so that all sampled
    candidates in a batch are unique. This requires some approximation to
    estimate the post-rejection sampling probabilities.
  range_max: An `int` that is `>= 1`.
    The sampler will sample integers from the interval [0, range_max).
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (sampled_candidates, true_expected_count, sampled_expected_count).

  sampled_candidates: A `Tensor` of type `int64`.
  true_expected_count: A `Tensor` of type `float32`.
  sampled_expected_count: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            Generates labels for candidate sampling with a log-uniform distribution.

See explanations of candidate sampling and the data formats at
go/candidate-sampling.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.

Args:
  true_classes: A `Tensor` of type `int64`.
    A batch_size * num_true matrix, in which each row contains the
    IDs of the num_true target_classes in the corresponding original label.
  num_true: An `int` that is `>= 1`. Number of true labels per context.
  num_sampled: An `int` that is `>= 1`.
    Number of candidates to randomly sample.
  unique: A `bool`.
    If unique is true, we sample with rejection, so that all sampled
    candidates in a batch are unique. This requires some approximation to
    estimate the post-rejection sampling probabilities.
  range_max: An `int` that is `>= 1`.
    The sampler will sample integers from the interval [0, range_max).
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (sampled_candidates, true_expected_count, sampled_expected_count).

  sampled_candidates: A `Tensor` of type `int64`.
  true_expected_count: A `Tensor` of type `float32`.
  sampled_expected_count: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.LogUniformCandidateSampler/fuzz.cpp
[GEN] tf.raw_ops.LogUniformCandidateSampler -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogUniformCandidateSampler`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogUniformCandidateSampler functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.LogUniformCandidateSampler/fuzz.cpp
[GEN] tf.raw_ops.FusedResizeAndPadConv2D -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            Performs a resize and padding as a preprocess during a convolution.

It's often possible to do spatial transformations more efficiently as part of
the packing stage of a convolution, so this op allows for an optimized
implementation where these stages are fused together. This prevents the need to
write out the intermediate results as whole tensors, reducing memory pressure,
and we can get some latency gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and defaults to
'NHWC' order.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  size: A `Tensor` of type `int32`.
    A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The
    new size for the images.
  paddings: A `Tensor` of type `int32`.
    A two-column matrix specifying the padding sizes. The number of
    rows must be the same as the rank of `input`.
  filter: A `Tensor`. Must have the same type as `input`. 4-D with shape
    `[filter_height, filter_width, in_channels, out_channels]`.
  mode: A `string` from: `"REFLECT", "SYMMETRIC"`.
  strides: A list of `ints`.
    1-D of length 4.  The stride of the sliding window for each dimension
    of `input`. Must be in the same order as the dimension specified with format.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  resize_align_corners: An optional `bool`. Defaults to `False`.
    If true, the centers of the 4 corner pixels of the input and output tensors are
    aligned, preserving the values at the corner pixels. Defaults to false.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedResizeAndPadConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedResizeAndPadConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            Performs a resize and padding as a preprocess during a convolution.

It's often possible to do spatial transformations more efficiently as part of
the packing stage of a convolution, so this op allows for an optimized
implementation where these stages are fused together. This prevents the need to
write out the intermediate results as whole tensors, reducing memory pressure,
and we can get some latency gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and defaults to
'NHWC' order.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  size: A `Tensor` of type `int32`.
    A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The
    new size for the images.
  paddings: A `Tensor` of type `int32`.
    A two-column matrix specifying the padding sizes. The number of
    rows must be the same as the rank of `input`.
  filter: A `Tensor`. Must have the same type as `input`. 4-D with shape
    `[filter_height, filter_width, in_channels, out_channels]`.
  mode: A `string` from: `"REFLECT", "SYMMETRIC"`.
  strides: A list of `ints`.
    1-D of length 4.  The stride of the sliding window for each dimension
    of `input`. Must be in the same order as the dimension specified with format.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  resize_align_corners: An optional `bool`. Defaults to `False`.
    If true, the centers of the 4 corner pixels of the input and output tensors are
    aligned, preserving the values at the corner pixels. Defaults to false.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedResizeAndPadConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedResizeAndPadConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FusedResizeAndPadConv2D/fuzz.cpp
[GEN] tf.raw_ops.FusedResizeAndPadConv2D -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedResizeAndPadConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedResizeAndPadConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedResizeAndPadConv2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedResizeAndPadConv2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FusedResizeAndPadConv2D/fuzz.cpp
[GEN] tf.raw_ops.FusedResizeAndPadConv2D -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            Performs a resize and padding as a preprocess during a convolution.

It's often possible to do spatial transformations more efficiently as part of
the packing stage of a convolution, so this op allows for an optimized
implementation where these stages are fused together. This prevents the need to
write out the intermediate results as whole tensors, reducing memory pressure,
and we can get some latency gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and defaults to
'NHWC' order.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  size: A `Tensor` of type `int32`.
    A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The
    new size for the images.
  paddings: A `Tensor` of type `int32`.
    A two-column matrix specifying the padding sizes. The number of
    rows must be the same as the rank of `input`.
  filter: A `Tensor`. Must have the same type as `input`. 4-D with shape
    `[filter_height, filter_width, in_channels, out_channels]`.
  mode: A `string` from: `"REFLECT", "SYMMETRIC"`.
  strides: A list of `ints`.
    1-D of length 4.  The stride of the sliding window for each dimension
    of `input`. Must be in the same order as the dimension specified with format.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  resize_align_corners: An optional `bool`. Defaults to `False`.
    If true, the centers of the 4 corner pixels of the input and output tensors are
    aligned, preserving the values at the corner pixels. Defaults to false.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            Performs a resize and padding as a preprocess during a convolution.

It's often possible to do spatial transformations more efficiently as part of
the packing stage of a convolution, so this op allows for an optimized
implementation where these stages are fused together. This prevents the need to
write out the intermediate results as whole tensors, reducing memory pressure,
and we can get some latency gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and defaults to
'NHWC' order.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.
    4-D with shape `[batch, in_height, in_width, in_channels]`.
  size: A `Tensor` of type `int32`.
    A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The
    new size for the images.
  paddings: A `Tensor` of type `int32`.
    A two-column matrix specifying the padding sizes. The number of
    rows must be the same as the rank of `input`.
  filter: A `Tensor`. Must have the same type as `input`. 4-D with shape
    `[filter_height, filter_width, in_channels, out_channels]`.
  mode: A `string` from: `"REFLECT", "SYMMETRIC"`.
  strides: A list of `ints`.
    1-D of length 4.  The stride of the sliding window for each dimension
    of `input`. Must be in the same order as the dimension specified with format.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  resize_align_corners: An optional `bool`. Defaults to `False`.
    If true, the centers of the 4 corner pixels of the input and output tensors are
    aligned, preserving the values at the corner pixels. Defaults to false.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FusedResizeAndPadConv2D/fuzz.cpp
[GEN] tf.raw_ops.FusedResizeAndPadConv2D -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedResizeAndPadConv2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedResizeAndPadConv2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FusedResizeAndPadConv2D/fuzz.cpp
[GEN] tf.raw_ops.LogicalOr -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            Returns the truth value of x OR y element-wise.

Logical OR function.

Requires that `x` and `y` have the same shape or have
[broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
shapes. For example, `x` and `y` can be:

- Two single elements of type `bool`.
- One `tf.Tensor` of type `bool` and one single `bool`, where the result will
  be calculated by applying logical OR with the single element to each
  element in the larger Tensor.
- Two `tf.Tensor` objects of type `bool` of the same shape. In this case,
  the result will be the element-wise logical OR of the two input tensors.

You can also use the `|` operator instead.

Usage:

  >>> a = tf.constant([True])
  >>> b = tf.constant([False])
  >>> tf.math.logical_or(a, b)
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
  >>> a | b
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

  >>> c = tf.constant([False])
  >>> x = tf.constant([False, True, True, False])
  >>> tf.math.logical_or(c, x)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>
  >>> c | x
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>

  >>> y = tf.constant([False, False, True, True])
  >>> z = tf.constant([False, True, False, True])
  >>> tf.math.logical_or(y, z)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>
  >>> y | z
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>

  This op also supports broadcasting

  >>> tf.logical_or([[True, False]], [[True], [False]])
  <tf.Tensor: shape=(2, 2), dtype=bool, numpy=
  array([[ True,  True],
       [ True, False]])>

The reduction version of this elementwise operation is `tf.math.reduce_any`.

Args:
    x: A `tf.Tensor` of type bool.
    y: A `tf.Tensor` of type bool.
    name: A name for the operation (optional).

Returns:
  A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.

Args:
  x: A `Tensor` of type `bool`.
  y: A `Tensor` of type `bool`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogicalOr`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogicalOr` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            Returns the truth value of x OR y element-wise.

Logical OR function.

Requires that `x` and `y` have the same shape or have
[broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
shapes. For example, `x` and `y` can be:

- Two single elements of type `bool`.
- One `tf.Tensor` of type `bool` and one single `bool`, where the result will
  be calculated by applying logical OR with the single element to each
  element in the larger Tensor.
- Two `tf.Tensor` objects of type `bool` of the same shape. In this case,
  the result will be the element-wise logical OR of the two input tensors.

You can also use the `|` operator instead.

Usage:

  >>> a = tf.constant([True])
  >>> b = tf.constant([False])
  >>> tf.math.logical_or(a, b)
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
  >>> a | b
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

  >>> c = tf.constant([False])
  >>> x = tf.constant([False, True, True, False])
  >>> tf.math.logical_or(c, x)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>
  >>> c | x
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>

  >>> y = tf.constant([False, False, True, True])
  >>> z = tf.constant([False, True, False, True])
  >>> tf.math.logical_or(y, z)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>
  >>> y | z
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>

  This op also supports broadcasting

  >>> tf.logical_or([[True, False]], [[True], [False]])
  <tf.Tensor: shape=(2, 2), dtype=bool, numpy=
  array([[ True,  True],
       [ True, False]])>

The reduction version of this elementwise operation is `tf.math.reduce_any`.

Args:
    x: A `tf.Tensor` of type bool.
    y: A `tf.Tensor` of type bool.
    name: A name for the operation (optional).

Returns:
  A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.

Args:
  x: A `Tensor` of type `bool`.
  y: A `Tensor` of type `bool`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogicalOr`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogicalOr` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.LogicalOr/fuzz.cpp
[GEN] tf.raw_ops.LogicalOr -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogicalOr`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogicalOr` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LogicalOr`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LogicalOr` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.LogicalOr/fuzz.cpp
[GEN] tf.raw_ops.LogicalOr -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            Returns the truth value of x OR y element-wise.

Logical OR function.

Requires that `x` and `y` have the same shape or have
[broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
shapes. For example, `x` and `y` can be:

- Two single elements of type `bool`.
- One `tf.Tensor` of type `bool` and one single `bool`, where the result will
  be calculated by applying logical OR with the single element to each
  element in the larger Tensor.
- Two `tf.Tensor` objects of type `bool` of the same shape. In this case,
  the result will be the element-wise logical OR of the two input tensors.

You can also use the `|` operator instead.

Usage:

  >>> a = tf.constant([True])
  >>> b = tf.constant([False])
  >>> tf.math.logical_or(a, b)
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
  >>> a | b
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

  >>> c = tf.constant([False])
  >>> x = tf.constant([False, True, True, False])
  >>> tf.math.logical_or(c, x)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>
  >>> c | x
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>

  >>> y = tf.constant([False, False, True, True])
  >>> z = tf.constant([False, True, False, True])
  >>> tf.math.logical_or(y, z)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>
  >>> y | z
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>

  This op also supports broadcasting

  >>> tf.logical_or([[True, False]], [[True], [False]])
  <tf.Tensor: shape=(2, 2), dtype=bool, numpy=
  array([[ True,  True],
       [ True, False]])>

The reduction version of this elementwise operation is `tf.math.reduce_any`.

Args:
    x: A `tf.Tensor` of type bool.
    y: A `tf.Tensor` of type bool.
    name: A name for the operation (optional).

Returns:
  A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.

Args:
  x: A `Tensor` of type `bool`.
  y: A `Tensor` of type `bool`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            Returns the truth value of x OR y element-wise.

Logical OR function.

Requires that `x` and `y` have the same shape or have
[broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
shapes. For example, `x` and `y` can be:

- Two single elements of type `bool`.
- One `tf.Tensor` of type `bool` and one single `bool`, where the result will
  be calculated by applying logical OR with the single element to each
  element in the larger Tensor.
- Two `tf.Tensor` objects of type `bool` of the same shape. In this case,
  the result will be the element-wise logical OR of the two input tensors.

You can also use the `|` operator instead.

Usage:

  >>> a = tf.constant([True])
  >>> b = tf.constant([False])
  >>> tf.math.logical_or(a, b)
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
  >>> a | b
  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

  >>> c = tf.constant([False])
  >>> x = tf.constant([False, True, True, False])
  >>> tf.math.logical_or(c, x)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>
  >>> c | x
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>

  >>> y = tf.constant([False, False, True, True])
  >>> z = tf.constant([False, True, False, True])
  >>> tf.math.logical_or(y, z)
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>
  >>> y | z
  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>

  This op also supports broadcasting

  >>> tf.logical_or([[True, False]], [[True], [False]])
  <tf.Tensor: shape=(2, 2), dtype=bool, numpy=
  array([[ True,  True],
       [ True, False]])>

The reduction version of this elementwise operation is `tf.math.reduce_any`.

Args:
    x: A `tf.Tensor` of type bool.
    y: A `tf.Tensor` of type bool.
    name: A name for the operation (optional).

Returns:
  A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.

Args:
  x: A `Tensor` of type `bool`.
  y: A `Tensor` of type `bool`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.LogicalOr/fuzz.cpp
[GEN] tf.raw_ops.LogicalOr -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LogicalOr`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LogicalOr functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.LogicalOr/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxVarsGradient -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxVars operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxVars operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxVars operation.
    min, max: Quantization interval, scalar floats.
  min: A `Tensor` of type `float32`.
  max: A `Tensor` of type `float32`.
  num_bits: An optional `int`. Defaults to `8`.
    The bitwidth of the quantization; between 2 and 8, inclusive.
  narrow_range: An optional `bool`. Defaults to `False`.
    Whether to quantize into 2^num_bits - 1 distinct values.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max).

  backprops_wrt_input: A `Tensor` of type `float32`.
  backprop_wrt_min: A `Tensor` of type `float32`.
  backprop_wrt_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxVars operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxVars operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxVars operation.
    min, max: Quantization interval, scalar floats.
  min: A `Tensor` of type `float32`.
  max: A `Tensor` of type `float32`.
  num_bits: An optional `int`. Defaults to `8`.
    The bitwidth of the quantization; between 2 and 8, inclusive.
  narrow_range: An optional `bool`. Defaults to `False`.
    Whether to quantize into 2^num_bits - 1 distinct values.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max).

  backprops_wrt_input: A `Tensor` of type `float32`.
  backprop_wrt_min: A `Tensor` of type `float32`.
  backprop_wrt_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FakeQuantWithMinMaxVarsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxVarsGradient -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxVarsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FakeQuantWithMinMaxVarsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxVarsGradient -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxVars operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxVars operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxVars operation.
    min, max: Quantization interval, scalar floats.
  min: A `Tensor` of type `float32`.
  max: A `Tensor` of type `float32`.
  num_bits: An optional `int`. Defaults to `8`.
    The bitwidth of the quantization; between 2 and 8, inclusive.
  narrow_range: An optional `bool`. Defaults to `False`.
    Whether to quantize into 2^num_bits - 1 distinct values.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max).

  backprops_wrt_input: A `Tensor` of type `float32`.
  backprop_wrt_min: A `Tensor` of type `float32`.
  backprop_wrt_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxVars operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxVars operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxVars operation.
    min, max: Quantization interval, scalar floats.
  min: A `Tensor` of type `float32`.
  max: A `Tensor` of type `float32`.
  num_bits: An optional `int`. Defaults to `8`.
    The bitwidth of the quantization; between 2 and 8, inclusive.
  narrow_range: An optional `bool`. Defaults to `False`.
    Whether to quantize into 2^num_bits - 1 distinct values.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max).

  backprops_wrt_input: A `Tensor` of type `float32`.
  backprop_wrt_min: A `Tensor` of type `float32`.
  backprop_wrt_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FakeQuantWithMinMaxVarsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxVarsGradient -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxVarsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxVarsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FakeQuantWithMinMaxVarsGradient/fuzz.cpp
[GEN] tf.raw_ops.LoopCond -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            Forwards the input to the output.

This operator represents the loop termination condition used by the
"pivot" switches of a loop.

Args:
  input: A `Tensor` of type `bool`.
    A boolean scalar, representing the branch predicate of the Switch op.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LoopCond`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LoopCond` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            Forwards the input to the output.

This operator represents the loop termination condition used by the
"pivot" switches of a loop.

Args:
  input: A `Tensor` of type `bool`.
    A boolean scalar, representing the branch predicate of the Switch op.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LoopCond`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LoopCond` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.LoopCond/fuzz.cpp
[GEN] tf.raw_ops.LoopCond -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LoopCond`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LoopCond` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.LoopCond`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.LoopCond` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.LoopCond/fuzz.cpp
[GEN] tf.raw_ops.LoopCond -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            Forwards the input to the output.

This operator represents the loop termination condition used by the
"pivot" switches of a loop.

Args:
  input: A `Tensor` of type `bool`.
    A boolean scalar, representing the branch predicate of the Switch op.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            Forwards the input to the output.

This operator represents the loop termination condition used by the
"pivot" switches of a loop.

Args:
  input: A `Tensor` of type `bool`.
    A boolean scalar, representing the branch predicate of the Switch op.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `bool`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.LoopCond/fuzz.cpp
[GEN] tf.raw_ops.LoopCond -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.LoopCond`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.LoopCond functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.LoopCond/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropFilter -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape based on `data_format`.  For example, if
    `data_format` is 'NHWC' then `input` is a 4-D `[batch, in_height,
    in_width, in_channels]` tensor.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, depthwise_multiplier]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape based on `data_format`.  For example, if
    `data_format` is 'NHWC' then `input` is a 4-D `[batch, in_height,
    in_width, in_channels]` tensor.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, depthwise_multiplier]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.DepthwiseConv2dNativeBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropFilter -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.DepthwiseConv2dNativeBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropFilter -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape based on `data_format`.  For example, if
    `data_format` is 'NHWC' then `input` is a 4-D `[batch, in_height,
    in_width, in_channels]` tensor.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, depthwise_multiplier]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            Computes the gradients of depthwise convolution with respect to the filter.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    4-D with shape based on `data_format`.  For example, if
    `data_format` is 'NHWC' then `input` is a 4-D `[batch, in_height,
    in_width, in_channels]` tensor.
  filter_sizes: A `Tensor` of type `int32`.
    An integer vector representing the tensor shape of `filter`,
    where `filter` is a 4-D
    `[filter_height, filter_width, in_channels, depthwise_multiplier]` tensor.
  out_backprop: A `Tensor`. Must have the same type as `input`.
    4-D with shape  based on `data_format`.
    For example, if `data_format` is 'NHWC' then
    out_backprop shape is `[batch, out_height, out_width, out_channels]`.
    Gradients w.r.t. the output of the convolution.
  strides: A list of `ints`.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A `string` from: `"SAME", "VALID", "EXPLICIT"`.
    The type of padding algorithm to use.
  explicit_paddings: An optional list of `ints`. Defaults to `[]`.
  data_format: An optional `string` from: `"NHWC", "NCHW"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, height, width, channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, channels, height, width].
  dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.
    1-D tensor of length 4.  The dilation factor for each dimension of
    `input`. If set to k > 1, there will be k-1 skipped cells between each filter
    element on that dimension. The dimension order is determined by the value of
    `data_format`, see above for details. Dilations in the batch and depth
    dimensions must be 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.DepthwiseConv2dNativeBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.DepthwiseConv2dNativeBackpropFilter -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DepthwiseConv2dNativeBackpropFilter`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DepthwiseConv2dNativeBackpropFilter functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.DepthwiseConv2dNativeBackpropFilter/fuzz.cpp
[GEN] tf.raw_ops.QuantizedMul -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            Returns x * y element-wise, working on quantized buffers.

Args:
  x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  min_x: A `Tensor` of type `float32`.
    The float value that the lowest quantized `x` value represents.
  max_x: A `Tensor` of type `float32`.
    The float value that the highest quantized `x` value represents.
  min_y: A `Tensor` of type `float32`.
    The float value that the lowest quantized `y` value represents.
  max_y: A `Tensor` of type `float32`.
    The float value that the highest quantized `y` value represents.
  Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (z, min_z, max_z).

  z: A `Tensor` of type `Toutput`.
  min_z: A `Tensor` of type `float32`.
  max_z: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            Returns x * y element-wise, working on quantized buffers.

Args:
  x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  min_x: A `Tensor` of type `float32`.
    The float value that the lowest quantized `x` value represents.
  max_x: A `Tensor` of type `float32`.
    The float value that the highest quantized `x` value represents.
  min_y: A `Tensor` of type `float32`.
    The float value that the lowest quantized `y` value represents.
  max_y: A `Tensor` of type `float32`.
    The float value that the highest quantized `y` value represents.
  Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (z, min_z, max_z).

  z: A `Tensor` of type `Toutput`.
  min_z: A `Tensor` of type `float32`.
  max_z: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.QuantizedMul/fuzz.cpp
[GEN] tf.raw_ops.QuantizedMul -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizedMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizedMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.QuantizedMul/fuzz.cpp
[GEN] tf.raw_ops.QuantizedMul -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            Returns x * y element-wise, working on quantized buffers.

Args:
  x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  min_x: A `Tensor` of type `float32`.
    The float value that the lowest quantized `x` value represents.
  max_x: A `Tensor` of type `float32`.
    The float value that the highest quantized `x` value represents.
  min_y: A `Tensor` of type `float32`.
    The float value that the lowest quantized `y` value represents.
  max_y: A `Tensor` of type `float32`.
    The float value that the highest quantized `y` value represents.
  Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (z, min_z, max_z).

  z: A `Tensor` of type `Toutput`.
  min_z: A `Tensor` of type `float32`.
  max_z: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            Returns x * y element-wise, working on quantized buffers.

Args:
  x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.
  min_x: A `Tensor` of type `float32`.
    The float value that the lowest quantized `x` value represents.
  max_x: A `Tensor` of type `float32`.
    The float value that the highest quantized `x` value represents.
  min_y: A `Tensor` of type `float32`.
    The float value that the lowest quantized `y` value represents.
  max_y: A `Tensor` of type `float32`.
    The float value that the highest quantized `y` value represents.
  Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (z, min_z, max_z).

  z: A `Tensor` of type `Toutput`.
  min_z: A `Tensor` of type `float32`.
  max_z: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.QuantizedMul/fuzz.cpp
[GEN] tf.raw_ops.QuantizedMul -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizedMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizedMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.QuantizedMul/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxArgsGradient -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxArgs operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxArgs operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxArgs operation.
  min: An optional `float`. Defaults to `-6`.
  max: An optional `float`. Defaults to `6`.
  num_bits: An optional `int`. Defaults to `8`.
  narrow_range: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxArgs operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxArgs operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxArgs operation.
  min: An optional `float`. Defaults to `-6`.
  max: An optional `float`. Defaults to `6`.
  num_bits: An optional `int`. Defaults to `8`.
  narrow_range: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FakeQuantWithMinMaxArgsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxArgsGradient -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FakeQuantWithMinMaxArgsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxArgsGradient -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxArgs operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxArgs operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxArgs operation.
  min: An optional `float`. Defaults to `-6`.
  max: An optional `float`. Defaults to `6`.
  num_bits: An optional `int`. Defaults to `8`.
  narrow_range: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            Compute gradients for a FakeQuantWithMinMaxArgs operation.

Args:
  gradients: A `Tensor` of type `float32`.
    Backpropagated gradients above the FakeQuantWithMinMaxArgs operation.
  inputs: A `Tensor` of type `float32`.
    Values passed as inputs to the FakeQuantWithMinMaxArgs operation.
  min: An optional `float`. Defaults to `-6`.
  max: An optional `float`. Defaults to `6`.
  num_bits: An optional `int`. Defaults to `8`.
  narrow_range: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FakeQuantWithMinMaxArgsGradient/fuzz.cpp
[GEN] tf.raw_ops.FakeQuantWithMinMaxArgsGradient -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FakeQuantWithMinMaxArgsGradient`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FakeQuantWithMinMaxArgsGradient functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FakeQuantWithMinMaxArgsGradient/fuzz.cpp
[GEN] tf.raw_ops.InplaceAdd -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            Adds v into specified rows of x.

    Computes y = x; y[i, :] += v; return y.

Args:
  x: A `Tensor`. A `Tensor` of type T.
  i: A `Tensor` of type `int32`.
    A vector. Indices into the left-most dimension of `x`.
  v: A `Tensor`. Must have the same type as `x`.
    A `Tensor` of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InplaceAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InplaceAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            Adds v into specified rows of x.

    Computes y = x; y[i, :] += v; return y.

Args:
  x: A `Tensor`. A `Tensor` of type T.
  i: A `Tensor` of type `int32`.
    A vector. Indices into the left-most dimension of `x`.
  v: A `Tensor`. Must have the same type as `x`.
    A `Tensor` of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InplaceAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InplaceAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.InplaceAdd/fuzz.cpp
[GEN] tf.raw_ops.InplaceAdd -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InplaceAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InplaceAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.InplaceAdd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.InplaceAdd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.InplaceAdd/fuzz.cpp
[GEN] tf.raw_ops.InplaceAdd -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            Adds v into specified rows of x.

    Computes y = x; y[i, :] += v; return y.

Args:
  x: A `Tensor`. A `Tensor` of type T.
  i: A `Tensor` of type `int32`.
    A vector. Indices into the left-most dimension of `x`.
  v: A `Tensor`. Must have the same type as `x`.
    A `Tensor` of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            Adds v into specified rows of x.

    Computes y = x; y[i, :] += v; return y.

Args:
  x: A `Tensor`. A `Tensor` of type T.
  i: A `Tensor` of type `int32`.
    A vector. Indices into the left-most dimension of `x`.
  v: A `Tensor`. Must have the same type as `x`.
    A `Tensor` of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.InplaceAdd/fuzz.cpp
[GEN] tf.raw_ops.InplaceAdd -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.InplaceAdd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.InplaceAdd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.InplaceAdd/fuzz.cpp
[GEN] tf.raw_ops.Inv -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Inv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Inv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Inv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Inv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Inv/fuzz.cpp
[GEN] tf.raw_ops.Inv -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Inv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Inv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Inv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Inv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Inv/fuzz.cpp
[GEN] tf.raw_ops.Inv -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            Computes the reciprocal of x element-wise.

I.e., \\(y = 1 / x\\).

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Inv/fuzz.cpp
[GEN] tf.raw_ops.Inv -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Inv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Inv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Inv/fuzz.cpp
[GEN] tf.raw_ops.ParseTensor -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            Transforms a serialized tensorflow.TensorProto proto into a Tensor.

Args:
  serialized: A `Tensor` of type `string`.
    A scalar string containing a serialized TensorProto proto.
  out_type: A `tf.DType`.
    The type of the serialized tensor.  The provided type must match the
    type of the serialized tensor and no implicit conversion will take place.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `out_type`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ParseTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ParseTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            Transforms a serialized tensorflow.TensorProto proto into a Tensor.

Args:
  serialized: A `Tensor` of type `string`.
    A scalar string containing a serialized TensorProto proto.
  out_type: A `tf.DType`.
    The type of the serialized tensor.  The provided type must match the
    type of the serialized tensor and no implicit conversion will take place.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `out_type`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ParseTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ParseTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ParseTensor/fuzz.cpp
[GEN] tf.raw_ops.ParseTensor -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ParseTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ParseTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ParseTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ParseTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ParseTensor/fuzz.cpp
[GEN] tf.raw_ops.ParseTensor -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            Transforms a serialized tensorflow.TensorProto proto into a Tensor.

Args:
  serialized: A `Tensor` of type `string`.
    A scalar string containing a serialized TensorProto proto.
  out_type: A `tf.DType`.
    The type of the serialized tensor.  The provided type must match the
    type of the serialized tensor and no implicit conversion will take place.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `out_type`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            Transforms a serialized tensorflow.TensorProto proto into a Tensor.

Args:
  serialized: A `Tensor` of type `string`.
    A scalar string containing a serialized TensorProto proto.
  out_type: A `tf.DType`.
    The type of the serialized tensor.  The provided type must match the
    type of the serialized tensor and no implicit conversion will take place.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `out_type`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ParseTensor/fuzz.cpp
[GEN] tf.raw_ops.ParseTensor -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ParseTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ParseTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ParseTensor/fuzz.cpp
[GEN] tf.raw_ops.FractionalAvgPool -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            Performs fractional average pooling on the input.

Fractional average pooling is similar to Fractional max pooling in the pooling
region generation step. The only difference is that after pooling regions are
generated, a mean operation is performed instead of a max operation in each
pooling region.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.
    4-D with shape `[batch, height, width, channels]`.
  pooling_ratio: A list of `floats` that has length `>= 4`.
    Pooling ratio for each dimension of `value`, currently only
    supports row and col dimension and should be >= 1.0. For example, a valid
    pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements
    must be 1.0 because we don't allow pooling on batch and channels
    dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions
    respectively.
  pseudo_random: An optional `bool`. Defaults to `False`.
    When set to True, generates the pooling sequence in a
    pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin
    Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for
    difference between pseudorandom and random.
  overlapping: An optional `bool`. Defaults to `False`.
    When set to True, it means when pooling, the values at the boundary
    of adjacent pooling cells are used by both cells. For example:

    `index  0  1  2  3  4`

    `value  20 5  16 3  7`

    If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.
    The result would be [41/3, 26/3] for fractional avg pooling.
  deterministic: An optional `bool`. Defaults to `False`.
    When set to True, a fixed pooling region will be used when
    iterating over a FractionalAvgPool node in the computation graph. Mainly used
    in unit test to make FractionalAvgPool deterministic.
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).

  output: A `Tensor`. Has the same type as `value`.
  row_pooling_sequence: A `Tensor` of type `int64`.
  col_pooling_sequence: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FractionalAvgPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FractionalAvgPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            Performs fractional average pooling on the input.

Fractional average pooling is similar to Fractional max pooling in the pooling
region generation step. The only difference is that after pooling regions are
generated, a mean operation is performed instead of a max operation in each
pooling region.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.
    4-D with shape `[batch, height, width, channels]`.
  pooling_ratio: A list of `floats` that has length `>= 4`.
    Pooling ratio for each dimension of `value`, currently only
    supports row and col dimension and should be >= 1.0. For example, a valid
    pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements
    must be 1.0 because we don't allow pooling on batch and channels
    dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions
    respectively.
  pseudo_random: An optional `bool`. Defaults to `False`.
    When set to True, generates the pooling sequence in a
    pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin
    Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for
    difference between pseudorandom and random.
  overlapping: An optional `bool`. Defaults to `False`.
    When set to True, it means when pooling, the values at the boundary
    of adjacent pooling cells are used by both cells. For example:

    `index  0  1  2  3  4`

    `value  20 5  16 3  7`

    If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.
    The result would be [41/3, 26/3] for fractional avg pooling.
  deterministic: An optional `bool`. Defaults to `False`.
    When set to True, a fixed pooling region will be used when
    iterating over a FractionalAvgPool node in the computation graph. Mainly used
    in unit test to make FractionalAvgPool deterministic.
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).

  output: A `Tensor`. Has the same type as `value`.
  row_pooling_sequence: A `Tensor` of type `int64`.
  col_pooling_sequence: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FractionalAvgPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FractionalAvgPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FractionalAvgPool/fuzz.cpp
[GEN] tf.raw_ops.FractionalAvgPool -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FractionalAvgPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FractionalAvgPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FractionalAvgPool`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FractionalAvgPool` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FractionalAvgPool/fuzz.cpp
[GEN] tf.raw_ops.FractionalAvgPool -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            Performs fractional average pooling on the input.

Fractional average pooling is similar to Fractional max pooling in the pooling
region generation step. The only difference is that after pooling regions are
generated, a mean operation is performed instead of a max operation in each
pooling region.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.
    4-D with shape `[batch, height, width, channels]`.
  pooling_ratio: A list of `floats` that has length `>= 4`.
    Pooling ratio for each dimension of `value`, currently only
    supports row and col dimension and should be >= 1.0. For example, a valid
    pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements
    must be 1.0 because we don't allow pooling on batch and channels
    dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions
    respectively.
  pseudo_random: An optional `bool`. Defaults to `False`.
    When set to True, generates the pooling sequence in a
    pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin
    Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for
    difference between pseudorandom and random.
  overlapping: An optional `bool`. Defaults to `False`.
    When set to True, it means when pooling, the values at the boundary
    of adjacent pooling cells are used by both cells. For example:

    `index  0  1  2  3  4`

    `value  20 5  16 3  7`

    If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.
    The result would be [41/3, 26/3] for fractional avg pooling.
  deterministic: An optional `bool`. Defaults to `False`.
    When set to True, a fixed pooling region will be used when
    iterating over a FractionalAvgPool node in the computation graph. Mainly used
    in unit test to make FractionalAvgPool deterministic.
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).

  output: A `Tensor`. Has the same type as `value`.
  row_pooling_sequence: A `Tensor` of type `int64`.
  col_pooling_sequence: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            Performs fractional average pooling on the input.

Fractional average pooling is similar to Fractional max pooling in the pooling
region generation step. The only difference is that after pooling regions are
generated, a mean operation is performed instead of a max operation in each
pooling region.

Args:
  value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.
    4-D with shape `[batch, height, width, channels]`.
  pooling_ratio: A list of `floats` that has length `>= 4`.
    Pooling ratio for each dimension of `value`, currently only
    supports row and col dimension and should be >= 1.0. For example, a valid
    pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements
    must be 1.0 because we don't allow pooling on batch and channels
    dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions
    respectively.
  pseudo_random: An optional `bool`. Defaults to `False`.
    When set to True, generates the pooling sequence in a
    pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin
    Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for
    difference between pseudorandom and random.
  overlapping: An optional `bool`. Defaults to `False`.
    When set to True, it means when pooling, the values at the boundary
    of adjacent pooling cells are used by both cells. For example:

    `index  0  1  2  3  4`

    `value  20 5  16 3  7`

    If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.
    The result would be [41/3, 26/3] for fractional avg pooling.
  deterministic: An optional `bool`. Defaults to `False`.
    When set to True, a fixed pooling region will be used when
    iterating over a FractionalAvgPool node in the computation graph. Mainly used
    in unit test to make FractionalAvgPool deterministic.
  seed: An optional `int`. Defaults to `0`.
    If either seed or seed2 are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    An second seed to avoid seed collision.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).

  output: A `Tensor`. Has the same type as `value`.
  row_pooling_sequence: A `Tensor` of type `int64`.
  col_pooling_sequence: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FractionalAvgPool/fuzz.cpp
[GEN] tf.raw_ops.FractionalAvgPool -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FractionalAvgPool`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FractionalAvgPool functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FractionalAvgPool/fuzz.cpp
[GEN] tf.raw_ops.SelectV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  condition: A `Tensor` of type `bool`.
  t: A `Tensor`.
  e: A `Tensor`. Must have the same type as `t`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `t`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SelectV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SelectV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  condition: A `Tensor` of type `bool`.
  t: A `Tensor`.
  e: A `Tensor`. Must have the same type as `t`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `t`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SelectV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SelectV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SelectV2/fuzz.cpp
[GEN] tf.raw_ops.SelectV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SelectV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SelectV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SelectV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SelectV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SelectV2/fuzz.cpp
[GEN] tf.raw_ops.SelectV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  condition: A `Tensor` of type `bool`.
  t: A `Tensor`.
  e: A `Tensor`. Must have the same type as `t`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `t`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  condition: A `Tensor` of type `bool`.
  t: A `Tensor`.
  e: A `Tensor`. Must have the same type as `t`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `t`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SelectV2/fuzz.cpp
[GEN] tf.raw_ops.SelectV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SelectV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SelectV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SelectV2/fuzz.cpp
[GEN] tf.raw_ops.Sum -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a tensor.

Reduces `input` along the dimensions given in `axis`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`axis`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The tensor to reduce.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The dimensions to reduce. Must be in the range
    `[-rank(input), rank(input))`.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a tensor.

Reduces `input` along the dimensions given in `axis`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`axis`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The tensor to reduce.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The dimensions to reduce. Must be in the range
    `[-rank(input), rank(input))`.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Sum/fuzz.cpp
[GEN] tf.raw_ops.Sum -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Sum/fuzz.cpp
[GEN] tf.raw_ops.Sum -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a tensor.

Reduces `input` along the dimensions given in `axis`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`axis`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The tensor to reduce.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The dimensions to reduce. Must be in the range
    `[-rank(input), rank(input))`.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a tensor.

Reduces `input` along the dimensions given in `axis`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`axis`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    The tensor to reduce.
  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    The dimensions to reduce. Must be in the range
    `[-rank(input), rank(input))`.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Sum/fuzz.cpp
[GEN] tf.raw_ops.Sum -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Sum/fuzz.cpp
[GEN] tf.raw_ops.SparseReduceSumSparse -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a SparseTensor.

This Op takes a SparseTensor and is the sparse counterpart to
`tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
SparseTensor.

Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
with length 1.

If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
with a single element is returned.  Additionally, the axes can be negative,
which are interpreted according to the indexing rules in Python.

Args:
  input_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, possibly not in canonical ordering.
  input_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `input_indices`.
  input_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  reduction_axes: A `Tensor` of type `int32`.
    1-D.  Length-`K` vector containing the reduction axes.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values, output_shape).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `input_values`.
  output_shape: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseReduceSumSparse`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseReduceSumSparse` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a SparseTensor.

This Op takes a SparseTensor and is the sparse counterpart to
`tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
SparseTensor.

Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
with length 1.

If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
with a single element is returned.  Additionally, the axes can be negative,
which are interpreted according to the indexing rules in Python.

Args:
  input_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, possibly not in canonical ordering.
  input_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `input_indices`.
  input_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  reduction_axes: A `Tensor` of type `int32`.
    1-D.  Length-`K` vector containing the reduction axes.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values, output_shape).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `input_values`.
  output_shape: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseReduceSumSparse`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseReduceSumSparse` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseReduceSumSparse/fuzz.cpp
[GEN] tf.raw_ops.SparseReduceSumSparse -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseReduceSumSparse`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseReduceSumSparse` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseReduceSumSparse`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseReduceSumSparse` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseReduceSumSparse/fuzz.cpp
[GEN] tf.raw_ops.SparseReduceSumSparse -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a SparseTensor.

This Op takes a SparseTensor and is the sparse counterpart to
`tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
SparseTensor.

Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
with length 1.

If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
with a single element is returned.  Additionally, the axes can be negative,
which are interpreted according to the indexing rules in Python.

Args:
  input_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, possibly not in canonical ordering.
  input_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `input_indices`.
  input_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  reduction_axes: A `Tensor` of type `int32`.
    1-D.  Length-`K` vector containing the reduction axes.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values, output_shape).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `input_values`.
  output_shape: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            Computes the sum of elements across dimensions of a SparseTensor.

This Op takes a SparseTensor and is the sparse counterpart to
`tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
SparseTensor.

Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
with length 1.

If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
with a single element is returned.  Additionally, the axes can be negative,
which are interpreted according to the indexing rules in Python.

Args:
  input_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, possibly not in canonical ordering.
  input_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `input_indices`.
  input_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  reduction_axes: A `Tensor` of type `int32`.
    1-D.  Length-`K` vector containing the reduction axes.
  keep_dims: An optional `bool`. Defaults to `False`.
    If true, retain reduced dimensions with length 1.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values, output_shape).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `input_values`.
  output_shape: A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseReduceSumSparse/fuzz.cpp
[GEN] tf.raw_ops.SparseReduceSumSparse -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseReduceSumSparse`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseReduceSumSparse functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseReduceSumSparse/fuzz.cpp
[GEN] tf.raw_ops.FusedBatchNormGradV3 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            Gradient for batch normalization.

Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.

Args:
  y_backprop: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    A 4D Tensor for the gradient with respect to y.
  x: A `Tensor`. Must have the same type as `y_backprop`.
    A 4D Tensor for input data.
  scale: A `Tensor` of type `float32`.
    A 1D Tensor for scaling factor, to scale the normalized x.
  reserve_space_1: A `Tensor`. Must be one of the following types: `float32`.
    When is_training is True, a 1D Tensor for the computed batch
    mean to be reused in gradient computation. When is_training is
    False, a 1D Tensor for the population mean to be reused in both
    1st and 2nd order gradient computation.
  reserve_space_2: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for the computed batch
    variance (inverted variance in the cuDNN case) to be reused in
    gradient computation. When is_training is False, a 1D Tensor
    for the population variance to be reused in both 1st and 2nd
    order gradient computation.
  reserve_space_3: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for some intermediate results to be reused
    in gradient computation. When is_training is False, a dummy empty Tensor will be
    created.
  epsilon: An optional `float`. Defaults to `0.0001`.
    A small float number added to the variance of x.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NDHWC", "NCDHW"`. Defaults to `"NHWC"`.
    The data format for y_backprop, x, x_backprop.
    Either "NHWC" (default) or "NCHW".
  is_training: An optional `bool`. Defaults to `True`.
    A bool value to indicate the operation is for training (default)
    or inference.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5).

  x_backprop: A `Tensor`. Has the same type as `y_backprop`.
  scale_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  offset_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_4: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_5: A `Tensor`. Has the same type as `reserve_space_1`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedBatchNormGradV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedBatchNormGradV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            Gradient for batch normalization.

Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.

Args:
  y_backprop: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    A 4D Tensor for the gradient with respect to y.
  x: A `Tensor`. Must have the same type as `y_backprop`.
    A 4D Tensor for input data.
  scale: A `Tensor` of type `float32`.
    A 1D Tensor for scaling factor, to scale the normalized x.
  reserve_space_1: A `Tensor`. Must be one of the following types: `float32`.
    When is_training is True, a 1D Tensor for the computed batch
    mean to be reused in gradient computation. When is_training is
    False, a 1D Tensor for the population mean to be reused in both
    1st and 2nd order gradient computation.
  reserve_space_2: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for the computed batch
    variance (inverted variance in the cuDNN case) to be reused in
    gradient computation. When is_training is False, a 1D Tensor
    for the population variance to be reused in both 1st and 2nd
    order gradient computation.
  reserve_space_3: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for some intermediate results to be reused
    in gradient computation. When is_training is False, a dummy empty Tensor will be
    created.
  epsilon: An optional `float`. Defaults to `0.0001`.
    A small float number added to the variance of x.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NDHWC", "NCDHW"`. Defaults to `"NHWC"`.
    The data format for y_backprop, x, x_backprop.
    Either "NHWC" (default) or "NCHW".
  is_training: An optional `bool`. Defaults to `True`.
    A bool value to indicate the operation is for training (default)
    or inference.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5).

  x_backprop: A `Tensor`. Has the same type as `y_backprop`.
  scale_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  offset_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_4: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_5: A `Tensor`. Has the same type as `reserve_space_1`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedBatchNormGradV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedBatchNormGradV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FusedBatchNormGradV3/fuzz.cpp
[GEN] tf.raw_ops.FusedBatchNormGradV3 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedBatchNormGradV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedBatchNormGradV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FusedBatchNormGradV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FusedBatchNormGradV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FusedBatchNormGradV3/fuzz.cpp
[GEN] tf.raw_ops.FusedBatchNormGradV3 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            Gradient for batch normalization.

Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.

Args:
  y_backprop: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    A 4D Tensor for the gradient with respect to y.
  x: A `Tensor`. Must have the same type as `y_backprop`.
    A 4D Tensor for input data.
  scale: A `Tensor` of type `float32`.
    A 1D Tensor for scaling factor, to scale the normalized x.
  reserve_space_1: A `Tensor`. Must be one of the following types: `float32`.
    When is_training is True, a 1D Tensor for the computed batch
    mean to be reused in gradient computation. When is_training is
    False, a 1D Tensor for the population mean to be reused in both
    1st and 2nd order gradient computation.
  reserve_space_2: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for the computed batch
    variance (inverted variance in the cuDNN case) to be reused in
    gradient computation. When is_training is False, a 1D Tensor
    for the population variance to be reused in both 1st and 2nd
    order gradient computation.
  reserve_space_3: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for some intermediate results to be reused
    in gradient computation. When is_training is False, a dummy empty Tensor will be
    created.
  epsilon: An optional `float`. Defaults to `0.0001`.
    A small float number added to the variance of x.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NDHWC", "NCDHW"`. Defaults to `"NHWC"`.
    The data format for y_backprop, x, x_backprop.
    Either "NHWC" (default) or "NCHW".
  is_training: An optional `bool`. Defaults to `True`.
    A bool value to indicate the operation is for training (default)
    or inference.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5).

  x_backprop: A `Tensor`. Has the same type as `y_backprop`.
  scale_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  offset_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_4: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_5: A `Tensor`. Has the same type as `reserve_space_1`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            Gradient for batch normalization.

Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.

Args:
  y_backprop: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.
    A 4D Tensor for the gradient with respect to y.
  x: A `Tensor`. Must have the same type as `y_backprop`.
    A 4D Tensor for input data.
  scale: A `Tensor` of type `float32`.
    A 1D Tensor for scaling factor, to scale the normalized x.
  reserve_space_1: A `Tensor`. Must be one of the following types: `float32`.
    When is_training is True, a 1D Tensor for the computed batch
    mean to be reused in gradient computation. When is_training is
    False, a 1D Tensor for the population mean to be reused in both
    1st and 2nd order gradient computation.
  reserve_space_2: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for the computed batch
    variance (inverted variance in the cuDNN case) to be reused in
    gradient computation. When is_training is False, a 1D Tensor
    for the population variance to be reused in both 1st and 2nd
    order gradient computation.
  reserve_space_3: A `Tensor`. Must have the same type as `reserve_space_1`.
    When is_training is True, a 1D Tensor for some intermediate results to be reused
    in gradient computation. When is_training is False, a dummy empty Tensor will be
    created.
  epsilon: An optional `float`. Defaults to `0.0001`.
    A small float number added to the variance of x.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NDHWC", "NCDHW"`. Defaults to `"NHWC"`.
    The data format for y_backprop, x, x_backprop.
    Either "NHWC" (default) or "NCHW".
  is_training: An optional `bool`. Defaults to `True`.
    A bool value to indicate the operation is for training (default)
    or inference.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5).

  x_backprop: A `Tensor`. Has the same type as `y_backprop`.
  scale_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  offset_backprop: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_4: A `Tensor`. Has the same type as `reserve_space_1`.
  reserve_space_5: A `Tensor`. Has the same type as `reserve_space_1`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FusedBatchNormGradV3/fuzz.cpp
[GEN] tf.raw_ops.FusedBatchNormGradV3 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FusedBatchNormGradV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FusedBatchNormGradV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FusedBatchNormGradV3/fuzz.cpp
[GEN] tf.raw_ops.TruncateDiv -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            Returns x / y element-wise, rounded towards zero.

Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See `FloorDiv` for a division function that matches
Python Semantics.

*NOTE*: `truncatediv` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TruncateDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TruncateDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            Returns x / y element-wise, rounded towards zero.

Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See `FloorDiv` for a division function that matches
Python Semantics.

*NOTE*: `truncatediv` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TruncateDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TruncateDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.TruncateDiv/fuzz.cpp
[GEN] tf.raw_ops.TruncateDiv -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TruncateDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TruncateDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TruncateDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TruncateDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.TruncateDiv/fuzz.cpp
[GEN] tf.raw_ops.TruncateDiv -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            Returns x / y element-wise, rounded towards zero.

Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See `FloorDiv` for a division function that matches
Python Semantics.

*NOTE*: `truncatediv` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            Returns x / y element-wise, rounded towards zero.

Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See `FloorDiv` for a division function that matches
Python Semantics.

*NOTE*: `truncatediv` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.TruncateDiv/fuzz.cpp
[GEN] tf.raw_ops.TruncateDiv -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TruncateDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TruncateDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.TruncateDiv/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiag -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the main diagonal of the
innermost matrices.  These will be overwritten by the values in `diagonal`.

The output is computed as follows:

Assume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has
`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a
tensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:

  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.
  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.

Args:
  input: A `Tensor`. Rank `k+1`, where `k >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `k`, where `k >= 1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiag`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiag` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the main diagonal of the
innermost matrices.  These will be overwritten by the values in `diagonal`.

The output is computed as follows:

Assume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has
`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a
tensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:

  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.
  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.

Args:
  input: A `Tensor`. Rank `k+1`, where `k >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `k`, where `k >= 1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiag`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiag` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MatrixSetDiag/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiag -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiag`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiag` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiag`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiag` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MatrixSetDiag/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiag -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the main diagonal of the
innermost matrices.  These will be overwritten by the values in `diagonal`.

The output is computed as follows:

Assume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has
`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a
tensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:

  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.
  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.

Args:
  input: A `Tensor`. Rank `k+1`, where `k >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `k`, where `k >= 1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the main diagonal of the
innermost matrices.  These will be overwritten by the values in `diagonal`.

The output is computed as follows:

Assume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has
`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a
tensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:

  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.
  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.

Args:
  input: A `Tensor`. Rank `k+1`, where `k >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `k`, where `k >= 1`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MatrixSetDiag/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiag -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiag`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiag functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MatrixSetDiag/fuzz.cpp
[GEN] tf.raw_ops.Softmax -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            Computes softmax activations.

For each batch `i` and class `j` we have

    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$

Args:
  logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    2-D with shape `[batch_size, num_classes]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `logits`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Softmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Softmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            Computes softmax activations.

For each batch `i` and class `j` we have

    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$

Args:
  logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    2-D with shape `[batch_size, num_classes]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `logits`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Softmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Softmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Softmax/fuzz.cpp
[GEN] tf.raw_ops.Softmax -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Softmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Softmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Softmax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Softmax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Softmax/fuzz.cpp
[GEN] tf.raw_ops.Softmax -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            Computes softmax activations.

For each batch `i` and class `j` we have

    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$

Args:
  logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    2-D with shape `[batch_size, num_classes]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `logits`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            Computes softmax activations.

For each batch `i` and class `j` we have

    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$

Args:
  logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.
    2-D with shape `[batch_size, num_classes]`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `logits`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Softmax/fuzz.cpp
[GEN] tf.raw_ops.Softmax -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Softmax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Softmax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Softmax/fuzz.cpp
[GEN] tf.raw_ops.TensorArrayRead -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  handle: A `Tensor` of type mutable `string`.
  index: A `Tensor` of type `int32`.
  flow_in: A `Tensor` of type `float32`.
  dtype: A `tf.DType`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorArrayRead`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorArrayRead` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  handle: A `Tensor` of type mutable `string`.
  index: A `Tensor` of type `int32`.
  flow_in: A `Tensor` of type `float32`.
  dtype: A `tf.DType`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorArrayRead`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorArrayRead` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.TensorArrayRead/fuzz.cpp
[GEN] tf.raw_ops.TensorArrayRead -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorArrayRead`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorArrayRead` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.TensorArrayRead`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.TensorArrayRead` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.TensorArrayRead/fuzz.cpp
[GEN] tf.raw_ops.TensorArrayRead -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  handle: A `Tensor` of type mutable `string`.
  index: A `Tensor` of type `int32`.
  flow_in: A `Tensor` of type `float32`.
  dtype: A `tf.DType`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  handle: A `Tensor` of type mutable `string`.
  index: A `Tensor` of type `int32`.
  flow_in: A `Tensor` of type `float32`.
  dtype: A `tf.DType`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.TensorArrayRead/fuzz.cpp
[GEN] tf.raw_ops.TensorArrayRead -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.TensorArrayRead`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.TensorArrayRead functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.TensorArrayRead/fuzz.cpp
[GEN] tf.raw_ops.AddManySparseToTensorsMap -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.

A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`, where

```sparse_indices.shape[1] == sparse_shape.shape[0] == R```

An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
having a first `sparse_indices` column taking values between `[0, N)`, where
the minibatch size `N == sparse_shape[0]`.

The input `SparseTensor` must have rank `R` greater than 1, and the first
dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
must be sorted in increasing order of this first dimension.  The stored
`SparseTensor` objects pointed to by each row of the output `sparse_handles`
will have rank `R-1`.

The `SparseTensor` values can then be read out as part of a minibatch by passing
the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddManySparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.

Args:
  sparse_indices: A `Tensor` of type `int64`.
    2-D.  The `indices` of the minibatch `SparseTensor`.
    `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
  sparse_values: A `Tensor`.
    1-D.  The `values` of the minibatch `SparseTensor`.
  sparse_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the minibatch `SparseTensor`.
    The minibatch size `N == sparse_shape[0]`.
  container: An optional `string`. Defaults to `""`.
    The container name for the `SparseTensorsMap` created by this op.
  shared_name: An optional `string`. Defaults to `""`.
    The shared name for the `SparseTensorsMap` created by this op.
    If blank, the new Operation's unique name is used.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AddManySparseToTensorsMap`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AddManySparseToTensorsMap` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.

A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`, where

```sparse_indices.shape[1] == sparse_shape.shape[0] == R```

An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
having a first `sparse_indices` column taking values between `[0, N)`, where
the minibatch size `N == sparse_shape[0]`.

The input `SparseTensor` must have rank `R` greater than 1, and the first
dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
must be sorted in increasing order of this first dimension.  The stored
`SparseTensor` objects pointed to by each row of the output `sparse_handles`
will have rank `R-1`.

The `SparseTensor` values can then be read out as part of a minibatch by passing
the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddManySparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.

Args:
  sparse_indices: A `Tensor` of type `int64`.
    2-D.  The `indices` of the minibatch `SparseTensor`.
    `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
  sparse_values: A `Tensor`.
    1-D.  The `values` of the minibatch `SparseTensor`.
  sparse_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the minibatch `SparseTensor`.
    The minibatch size `N == sparse_shape[0]`.
  container: An optional `string`. Defaults to `""`.
    The container name for the `SparseTensorsMap` created by this op.
  shared_name: An optional `string`. Defaults to `""`.
    The shared name for the `SparseTensorsMap` created by this op.
    If blank, the new Operation's unique name is used.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AddManySparseToTensorsMap`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AddManySparseToTensorsMap` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.AddManySparseToTensorsMap/fuzz.cpp
[GEN] tf.raw_ops.AddManySparseToTensorsMap -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AddManySparseToTensorsMap`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AddManySparseToTensorsMap` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AddManySparseToTensorsMap`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AddManySparseToTensorsMap` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.AddManySparseToTensorsMap/fuzz.cpp
[GEN] tf.raw_ops.AddManySparseToTensorsMap -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.

A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`, where

```sparse_indices.shape[1] == sparse_shape.shape[0] == R```

An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
having a first `sparse_indices` column taking values between `[0, N)`, where
the minibatch size `N == sparse_shape[0]`.

The input `SparseTensor` must have rank `R` greater than 1, and the first
dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
must be sorted in increasing order of this first dimension.  The stored
`SparseTensor` objects pointed to by each row of the output `sparse_handles`
will have rank `R-1`.

The `SparseTensor` values can then be read out as part of a minibatch by passing
the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddManySparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.

Args:
  sparse_indices: A `Tensor` of type `int64`.
    2-D.  The `indices` of the minibatch `SparseTensor`.
    `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
  sparse_values: A `Tensor`.
    1-D.  The `values` of the minibatch `SparseTensor`.
  sparse_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the minibatch `SparseTensor`.
    The minibatch size `N == sparse_shape[0]`.
  container: An optional `string`. Defaults to `""`.
    The container name for the `SparseTensorsMap` created by this op.
  shared_name: An optional `string`. Defaults to `""`.
    The shared name for the `SparseTensorsMap` created by this op.
    If blank, the new Operation's unique name is used.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.

A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`, where

```sparse_indices.shape[1] == sparse_shape.shape[0] == R```

An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
having a first `sparse_indices` column taking values between `[0, N)`, where
the minibatch size `N == sparse_shape[0]`.

The input `SparseTensor` must have rank `R` greater than 1, and the first
dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
must be sorted in increasing order of this first dimension.  The stored
`SparseTensor` objects pointed to by each row of the output `sparse_handles`
will have rank `R-1`.

The `SparseTensor` values can then be read out as part of a minibatch by passing
the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddManySparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.

Args:
  sparse_indices: A `Tensor` of type `int64`.
    2-D.  The `indices` of the minibatch `SparseTensor`.
    `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
  sparse_values: A `Tensor`.
    1-D.  The `values` of the minibatch `SparseTensor`.
  sparse_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the minibatch `SparseTensor`.
    The minibatch size `N == sparse_shape[0]`.
  container: An optional `string`. Defaults to `""`.
    The container name for the `SparseTensorsMap` created by this op.
  shared_name: An optional `string`. Defaults to `""`.
    The shared name for the `SparseTensorsMap` created by this op.
    If blank, the new Operation's unique name is used.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int64`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.AddManySparseToTensorsMap/fuzz.cpp
[GEN] tf.raw_ops.AddManySparseToTensorsMap -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AddManySparseToTensorsMap`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AddManySparseToTensorsMap functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.AddManySparseToTensorsMap/fuzz.cpp
[GEN] tf.raw_ops.SparseTensorDenseMatMul -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            Multiply SparseTensor (of rank 2) "A" by dense matrix "B".

No validity checking is performed on the indices of A.  However, the following
input format is recommended for optimal behavior:

if adjoint_a == false:
  A should be sorted in lexicographically increasing order.  Use SparseReorder
  if you're not sure.
if adjoint_a == true:
  A should be sorted in order of increasing dimension 1 (i.e., "column major"
  order instead of "row major" order).

Args:
  a_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
  a_values: A `Tensor`.
    1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
  a_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
  b: A `Tensor`. Must have the same type as `a_values`.
    2-D.  A dense Matrix.
  adjoint_a: An optional `bool`. Defaults to `False`.
    Use the adjoint of A in the matrix multiply.  If A is complex, this
    is transpose(conj(A)).  Otherwise it's transpose(A).
  adjoint_b: An optional `bool`. Defaults to `False`.
    Use the adjoint of B in the matrix multiply.  If B is complex, this
    is transpose(conj(B)).  Otherwise it's transpose(B).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseTensorDenseMatMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseTensorDenseMatMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            Multiply SparseTensor (of rank 2) "A" by dense matrix "B".

No validity checking is performed on the indices of A.  However, the following
input format is recommended for optimal behavior:

if adjoint_a == false:
  A should be sorted in lexicographically increasing order.  Use SparseReorder
  if you're not sure.
if adjoint_a == true:
  A should be sorted in order of increasing dimension 1 (i.e., "column major"
  order instead of "row major" order).

Args:
  a_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
  a_values: A `Tensor`.
    1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
  a_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
  b: A `Tensor`. Must have the same type as `a_values`.
    2-D.  A dense Matrix.
  adjoint_a: An optional `bool`. Defaults to `False`.
    Use the adjoint of A in the matrix multiply.  If A is complex, this
    is transpose(conj(A)).  Otherwise it's transpose(A).
  adjoint_b: An optional `bool`. Defaults to `False`.
    Use the adjoint of B in the matrix multiply.  If B is complex, this
    is transpose(conj(B)).  Otherwise it's transpose(B).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseTensorDenseMatMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseTensorDenseMatMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseTensorDenseMatMul/fuzz.cpp
[GEN] tf.raw_ops.SparseTensorDenseMatMul -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseTensorDenseMatMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseTensorDenseMatMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseTensorDenseMatMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseTensorDenseMatMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseTensorDenseMatMul/fuzz.cpp
[GEN] tf.raw_ops.SparseTensorDenseMatMul -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            Multiply SparseTensor (of rank 2) "A" by dense matrix "B".

No validity checking is performed on the indices of A.  However, the following
input format is recommended for optimal behavior:

if adjoint_a == false:
  A should be sorted in lexicographically increasing order.  Use SparseReorder
  if you're not sure.
if adjoint_a == true:
  A should be sorted in order of increasing dimension 1 (i.e., "column major"
  order instead of "row major" order).

Args:
  a_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
  a_values: A `Tensor`.
    1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
  a_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
  b: A `Tensor`. Must have the same type as `a_values`.
    2-D.  A dense Matrix.
  adjoint_a: An optional `bool`. Defaults to `False`.
    Use the adjoint of A in the matrix multiply.  If A is complex, this
    is transpose(conj(A)).  Otherwise it's transpose(A).
  adjoint_b: An optional `bool`. Defaults to `False`.
    Use the adjoint of B in the matrix multiply.  If B is complex, this
    is transpose(conj(B)).  Otherwise it's transpose(B).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            Multiply SparseTensor (of rank 2) "A" by dense matrix "B".

No validity checking is performed on the indices of A.  However, the following
input format is recommended for optimal behavior:

if adjoint_a == false:
  A should be sorted in lexicographically increasing order.  Use SparseReorder
  if you're not sure.
if adjoint_a == true:
  A should be sorted in order of increasing dimension 1 (i.e., "column major"
  order instead of "row major" order).

Args:
  a_indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
  a_values: A `Tensor`.
    1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
  a_shape: A `Tensor` of type `int64`.
    1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
  b: A `Tensor`. Must have the same type as `a_values`.
    2-D.  A dense Matrix.
  adjoint_a: An optional `bool`. Defaults to `False`.
    Use the adjoint of A in the matrix multiply.  If A is complex, this
    is transpose(conj(A)).  Otherwise it's transpose(A).
  adjoint_b: An optional `bool`. Defaults to `False`.
    Use the adjoint of B in the matrix multiply.  If B is complex, this
    is transpose(conj(B)).  Otherwise it's transpose(B).
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseTensorDenseMatMul/fuzz.cpp
[GEN] tf.raw_ops.SparseTensorDenseMatMul -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseTensorDenseMatMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseTensorDenseMatMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseTensorDenseMatMul/fuzz.cpp
[GEN] tf.raw_ops.SegmentProd -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            Computes the product along segments of a tensor.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Computes a tensor such that
\\(output_i = \prod_j data_j\\) where the product is over `j` such
that `segment_ids[j] == i`.

If the product is empty for a given segment ID `i`, `output[i] = 1`.

Caution: On CPU, values in `segment_ids` are always validated to be sorted,
and an error is thrown for indices that are not increasing. On GPU, this
does not throw an error for unsorted indices. On GPU, out-of-order indices
result in safe but unspecified behavior, which may include treating
out-of-order indices as the same as a smaller following index.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentProd.png" alt>
</div>

For example:

>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)

Args:
  data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor whose size is equal to the size of `data`'s
    first dimension.  Values should be sorted and can be repeated.

    Caution: The values are always validated to be sorted on CPU, never validated
    on GPU.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SegmentProd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SegmentProd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            Computes the product along segments of a tensor.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Computes a tensor such that
\\(output_i = \prod_j data_j\\) where the product is over `j` such
that `segment_ids[j] == i`.

If the product is empty for a given segment ID `i`, `output[i] = 1`.

Caution: On CPU, values in `segment_ids` are always validated to be sorted,
and an error is thrown for indices that are not increasing. On GPU, this
does not throw an error for unsorted indices. On GPU, out-of-order indices
result in safe but unspecified behavior, which may include treating
out-of-order indices as the same as a smaller following index.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentProd.png" alt>
</div>

For example:

>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)

Args:
  data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor whose size is equal to the size of `data`'s
    first dimension.  Values should be sorted and can be repeated.

    Caution: The values are always validated to be sorted on CPU, never validated
    on GPU.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SegmentProd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SegmentProd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SegmentProd/fuzz.cpp
[GEN] tf.raw_ops.SegmentProd -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SegmentProd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SegmentProd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SegmentProd`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SegmentProd` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SegmentProd/fuzz.cpp
[GEN] tf.raw_ops.SegmentProd -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            Computes the product along segments of a tensor.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Computes a tensor such that
\\(output_i = \prod_j data_j\\) where the product is over `j` such
that `segment_ids[j] == i`.

If the product is empty for a given segment ID `i`, `output[i] = 1`.

Caution: On CPU, values in `segment_ids` are always validated to be sorted,
and an error is thrown for indices that are not increasing. On GPU, this
does not throw an error for unsorted indices. On GPU, out-of-order indices
result in safe but unspecified behavior, which may include treating
out-of-order indices as the same as a smaller following index.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentProd.png" alt>
</div>

For example:

>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)

Args:
  data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor whose size is equal to the size of `data`'s
    first dimension.  Values should be sorted and can be repeated.

    Caution: The values are always validated to be sorted on CPU, never validated
    on GPU.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            Computes the product along segments of a tensor.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Computes a tensor such that
\\(output_i = \prod_j data_j\\) where the product is over `j` such
that `segment_ids[j] == i`.

If the product is empty for a given segment ID `i`, `output[i] = 1`.

Caution: On CPU, values in `segment_ids` are always validated to be sorted,
and an error is thrown for indices that are not increasing. On GPU, this
does not throw an error for unsorted indices. On GPU, out-of-order indices
result in safe but unspecified behavior, which may include treating
out-of-order indices as the same as a smaller following index.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentProd.png" alt>
</div>

For example:

>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)

Args:
  data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor whose size is equal to the size of `data`'s
    first dimension.  Values should be sorted and can be repeated.

    Caution: The values are always validated to be sorted on CPU, never validated
    on GPU.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SegmentProd/fuzz.cpp
[GEN] tf.raw_ops.SegmentProd -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SegmentProd`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SegmentProd functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SegmentProd/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MaxPoolV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MaxPoolV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MaxPoolV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MaxPoolV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            Performs max pooling on the input.

Args:
  input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.
    4-D input to pool over.
  ksize: A `Tensor` of type `int32`.
    The size of the window for each dimension of the input tensor.
  strides: A `Tensor` of type `int32`.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  data_format: An optional `string` from: `"NHWC", "NCHW", "NCHW_VECT_C"`. Defaults to `"NHWC"`.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MaxPoolV2/fuzz.cpp
[GEN] tf.raw_ops.MaxPoolV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MaxPoolV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MaxPoolV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MaxPoolV2/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentMeanWithNumSegments -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            Computes the mean along sparse segments of a tensor.

Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentMeanWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentMeanWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            Computes the mean along sparse segments of a tensor.

Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentMeanWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentMeanWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSegmentMeanWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentMeanWithNumSegments -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentMeanWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentMeanWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentMeanWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentMeanWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSegmentMeanWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentMeanWithNumSegments -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            Computes the mean along sparse segments of a tensor.

Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            Computes the mean along sparse segments of a tensor.

Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSegmentMeanWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentMeanWithNumSegments -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentMeanWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentMeanWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSegmentMeanWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.QuantizeV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.

[min_range, max_range] are scalar floats that specify the range for
the 'input' data. The 'mode' attribute controls exactly which calculations are
used to convert the float values to their quantized equivalents.  The
'round_mode' attribute controls which rounding tie-breaking algorithm is used
when rounding float values to their quantized equivalents.

In 'MIN_COMBINED' mode, each value of the tensor will undergo the following:

```
out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
```

here `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`

*MIN_COMBINED Mode Example*

Assume the input is type float and has a possible range of [0.0, 6.0] and the
output type is quint8 ([0, 255]). The min_range and max_range values should be
specified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each
value of the input by 255/6 and cast to quint8.

If the output type was qint8 ([-128, 127]), the operation will additionally
subtract each value by 128 prior to casting, so that the range of values aligns
with the range of qint8.

If the mode is 'MIN_FIRST', then this approach is used:

```
num_discrete_values = 1 << (# of bits in T)
range_adjust = num_discrete_values / (num_discrete_values - 1)
range = (range_max - range_min) * range_adjust
range_scale = num_discrete_values / range
quantized = round(input * range_scale) - round(range_min * range_scale) +
  numeric_limits<T>::min()
quantized = max(quantized, numeric_limits<T>::min())
quantized = min(quantized, numeric_limits<T>::max())
```

The biggest difference between this and MIN_COMBINED is that the minimum range
is rounded first, before it's subtracted from the rounded value. With
MIN_COMBINED, a small bias is introduced where repeated iterations of quantizing
and dequantizing will introduce a larger and larger error.

*SCALED mode Example*

`SCALED` mode matches the quantization approach used in
`QuantizeAndDequantize{V2|V3}`.

If the mode is `SCALED`, the quantization is performed by multiplying each
input value by a scaling_factor.
The scaling_factor is determined from `min_range` and `max_range` to be as large
as possible such that the range from `min_range` to `max_range` is representable
within values of type T.

```c++

  const int min_T = std::numeric_limits<T>::min();
  const int max_T = std::numeric_limits<T>::max();
  const float max_float = std::numeric_limits<float>::max();

  const float scale_factor_from_min_side =
      (min_T * min_range > 0) ? min_T / min_range : max_float;
  const float scale_factor_from_max_side =
      (max_T * max_range > 0) ? max_T / max_range : max_float;

  const float scale_factor = std::min(scale_factor_from_min_side,
                                      scale_factor_from_max_side);
```

We next use the scale_factor to adjust min_range and max_range as follows:

```c++
      min_range = min_T / scale_factor;
      max_range = max_T / scale_factor;
```


e.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would
compare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8
In this case, min_range would remain -10, but max_range would be adjusted to
127 / 12.8 = 9.921875

So we will quantize input values in the range (-10, 9.921875) to (-128, 127).

The input tensor can now be quantized by clipping values to the range
`min_range` to `max_range`, then multiplying by scale_factor as follows:

```c++
result = round(min(max_range, max(min_range, input)) * scale_factor)
```

The adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of
this operation. These outputs should be used as the range for any further
calculations.


*narrow_range (bool) attribute*

If true, we do not use the minimum quantized value.
i.e. for int8 the quantized output, it would be restricted to the range
-127..127 instead of the full -128..127 range.
This is provided for compatibility with certain inference backends.
(Only applies to SCALED mode)


*axis (int) attribute*

An optional `axis` attribute can specify a dimension index of the input tensor,
such that quantization ranges will be calculated and applied separately for each
slice of the tensor along that dimension. This is useful for per-channel
quantization.

If axis is specified, min_range and max_range

if `axis`=None, per-tensor quantization is performed as normal.


*ensure_minimum_range (float) attribute*

Ensures the minimum quantization range is at least this value.
The legacy default value for this is 0.01, but it is strongly suggested to
set it to 0 for new uses.

Args:
  input: A `Tensor` of type `float32`.
  min_range: A `Tensor` of type `float32`.
    The minimum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_min`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  max_range: A `Tensor` of type `float32`.
    The maximum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_max`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  T: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.
  mode: An optional `string` from: `"MIN_COMBINED", "MIN_FIRST", "SCALED"`. Defaults to `"MIN_COMBINED"`.
  round_mode: An optional `string` from: `"HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"`. Defaults to `"HALF_AWAY_FROM_ZERO"`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  ensure_minimum_range: An optional `float`. Defaults to `0.01`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, output_min, output_max).

  output: A `Tensor` of type `T`.
  output_min: A `Tensor` of type `float32`.
  output_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.

[min_range, max_range] are scalar floats that specify the range for
the 'input' data. The 'mode' attribute controls exactly which calculations are
used to convert the float values to their quantized equivalents.  The
'round_mode' attribute controls which rounding tie-breaking algorithm is used
when rounding float values to their quantized equivalents.

In 'MIN_COMBINED' mode, each value of the tensor will undergo the following:

```
out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
```

here `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`

*MIN_COMBINED Mode Example*

Assume the input is type float and has a possible range of [0.0, 6.0] and the
output type is quint8 ([0, 255]). The min_range and max_range values should be
specified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each
value of the input by 255/6 and cast to quint8.

If the output type was qint8 ([-128, 127]), the operation will additionally
subtract each value by 128 prior to casting, so that the range of values aligns
with the range of qint8.

If the mode is 'MIN_FIRST', then this approach is used:

```
num_discrete_values = 1 << (# of bits in T)
range_adjust = num_discrete_values / (num_discrete_values - 1)
range = (range_max - range_min) * range_adjust
range_scale = num_discrete_values / range
quantized = round(input * range_scale) - round(range_min * range_scale) +
  numeric_limits<T>::min()
quantized = max(quantized, numeric_limits<T>::min())
quantized = min(quantized, numeric_limits<T>::max())
```

The biggest difference between this and MIN_COMBINED is that the minimum range
is rounded first, before it's subtracted from the rounded value. With
MIN_COMBINED, a small bias is introduced where repeated iterations of quantizing
and dequantizing will introduce a larger and larger error.

*SCALED mode Example*

`SCALED` mode matches the quantization approach used in
`QuantizeAndDequantize{V2|V3}`.

If the mode is `SCALED`, the quantization is performed by multiplying each
input value by a scaling_factor.
The scaling_factor is determined from `min_range` and `max_range` to be as large
as possible such that the range from `min_range` to `max_range` is representable
within values of type T.

```c++

  const int min_T = std::numeric_limits<T>::min();
  const int max_T = std::numeric_limits<T>::max();
  const float max_float = std::numeric_limits<float>::max();

  const float scale_factor_from_min_side =
      (min_T * min_range > 0) ? min_T / min_range : max_float;
  const float scale_factor_from_max_side =
      (max_T * max_range > 0) ? max_T / max_range : max_float;

  const float scale_factor = std::min(scale_factor_from_min_side,
                                      scale_factor_from_max_side);
```

We next use the scale_factor to adjust min_range and max_range as follows:

```c++
      min_range = min_T / scale_factor;
      max_range = max_T / scale_factor;
```


e.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would
compare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8
In this case, min_range would remain -10, but max_range would be adjusted to
127 / 12.8 = 9.921875

So we will quantize input values in the range (-10, 9.921875) to (-128, 127).

The input tensor can now be quantized by clipping values to the range
`min_range` to `max_range`, then multiplying by scale_factor as follows:

```c++
result = round(min(max_range, max(min_range, input)) * scale_factor)
```

The adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of
this operation. These outputs should be used as the range for any further
calculations.


*narrow_range (bool) attribute*

If true, we do not use the minimum quantized value.
i.e. for int8 the quantized output, it would be restricted to the range
-127..127 instead of the full -128..127 range.
This is provided for compatibility with certain inference backends.
(Only applies to SCALED mode)


*axis (int) attribute*

An optional `axis` attribute can specify a dimension index of the input tensor,
such that quantization ranges will be calculated and applied separately for each
slice of the tensor along that dimension. This is useful for per-channel
quantization.

If axis is specified, min_range and max_range

if `axis`=None, per-tensor quantization is performed as normal.


*ensure_minimum_range (float) attribute*

Ensures the minimum quantization range is at least this value.
The legacy default value for this is 0.01, but it is strongly suggested to
set it to 0 for new uses.

Args:
  input: A `Tensor` of type `float32`.
  min_range: A `Tensor` of type `float32`.
    The minimum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_min`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  max_range: A `Tensor` of type `float32`.
    The maximum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_max`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  T: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.
  mode: An optional `string` from: `"MIN_COMBINED", "MIN_FIRST", "SCALED"`. Defaults to `"MIN_COMBINED"`.
  round_mode: An optional `string` from: `"HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"`. Defaults to `"HALF_AWAY_FROM_ZERO"`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  ensure_minimum_range: An optional `float`. Defaults to `0.01`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, output_min, output_max).

  output: A `Tensor` of type `T`.
  output_min: A `Tensor` of type `float32`.
  output_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.QuantizeV2/fuzz.cpp
[GEN] tf.raw_ops.QuantizeV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.QuantizeV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.QuantizeV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.QuantizeV2/fuzz.cpp
[GEN] tf.raw_ops.QuantizeV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.

[min_range, max_range] are scalar floats that specify the range for
the 'input' data. The 'mode' attribute controls exactly which calculations are
used to convert the float values to their quantized equivalents.  The
'round_mode' attribute controls which rounding tie-breaking algorithm is used
when rounding float values to their quantized equivalents.

In 'MIN_COMBINED' mode, each value of the tensor will undergo the following:

```
out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
```

here `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`

*MIN_COMBINED Mode Example*

Assume the input is type float and has a possible range of [0.0, 6.0] and the
output type is quint8 ([0, 255]). The min_range and max_range values should be
specified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each
value of the input by 255/6 and cast to quint8.

If the output type was qint8 ([-128, 127]), the operation will additionally
subtract each value by 128 prior to casting, so that the range of values aligns
with the range of qint8.

If the mode is 'MIN_FIRST', then this approach is used:

```
num_discrete_values = 1 << (# of bits in T)
range_adjust = num_discrete_values / (num_discrete_values - 1)
range = (range_max - range_min) * range_adjust
range_scale = num_discrete_values / range
quantized = round(input * range_scale) - round(range_min * range_scale) +
  numeric_limits<T>::min()
quantized = max(quantized, numeric_limits<T>::min())
quantized = min(quantized, numeric_limits<T>::max())
```

The biggest difference between this and MIN_COMBINED is that the minimum range
is rounded first, before it's subtracted from the rounded value. With
MIN_COMBINED, a small bias is introduced where repeated iterations of quantizing
and dequantizing will introduce a larger and larger error.

*SCALED mode Example*

`SCALED` mode matches the quantization approach used in
`QuantizeAndDequantize{V2|V3}`.

If the mode is `SCALED`, the quantization is performed by multiplying each
input value by a scaling_factor.
The scaling_factor is determined from `min_range` and `max_range` to be as large
as possible such that the range from `min_range` to `max_range` is representable
within values of type T.

```c++

  const int min_T = std::numeric_limits<T>::min();
  const int max_T = std::numeric_limits<T>::max();
  const float max_float = std::numeric_limits<float>::max();

  const float scale_factor_from_min_side =
      (min_T * min_range > 0) ? min_T / min_range : max_float;
  const float scale_factor_from_max_side =
      (max_T * max_range > 0) ? max_T / max_range : max_float;

  const float scale_factor = std::min(scale_factor_from_min_side,
                                      scale_factor_from_max_side);
```

We next use the scale_factor to adjust min_range and max_range as follows:

```c++
      min_range = min_T / scale_factor;
      max_range = max_T / scale_factor;
```


e.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would
compare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8
In this case, min_range would remain -10, but max_range would be adjusted to
127 / 12.8 = 9.921875

So we will quantize input values in the range (-10, 9.921875) to (-128, 127).

The input tensor can now be quantized by clipping values to the range
`min_range` to `max_range`, then multiplying by scale_factor as follows:

```c++
result = round(min(max_range, max(min_range, input)) * scale_factor)
```

The adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of
this operation. These outputs should be used as the range for any further
calculations.


*narrow_range (bool) attribute*

If true, we do not use the minimum quantized value.
i.e. for int8 the quantized output, it would be restricted to the range
-127..127 instead of the full -128..127 range.
This is provided for compatibility with certain inference backends.
(Only applies to SCALED mode)


*axis (int) attribute*

An optional `axis` attribute can specify a dimension index of the input tensor,
such that quantization ranges will be calculated and applied separately for each
slice of the tensor along that dimension. This is useful for per-channel
quantization.

If axis is specified, min_range and max_range

if `axis`=None, per-tensor quantization is performed as normal.


*ensure_minimum_range (float) attribute*

Ensures the minimum quantization range is at least this value.
The legacy default value for this is 0.01, but it is strongly suggested to
set it to 0 for new uses.

Args:
  input: A `Tensor` of type `float32`.
  min_range: A `Tensor` of type `float32`.
    The minimum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_min`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  max_range: A `Tensor` of type `float32`.
    The maximum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_max`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  T: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.
  mode: An optional `string` from: `"MIN_COMBINED", "MIN_FIRST", "SCALED"`. Defaults to `"MIN_COMBINED"`.
  round_mode: An optional `string` from: `"HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"`. Defaults to `"HALF_AWAY_FROM_ZERO"`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  ensure_minimum_range: An optional `float`. Defaults to `0.01`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, output_min, output_max).

  output: A `Tensor` of type `T`.
  output_min: A `Tensor` of type `float32`.
  output_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.

[min_range, max_range] are scalar floats that specify the range for
the 'input' data. The 'mode' attribute controls exactly which calculations are
used to convert the float values to their quantized equivalents.  The
'round_mode' attribute controls which rounding tie-breaking algorithm is used
when rounding float values to their quantized equivalents.

In 'MIN_COMBINED' mode, each value of the tensor will undergo the following:

```
out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
```

here `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`

*MIN_COMBINED Mode Example*

Assume the input is type float and has a possible range of [0.0, 6.0] and the
output type is quint8 ([0, 255]). The min_range and max_range values should be
specified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each
value of the input by 255/6 and cast to quint8.

If the output type was qint8 ([-128, 127]), the operation will additionally
subtract each value by 128 prior to casting, so that the range of values aligns
with the range of qint8.

If the mode is 'MIN_FIRST', then this approach is used:

```
num_discrete_values = 1 << (# of bits in T)
range_adjust = num_discrete_values / (num_discrete_values - 1)
range = (range_max - range_min) * range_adjust
range_scale = num_discrete_values / range
quantized = round(input * range_scale) - round(range_min * range_scale) +
  numeric_limits<T>::min()
quantized = max(quantized, numeric_limits<T>::min())
quantized = min(quantized, numeric_limits<T>::max())
```

The biggest difference between this and MIN_COMBINED is that the minimum range
is rounded first, before it's subtracted from the rounded value. With
MIN_COMBINED, a small bias is introduced where repeated iterations of quantizing
and dequantizing will introduce a larger and larger error.

*SCALED mode Example*

`SCALED` mode matches the quantization approach used in
`QuantizeAndDequantize{V2|V3}`.

If the mode is `SCALED`, the quantization is performed by multiplying each
input value by a scaling_factor.
The scaling_factor is determined from `min_range` and `max_range` to be as large
as possible such that the range from `min_range` to `max_range` is representable
within values of type T.

```c++

  const int min_T = std::numeric_limits<T>::min();
  const int max_T = std::numeric_limits<T>::max();
  const float max_float = std::numeric_limits<float>::max();

  const float scale_factor_from_min_side =
      (min_T * min_range > 0) ? min_T / min_range : max_float;
  const float scale_factor_from_max_side =
      (max_T * max_range > 0) ? max_T / max_range : max_float;

  const float scale_factor = std::min(scale_factor_from_min_side,
                                      scale_factor_from_max_side);
```

We next use the scale_factor to adjust min_range and max_range as follows:

```c++
      min_range = min_T / scale_factor;
      max_range = max_T / scale_factor;
```


e.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would
compare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8
In this case, min_range would remain -10, but max_range would be adjusted to
127 / 12.8 = 9.921875

So we will quantize input values in the range (-10, 9.921875) to (-128, 127).

The input tensor can now be quantized by clipping values to the range
`min_range` to `max_range`, then multiplying by scale_factor as follows:

```c++
result = round(min(max_range, max(min_range, input)) * scale_factor)
```

The adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of
this operation. These outputs should be used as the range for any further
calculations.


*narrow_range (bool) attribute*

If true, we do not use the minimum quantized value.
i.e. for int8 the quantized output, it would be restricted to the range
-127..127 instead of the full -128..127 range.
This is provided for compatibility with certain inference backends.
(Only applies to SCALED mode)


*axis (int) attribute*

An optional `axis` attribute can specify a dimension index of the input tensor,
such that quantization ranges will be calculated and applied separately for each
slice of the tensor along that dimension. This is useful for per-channel
quantization.

If axis is specified, min_range and max_range

if `axis`=None, per-tensor quantization is performed as normal.


*ensure_minimum_range (float) attribute*

Ensures the minimum quantization range is at least this value.
The legacy default value for this is 0.01, but it is strongly suggested to
set it to 0 for new uses.

Args:
  input: A `Tensor` of type `float32`.
  min_range: A `Tensor` of type `float32`.
    The minimum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_min`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  max_range: A `Tensor` of type `float32`.
    The maximum value of the quantization range. This value may be adjusted by the
    op depending on other parameters. The adjusted value is written to `output_max`.
    If the `axis` attribute is specified, this must be a 1-D tensor whose size
    matches the `axis` dimension of the input and output tensors.
  T: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.
  mode: An optional `string` from: `"MIN_COMBINED", "MIN_FIRST", "SCALED"`. Defaults to `"MIN_COMBINED"`.
  round_mode: An optional `string` from: `"HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"`. Defaults to `"HALF_AWAY_FROM_ZERO"`.
  narrow_range: An optional `bool`. Defaults to `False`.
  axis: An optional `int`. Defaults to `-1`.
  ensure_minimum_range: An optional `float`. Defaults to `0.01`.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output, output_min, output_max).

  output: A `Tensor` of type `T`.
  output_min: A `Tensor` of type `float32`.
  output_max: A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.QuantizeV2/fuzz.cpp
[GEN] tf.raw_ops.QuantizeV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.QuantizeV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.QuantizeV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.QuantizeV2/fuzz.cpp
[GEN] tf.raw_ops.ImmutableConst -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            Returns immutable tensor from memory region.

The current implementation memmaps the tensor from a file.

Args:
  dtype: A `tf.DType`. Type of the returned tensor.
  shape: A `tf.TensorShape` or list of `ints`. Shape of the returned tensor.
  memory_region_name: A `string`.
    Name of readonly memory region used by the tensor, see
    NewReadOnlyMemoryRegionFromFile in tensorflow::Env.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ImmutableConst`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ImmutableConst` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            Returns immutable tensor from memory region.

The current implementation memmaps the tensor from a file.

Args:
  dtype: A `tf.DType`. Type of the returned tensor.
  shape: A `tf.TensorShape` or list of `ints`. Shape of the returned tensor.
  memory_region_name: A `string`.
    Name of readonly memory region used by the tensor, see
    NewReadOnlyMemoryRegionFromFile in tensorflow::Env.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ImmutableConst`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ImmutableConst` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ImmutableConst/fuzz.cpp
[GEN] tf.raw_ops.ImmutableConst -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ImmutableConst`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ImmutableConst` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ImmutableConst`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ImmutableConst` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ImmutableConst/fuzz.cpp
[GEN] tf.raw_ops.ImmutableConst -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            Returns immutable tensor from memory region.

The current implementation memmaps the tensor from a file.

Args:
  dtype: A `tf.DType`. Type of the returned tensor.
  shape: A `tf.TensorShape` or list of `ints`. Shape of the returned tensor.
  memory_region_name: A `string`.
    Name of readonly memory region used by the tensor, see
    NewReadOnlyMemoryRegionFromFile in tensorflow::Env.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            Returns immutable tensor from memory region.

The current implementation memmaps the tensor from a file.

Args:
  dtype: A `tf.DType`. Type of the returned tensor.
  shape: A `tf.TensorShape` or list of `ints`. Shape of the returned tensor.
  memory_region_name: A `string`.
    Name of readonly memory region used by the tensor, see
    NewReadOnlyMemoryRegionFromFile in tensorflow::Env.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ImmutableConst/fuzz.cpp
[GEN] tf.raw_ops.ImmutableConst -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ImmutableConst`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ImmutableConst functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ImmutableConst/fuzz.cpp
[GEN] tf.raw_ops.SerializeTensor -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            Transforms a Tensor into a serialized TensorProto proto.

Args:
  tensor: A `Tensor`. A Tensor of type `T`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SerializeTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SerializeTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            Transforms a Tensor into a serialized TensorProto proto.

Args:
  tensor: A `Tensor`. A Tensor of type `T`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SerializeTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SerializeTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SerializeTensor/fuzz.cpp
[GEN] tf.raw_ops.SerializeTensor -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SerializeTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SerializeTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SerializeTensor`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SerializeTensor` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SerializeTensor/fuzz.cpp
[GEN] tf.raw_ops.SerializeTensor -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            Transforms a Tensor into a serialized TensorProto proto.

Args:
  tensor: A `Tensor`. A Tensor of type `T`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            Transforms a Tensor into a serialized TensorProto proto.

Args:
  tensor: A `Tensor`. A Tensor of type `T`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SerializeTensor/fuzz.cpp
[GEN] tf.raw_ops.SerializeTensor -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SerializeTensor`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SerializeTensor functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SerializeTensor/fuzz.cpp
[GEN] tf.raw_ops.Sin -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            Computes sine of x element-wise.

  Given an input tensor, this function computes sine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10, float("inf")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            Computes sine of x element-wise.

  Given an input tensor, this function computes sine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10, float("inf")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Sin/fuzz.cpp
[GEN] tf.raw_ops.Sin -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Sin`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Sin` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Sin/fuzz.cpp
[GEN] tf.raw_ops.Sin -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            Computes sine of x element-wise.

  Given an input tensor, this function computes sine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10, float("inf")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            Computes sine of x element-wise.

  Given an input tensor, this function computes sine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10, float("inf")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Sin/fuzz.cpp
[GEN] tf.raw_ops.Sin -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Sin`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Sin functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Sin/fuzz.cpp
[GEN] tf.raw_ops.SparseSparseMaximum -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            Returns the element-wise max of two SparseTensors.

Assumes the two SparseTensors have the same shape, i.e., no broadcasting.

Args:
  a_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, in the canonical lexicographic ordering.
  a_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `a_indices`.
  a_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  b_indices: A `Tensor` of type `int64`.
    counterpart to `a_indices` for the other operand.
  b_values: A `Tensor`. Must have the same type as `a_values`.
    counterpart to `a_values` for the other operand; must be of the same dtype.
  b_shape: A `Tensor` of type `int64`.
    counterpart to `a_shape` for the other operand; the two shapes must be equal.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSparseMaximum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSparseMaximum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            Returns the element-wise max of two SparseTensors.

Assumes the two SparseTensors have the same shape, i.e., no broadcasting.

Args:
  a_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, in the canonical lexicographic ordering.
  a_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `a_indices`.
  a_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  b_indices: A `Tensor` of type `int64`.
    counterpart to `a_indices` for the other operand.
  b_values: A `Tensor`. Must have the same type as `a_values`.
    counterpart to `a_values` for the other operand; must be of the same dtype.
  b_shape: A `Tensor` of type `int64`.
    counterpart to `a_shape` for the other operand; the two shapes must be equal.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSparseMaximum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSparseMaximum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSparseMaximum/fuzz.cpp
[GEN] tf.raw_ops.SparseSparseMaximum -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSparseMaximum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSparseMaximum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSparseMaximum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSparseMaximum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSparseMaximum/fuzz.cpp
[GEN] tf.raw_ops.SparseSparseMaximum -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            Returns the element-wise max of two SparseTensors.

Assumes the two SparseTensors have the same shape, i.e., no broadcasting.

Args:
  a_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, in the canonical lexicographic ordering.
  a_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `a_indices`.
  a_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  b_indices: A `Tensor` of type `int64`.
    counterpart to `a_indices` for the other operand.
  b_values: A `Tensor`. Must have the same type as `a_values`.
    counterpart to `a_values` for the other operand; must be of the same dtype.
  b_shape: A `Tensor` of type `int64`.
    counterpart to `a_shape` for the other operand; the two shapes must be equal.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            Returns the element-wise max of two SparseTensors.

Assumes the two SparseTensors have the same shape, i.e., no broadcasting.

Args:
  a_indices: A `Tensor` of type `int64`.
    2-D.  `N x R` matrix with the indices of non-empty values in a
    SparseTensor, in the canonical lexicographic ordering.
  a_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    1-D.  `N` non-empty values corresponding to `a_indices`.
  a_shape: A `Tensor` of type `int64`.
    1-D.  Shape of the input SparseTensor.
  b_indices: A `Tensor` of type `int64`.
    counterpart to `a_indices` for the other operand.
  b_values: A `Tensor`. Must have the same type as `a_values`.
    counterpart to `a_values` for the other operand; must be of the same dtype.
  b_shape: A `Tensor` of type `int64`.
    counterpart to `a_shape` for the other operand; the two shapes must be equal.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (output_indices, output_values).

  output_indices: A `Tensor` of type `int64`.
  output_values: A `Tensor`. Has the same type as `a_values`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSparseMaximum/fuzz.cpp
[GEN] tf.raw_ops.SparseSparseMaximum -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSparseMaximum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSparseMaximum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSparseMaximum/fuzz.cpp
[GEN] tf.raw_ops.MapClear -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            Op removes all elements in the underlying container.

Args:
  dtypes: A list of `tf.DTypes`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapClear`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapClear` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            Op removes all elements in the underlying container.

Args:
  dtypes: A list of `tf.DTypes`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapClear`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapClear` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MapClear/fuzz.cpp
[GEN] tf.raw_ops.MapClear -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapClear`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapClear` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapClear`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapClear` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MapClear/fuzz.cpp
[GEN] tf.raw_ops.MapClear -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            Op removes all elements in the underlying container.

Args:
  dtypes: A list of `tf.DTypes`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            Op removes all elements in the underlying container.

Args:
  dtypes: A list of `tf.DTypes`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MapClear/fuzz.cpp
[GEN] tf.raw_ops.MapClear -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapClear`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapClear functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MapClear/fuzz.cpp
[GEN] tf.raw_ops.Cos -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            Computes cos of x element-wise.

  Given an input tensor, this function computes cosine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`. If input lies outside the boundary, `nan`
  is returned.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10000, float("inf")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cos`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cos` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            Computes cos of x element-wise.

  Given an input tensor, this function computes cosine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`. If input lies outside the boundary, `nan`
  is returned.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10000, float("inf")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cos`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cos` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Cos/fuzz.cpp
[GEN] tf.raw_ops.Cos -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cos`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cos` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Cos`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Cos` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Cos/fuzz.cpp
[GEN] tf.raw_ops.Cos -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            Computes cos of x element-wise.

  Given an input tensor, this function computes cosine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`. If input lies outside the boundary, `nan`
  is returned.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10000, float("inf")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            Computes cos of x element-wise.

  Given an input tensor, this function computes cosine of every
  element in the tensor. Input range is `(-inf, inf)` and
  output range is `[-1,1]`. If input lies outside the boundary, `nan`
  is returned.

  ```python
  x = tf.constant([-float("inf"), -9, -0.5, 1, 1.2, 200, 10000, float("inf")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
  ```

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Cos/fuzz.cpp
[GEN] tf.raw_ops.Cos -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Cos`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Cos functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Cos/fuzz.cpp
[GEN] tf.raw_ops.ScatterMax -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            Reduces sparse updates into a variable reference using the `max` operation.

This operation computes

    # Scalar indices
    ref[indices, ...] = max(ref[indices, ...], updates[...])

    # Vector indices (for each i)
    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions combine.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterAdd.png" alt>
</div>

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to reduce into `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the update will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            Reduces sparse updates into a variable reference using the `max` operation.

This operation computes

    # Scalar indices
    ref[indices, ...] = max(ref[indices, ...], updates[...])

    # Vector indices (for each i)
    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions combine.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterAdd.png" alt>
</div>

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to reduce into `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the update will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ScatterMax/fuzz.cpp
[GEN] tf.raw_ops.ScatterMax -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMax`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMax` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ScatterMax/fuzz.cpp
[GEN] tf.raw_ops.ScatterMax -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            Reduces sparse updates into a variable reference using the `max` operation.

This operation computes

    # Scalar indices
    ref[indices, ...] = max(ref[indices, ...], updates[...])

    # Vector indices (for each i)
    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions combine.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterAdd.png" alt>
</div>

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to reduce into `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the update will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            Reduces sparse updates into a variable reference using the `max` operation.

This operation computes

    # Scalar indices
    ref[indices, ...] = max(ref[indices, ...], updates[...])

    # Vector indices (for each i)
    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions combine.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterAdd.png" alt>
</div>

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to reduce into `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the update will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ScatterMax/fuzz.cpp
[GEN] tf.raw_ops.ScatterMax -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMax`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMax functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ScatterMax/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiagV3 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the specified diagonals of the
innermost matrices. These will be overwritten by the values in `diagonal`.

`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or
`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.
Otherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.
`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.
`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,
`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`

The output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.
If `k` is scalar or `k[0] == k[1]`:

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
```

Otherwise,

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]
    input[i, j, ..., l, m, n]                         ; otherwise
```
where `d = n - m`, `diag_index = k[1] - d`, and
`index_in_diag = n - max(d, 0) + offset`.

`offset` is zero except when the alignment of the diagonal is to the right.
```
offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}
                                           and `d >= 0`) or
                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}
                                           and `d <= 0`)
         0                          ; otherwise
```
where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.

For example:

```
# The main diagonal.
input = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]],
                  [[7, 7, 7, 7],
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]]])
diagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)
                     [4, 5, 6]])
tf.matrix_set_diag(input, diagonal)
  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)
        [7, 2, 7, 7],
        [7, 7, 3, 7]],
       [[4, 7, 7, 7],
        [7, 5, 7, 7],
        [7, 7, 6, 7]]]

# A superdiagonal (per batch).
tf.matrix_set_diag(input, diagonal, k = 1)
  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)
        [7, 7, 2, 7],
        [7, 7, 7, 3]],
       [[7, 4, 7, 7],
        [7, 7, 5, 7],
        [7, 7, 7, 6]]]

# A band of diagonals.
diagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [4, 5, 0]],
                      [[0, 1, 2],
                       [5, 6, 4],
                       [6, 1, 2],
                       [3, 4, 0]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2))
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

# LEFT_RIGHT alignment.
diagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [0, 4, 5]],
                      [[1, 2, 0],
                       [5, 6, 4],
                       [6, 1, 2],
                       [0, 3, 4]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2), align="LEFT_RIGHT")
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

```

Args:
  input: A `Tensor`. Rank `r+1`, where `r >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `r` when `k` is an integer or `k[0] == k[1]`. Otherwise, it has rank `r+1`.
    `k >= 1`.
  k: A `Tensor` of type `int32`.
    Diagonal offset(s). Positive value means superdiagonal, 0 refers to the main
    diagonal, and negative value means subdiagonals. `k` can be a single integer
    (for a single diagonal) or a pair of integers specifying the low and high ends
    of a matrix band. `k[0]` must not be larger than `k[1]`.
  align: An optional `string` from: `"LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"`. Defaults to `"RIGHT_LEFT"`.
    Some diagonals are shorter than `max_diag_len` and need to be padded. `align` is
    a string specifying how superdiagonals and subdiagonals should be aligned,
    respectively. There are four possible alignments: "RIGHT_LEFT" (default),
    "LEFT_RIGHT", "LEFT_LEFT", and "RIGHT_RIGHT". "RIGHT_LEFT" aligns superdiagonals
    to the right (left-pads the row) and subdiagonals to the left (right-pads the
    row). It is the packing format LAPACK uses. cuSPARSE uses "LEFT_RIGHT", which is
    the opposite alignment.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiagV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiagV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the specified diagonals of the
innermost matrices. These will be overwritten by the values in `diagonal`.

`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or
`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.
Otherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.
`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.
`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,
`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`

The output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.
If `k` is scalar or `k[0] == k[1]`:

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
```

Otherwise,

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]
    input[i, j, ..., l, m, n]                         ; otherwise
```
where `d = n - m`, `diag_index = k[1] - d`, and
`index_in_diag = n - max(d, 0) + offset`.

`offset` is zero except when the alignment of the diagonal is to the right.
```
offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}
                                           and `d >= 0`) or
                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}
                                           and `d <= 0`)
         0                          ; otherwise
```
where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.

For example:

```
# The main diagonal.
input = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]],
                  [[7, 7, 7, 7],
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]]])
diagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)
                     [4, 5, 6]])
tf.matrix_set_diag(input, diagonal)
  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)
        [7, 2, 7, 7],
        [7, 7, 3, 7]],
       [[4, 7, 7, 7],
        [7, 5, 7, 7],
        [7, 7, 6, 7]]]

# A superdiagonal (per batch).
tf.matrix_set_diag(input, diagonal, k = 1)
  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)
        [7, 7, 2, 7],
        [7, 7, 7, 3]],
       [[7, 4, 7, 7],
        [7, 7, 5, 7],
        [7, 7, 7, 6]]]

# A band of diagonals.
diagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [4, 5, 0]],
                      [[0, 1, 2],
                       [5, 6, 4],
                       [6, 1, 2],
                       [3, 4, 0]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2))
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

# LEFT_RIGHT alignment.
diagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [0, 4, 5]],
                      [[1, 2, 0],
                       [5, 6, 4],
                       [6, 1, 2],
                       [0, 3, 4]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2), align="LEFT_RIGHT")
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

```

Args:
  input: A `Tensor`. Rank `r+1`, where `r >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `r` when `k` is an integer or `k[0] == k[1]`. Otherwise, it has rank `r+1`.
    `k >= 1`.
  k: A `Tensor` of type `int32`.
    Diagonal offset(s). Positive value means superdiagonal, 0 refers to the main
    diagonal, and negative value means subdiagonals. `k` can be a single integer
    (for a single diagonal) or a pair of integers specifying the low and high ends
    of a matrix band. `k[0]` must not be larger than `k[1]`.
  align: An optional `string` from: `"LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"`. Defaults to `"RIGHT_LEFT"`.
    Some diagonals are shorter than `max_diag_len` and need to be padded. `align` is
    a string specifying how superdiagonals and subdiagonals should be aligned,
    respectively. There are four possible alignments: "RIGHT_LEFT" (default),
    "LEFT_RIGHT", "LEFT_LEFT", and "RIGHT_RIGHT". "RIGHT_LEFT" aligns superdiagonals
    to the right (left-pads the row) and subdiagonals to the left (right-pads the
    row). It is the packing format LAPACK uses. cuSPARSE uses "LEFT_RIGHT", which is
    the opposite alignment.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiagV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiagV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MatrixSetDiagV3/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiagV3 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiagV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiagV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MatrixSetDiagV3`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MatrixSetDiagV3` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MatrixSetDiagV3/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiagV3 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the specified diagonals of the
innermost matrices. These will be overwritten by the values in `diagonal`.

`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or
`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.
Otherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.
`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.
`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,
`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`

The output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.
If `k` is scalar or `k[0] == k[1]`:

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
```

Otherwise,

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]
    input[i, j, ..., l, m, n]                         ; otherwise
```
where `d = n - m`, `diag_index = k[1] - d`, and
`index_in_diag = n - max(d, 0) + offset`.

`offset` is zero except when the alignment of the diagonal is to the right.
```
offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}
                                           and `d >= 0`) or
                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}
                                           and `d <= 0`)
         0                          ; otherwise
```
where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.

For example:

```
# The main diagonal.
input = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]],
                  [[7, 7, 7, 7],
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]]])
diagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)
                     [4, 5, 6]])
tf.matrix_set_diag(input, diagonal)
  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)
        [7, 2, 7, 7],
        [7, 7, 3, 7]],
       [[4, 7, 7, 7],
        [7, 5, 7, 7],
        [7, 7, 6, 7]]]

# A superdiagonal (per batch).
tf.matrix_set_diag(input, diagonal, k = 1)
  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)
        [7, 7, 2, 7],
        [7, 7, 7, 3]],
       [[7, 4, 7, 7],
        [7, 7, 5, 7],
        [7, 7, 7, 6]]]

# A band of diagonals.
diagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [4, 5, 0]],
                      [[0, 1, 2],
                       [5, 6, 4],
                       [6, 1, 2],
                       [3, 4, 0]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2))
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

# LEFT_RIGHT alignment.
diagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [0, 4, 5]],
                      [[1, 2, 0],
                       [5, 6, 4],
                       [6, 1, 2],
                       [0, 3, 4]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2), align="LEFT_RIGHT")
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

```

Args:
  input: A `Tensor`. Rank `r+1`, where `r >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `r` when `k` is an integer or `k[0] == k[1]`. Otherwise, it has rank `r+1`.
    `k >= 1`.
  k: A `Tensor` of type `int32`.
    Diagonal offset(s). Positive value means superdiagonal, 0 refers to the main
    diagonal, and negative value means subdiagonals. `k` can be a single integer
    (for a single diagonal) or a pair of integers specifying the low and high ends
    of a matrix band. `k[0]` must not be larger than `k[1]`.
  align: An optional `string` from: `"LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"`. Defaults to `"RIGHT_LEFT"`.
    Some diagonals are shorter than `max_diag_len` and need to be padded. `align` is
    a string specifying how superdiagonals and subdiagonals should be aligned,
    respectively. There are four possible alignments: "RIGHT_LEFT" (default),
    "LEFT_RIGHT", "LEFT_LEFT", and "RIGHT_RIGHT". "RIGHT_LEFT" aligns superdiagonals
    to the right (left-pads the row) and subdiagonals to the left (right-pads the
    row). It is the packing format LAPACK uses. cuSPARSE uses "LEFT_RIGHT", which is
    the opposite alignment.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            Returns a batched matrix tensor with new batched diagonal values.

Given `input` and `diagonal`, this operation returns a tensor with the
same shape and values as `input`, except for the specified diagonals of the
innermost matrices. These will be overwritten by the values in `diagonal`.

`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or
`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.
Otherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.
`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.
`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,
`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`

The output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.
If `k` is scalar or `k[0] == k[1]`:

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
```

Otherwise,

```
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]
    input[i, j, ..., l, m, n]                         ; otherwise
```
where `d = n - m`, `diag_index = k[1] - d`, and
`index_in_diag = n - max(d, 0) + offset`.

`offset` is zero except when the alignment of the diagonal is to the right.
```
offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}
                                           and `d >= 0`) or
                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}
                                           and `d <= 0`)
         0                          ; otherwise
```
where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.

For example:

```
# The main diagonal.
input = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]],
                  [[7, 7, 7, 7],
                   [7, 7, 7, 7],
                   [7, 7, 7, 7]]])
diagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)
                     [4, 5, 6]])
tf.matrix_set_diag(input, diagonal)
  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)
        [7, 2, 7, 7],
        [7, 7, 3, 7]],
       [[4, 7, 7, 7],
        [7, 5, 7, 7],
        [7, 7, 6, 7]]]

# A superdiagonal (per batch).
tf.matrix_set_diag(input, diagonal, k = 1)
  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)
        [7, 7, 2, 7],
        [7, 7, 7, 3]],
       [[7, 4, 7, 7],
        [7, 7, 5, 7],
        [7, 7, 7, 6]]]

# A band of diagonals.
diagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [4, 5, 0]],
                      [[0, 1, 2],
                       [5, 6, 4],
                       [6, 1, 2],
                       [3, 4, 0]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2))
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

# LEFT_RIGHT alignment.
diagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)
                       [6, 5, 8],
                       [1, 2, 3],
                       [0, 4, 5]],
                      [[1, 2, 0],
                       [5, 6, 4],
                       [6, 1, 2],
                       [0, 3, 4]]])
tf.matrix_set_diag(input, diagonals, k = (-1, 2), align="LEFT_RIGHT")
  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)
        [4, 2, 5, 1],
        [7, 5, 3, 8]],
       [[6, 5, 1, 7],
        [3, 1, 6, 2],
        [7, 4, 2, 4]]]

```

Args:
  input: A `Tensor`. Rank `r+1`, where `r >= 1`.
  diagonal: A `Tensor`. Must have the same type as `input`.
    Rank `r` when `k` is an integer or `k[0] == k[1]`. Otherwise, it has rank `r+1`.
    `k >= 1`.
  k: A `Tensor` of type `int32`.
    Diagonal offset(s). Positive value means superdiagonal, 0 refers to the main
    diagonal, and negative value means subdiagonals. `k` can be a single integer
    (for a single diagonal) or a pair of integers specifying the low and high ends
    of a matrix band. `k[0]` must not be larger than `k[1]`.
  align: An optional `string` from: `"LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"`. Defaults to `"RIGHT_LEFT"`.
    Some diagonals are shorter than `max_diag_len` and need to be padded. `align` is
    a string specifying how superdiagonals and subdiagonals should be aligned,
    respectively. There are four possible alignments: "RIGHT_LEFT" (default),
    "LEFT_RIGHT", "LEFT_LEFT", and "RIGHT_RIGHT". "RIGHT_LEFT" aligns superdiagonals
    to the right (left-pads the row) and subdiagonals to the left (right-pads the
    row). It is the packing format LAPACK uses. cuSPARSE uses "LEFT_RIGHT", which is
    the opposite alignment.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MatrixSetDiagV3/fuzz.cpp
[GEN] tf.raw_ops.MatrixSetDiagV3 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MatrixSetDiagV3`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MatrixSetDiagV3 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MatrixSetDiagV3/fuzz.cpp
[GEN] tf.raw_ops.MapUnstage -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            Op removes and returns the values associated with the key

from the underlying container.   If the underlying container
does not contain this key, the op will block until it does.

Args:
  key: A `Tensor` of type `int64`.
  indices: A `Tensor` of type `int32`.
  dtypes: A list of `tf.DTypes` that has length `>= 1`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  A list of `Tensor` objects of type `dtypes`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapUnstage`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapUnstage` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            Op removes and returns the values associated with the key

from the underlying container.   If the underlying container
does not contain this key, the op will block until it does.

Args:
  key: A `Tensor` of type `int64`.
  indices: A `Tensor` of type `int32`.
  dtypes: A list of `tf.DTypes` that has length `>= 1`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  A list of `Tensor` objects of type `dtypes`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapUnstage`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapUnstage` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.MapUnstage/fuzz.cpp
[GEN] tf.raw_ops.MapUnstage -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapUnstage`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapUnstage` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.MapUnstage`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.MapUnstage` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.MapUnstage/fuzz.cpp
[GEN] tf.raw_ops.MapUnstage -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            Op removes and returns the values associated with the key

from the underlying container.   If the underlying container
does not contain this key, the op will block until it does.

Args:
  key: A `Tensor` of type `int64`.
  indices: A `Tensor` of type `int32`.
  dtypes: A list of `tf.DTypes` that has length `>= 1`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  A list of `Tensor` objects of type `dtypes`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            Op removes and returns the values associated with the key

from the underlying container.   If the underlying container
does not contain this key, the op will block until it does.

Args:
  key: A `Tensor` of type `int64`.
  indices: A `Tensor` of type `int32`.
  dtypes: A list of `tf.DTypes` that has length `>= 1`.
  capacity: An optional `int` that is `>= 0`. Defaults to `0`.
  memory_limit: An optional `int` that is `>= 0`. Defaults to `0`.
  container: An optional `string`. Defaults to `""`.
  shared_name: An optional `string`. Defaults to `""`.
  name: A name for the operation (optional).

Returns:
  A list of `Tensor` objects of type `dtypes`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.MapUnstage/fuzz.cpp
[GEN] tf.raw_ops.MapUnstage -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.MapUnstage`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.MapUnstage functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.MapUnstage/fuzz.cpp
[GEN] tf.raw_ops.AccumulatorNumAccumulated -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            Returns the number of gradients aggregated in the given accumulators.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to an accumulator.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AccumulatorNumAccumulated`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AccumulatorNumAccumulated` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            Returns the number of gradients aggregated in the given accumulators.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to an accumulator.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AccumulatorNumAccumulated`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AccumulatorNumAccumulated` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.AccumulatorNumAccumulated/fuzz.cpp
[GEN] tf.raw_ops.AccumulatorNumAccumulated -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AccumulatorNumAccumulated`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AccumulatorNumAccumulated` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AccumulatorNumAccumulated`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AccumulatorNumAccumulated` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.AccumulatorNumAccumulated/fuzz.cpp
[GEN] tf.raw_ops.AccumulatorNumAccumulated -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            Returns the number of gradients aggregated in the given accumulators.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to an accumulator.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            Returns the number of gradients aggregated in the given accumulators.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to an accumulator.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.AccumulatorNumAccumulated/fuzz.cpp
[GEN] tf.raw_ops.AccumulatorNumAccumulated -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AccumulatorNumAccumulated`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AccumulatorNumAccumulated functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.AccumulatorNumAccumulated/fuzz.cpp
[GEN] tf.raw_ops.RandomPoissonV2 -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            Outputs random values from the Poisson distribution(s) described by rate.

This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in rate.
  rate: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`.
    A tensor in which each scalar is a "rate" parameter describing the
    associated poisson distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  dtype: An optional `tf.DType` from: `tf.half, tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomPoissonV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomPoissonV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            Outputs random values from the Poisson distribution(s) described by rate.

This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in rate.
  rate: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`.
    A tensor in which each scalar is a "rate" parameter describing the
    associated poisson distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  dtype: An optional `tf.DType` from: `tf.half, tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomPoissonV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomPoissonV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.RandomPoissonV2/fuzz.cpp
[GEN] tf.raw_ops.RandomPoissonV2 -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomPoissonV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomPoissonV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.RandomPoissonV2`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.RandomPoissonV2` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.RandomPoissonV2/fuzz.cpp
[GEN] tf.raw_ops.RandomPoissonV2 -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            Outputs random values from the Poisson distribution(s) described by rate.

This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in rate.
  rate: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`.
    A tensor in which each scalar is a "rate" parameter describing the
    associated poisson distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  dtype: An optional `tf.DType` from: `tf.half, tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            Outputs random values from the Poisson distribution(s) described by rate.

This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley

Args:
  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D integer tensor. Shape of independent samples to draw from each
    distribution described by the shape parameters given in rate.
  rate: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`.
    A tensor in which each scalar is a "rate" parameter describing the
    associated poisson distribution.
  seed: An optional `int`. Defaults to `0`.
    If either `seed` or `seed2` are set to be non-zero, the random number
    generator is seeded by the given seed.  Otherwise, it is seeded by a
    random seed.
  seed2: An optional `int`. Defaults to `0`.
    A second seed to avoid seed collision.
  dtype: An optional `tf.DType` from: `tf.half, tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.RandomPoissonV2/fuzz.cpp
[GEN] tf.raw_ops.RandomPoissonV2 -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.RandomPoissonV2`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.RandomPoissonV2 functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.RandomPoissonV2/fuzz.cpp
[GEN] tf.raw_ops.Dilation2D -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
input channel is processed independently of the others with its own structuring
function. The `output` tensor has shape
`[batch, out_height, out_width, depth]`. The spatial dimensions of the output
tensor depend on the `padding` algorithm. We currently only support the default
"NHWC" `data_format`.

In detail, the grayscale morphological 2-D dilation is the max-sum correlation
(for consistency with `conv2d`, we use unmirrored filters):

    output[b, y, x, c] =
       max_{dy, dx} input[b,
                          strides[1] * y + rates[1] * dy,
                          strides[2] * x + rates[2] * dx,
                          c] +
                    filter[dy, dx, c]

Max-pooling is a special case when the filter has size equal to the pooling
kernel size and contains all zeros.

Note on duality: The dilation of `input` by the `filter` is equal to the
negation of the erosion of `-input` by the reflected `filter`.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, in_height, in_width, depth]`.
  filter: A `Tensor`. Must have the same type as `input`.
    3-D with shape `[filter_height, filter_width, depth]`.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the input
    tensor. Must be: `[1, stride_height, stride_width, 1]`.
  rates: A list of `ints` that has length `>= 4`.
    The input stride for atrous morphological dilation. Must be:
    `[1, rate_height, rate_width, 1]`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Dilation2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Dilation2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
input channel is processed independently of the others with its own structuring
function. The `output` tensor has shape
`[batch, out_height, out_width, depth]`. The spatial dimensions of the output
tensor depend on the `padding` algorithm. We currently only support the default
"NHWC" `data_format`.

In detail, the grayscale morphological 2-D dilation is the max-sum correlation
(for consistency with `conv2d`, we use unmirrored filters):

    output[b, y, x, c] =
       max_{dy, dx} input[b,
                          strides[1] * y + rates[1] * dy,
                          strides[2] * x + rates[2] * dx,
                          c] +
                    filter[dy, dx, c]

Max-pooling is a special case when the filter has size equal to the pooling
kernel size and contains all zeros.

Note on duality: The dilation of `input` by the `filter` is equal to the
negation of the erosion of `-input` by the reflected `filter`.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, in_height, in_width, depth]`.
  filter: A `Tensor`. Must have the same type as `input`.
    3-D with shape `[filter_height, filter_width, depth]`.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the input
    tensor. Must be: `[1, stride_height, stride_width, 1]`.
  rates: A list of `ints` that has length `>= 4`.
    The input stride for atrous morphological dilation. Must be:
    `[1, rate_height, rate_width, 1]`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Dilation2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Dilation2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Dilation2D/fuzz.cpp
[GEN] tf.raw_ops.Dilation2D -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Dilation2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Dilation2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Dilation2D`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Dilation2D` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Dilation2D/fuzz.cpp
[GEN] tf.raw_ops.Dilation2D -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
input channel is processed independently of the others with its own structuring
function. The `output` tensor has shape
`[batch, out_height, out_width, depth]`. The spatial dimensions of the output
tensor depend on the `padding` algorithm. We currently only support the default
"NHWC" `data_format`.

In detail, the grayscale morphological 2-D dilation is the max-sum correlation
(for consistency with `conv2d`, we use unmirrored filters):

    output[b, y, x, c] =
       max_{dy, dx} input[b,
                          strides[1] * y + rates[1] * dy,
                          strides[2] * x + rates[2] * dx,
                          c] +
                    filter[dy, dx, c]

Max-pooling is a special case when the filter has size equal to the pooling
kernel size and contains all zeros.

Note on duality: The dilation of `input` by the `filter` is equal to the
negation of the erosion of `-input` by the reflected `filter`.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, in_height, in_width, depth]`.
  filter: A `Tensor`. Must have the same type as `input`.
    3-D with shape `[filter_height, filter_width, depth]`.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the input
    tensor. Must be: `[1, stride_height, stride_width, 1]`.
  rates: A list of `ints` that has length `>= 4`.
    The input stride for atrous morphological dilation. Must be:
    `[1, rate_height, rate_width, 1]`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
input channel is processed independently of the others with its own structuring
function. The `output` tensor has shape
`[batch, out_height, out_width, depth]`. The spatial dimensions of the output
tensor depend on the `padding` algorithm. We currently only support the default
"NHWC" `data_format`.

In detail, the grayscale morphological 2-D dilation is the max-sum correlation
(for consistency with `conv2d`, we use unmirrored filters):

    output[b, y, x, c] =
       max_{dy, dx} input[b,
                          strides[1] * y + rates[1] * dy,
                          strides[2] * x + rates[2] * dx,
                          c] +
                    filter[dy, dx, c]

Max-pooling is a special case when the filter has size equal to the pooling
kernel size and contains all zeros.

Note on duality: The dilation of `input` by the `filter` is equal to the
negation of the erosion of `-input` by the reflected `filter`.

Args:
  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
    4-D with shape `[batch, in_height, in_width, depth]`.
  filter: A `Tensor`. Must have the same type as `input`.
    3-D with shape `[filter_height, filter_width, depth]`.
  strides: A list of `ints` that has length `>= 4`.
    The stride of the sliding window for each dimension of the input
    tensor. Must be: `[1, stride_height, stride_width, 1]`.
  rates: A list of `ints` that has length `>= 4`.
    The input stride for atrous morphological dilation. Must be:
    `[1, rate_height, rate_width, 1]`.
  padding: A `string` from: `"SAME", "VALID"`.
    The type of padding algorithm to use.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Dilation2D/fuzz.cpp
[GEN] tf.raw_ops.Dilation2D -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Dilation2D`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Dilation2D functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Dilation2D/fuzz.cpp
[GEN] tf.raw_ops.Minimum -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            Returns the min of x and y (i.e. x < y ? x : y) element-wise.

Both inputs are number-type tensors (except complex).  `minimum` expects that
both tensors have the same `dtype`.

Examples:

>>> x = tf.constant([0., 0., 0., 0.])
>>> y = tf.constant([-5., -2., 0., 3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>

Note that `minimum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.

>>> x = tf.constant([-5., 0., 0., 0.])
>>> y = tf.constant([-3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -3., -3., -3.], dtype=float32)>

The reduction version of this elementwise operation is `tf.math.reduce_min`

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Minimum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Minimum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            Returns the min of x and y (i.e. x < y ? x : y) element-wise.

Both inputs are number-type tensors (except complex).  `minimum` expects that
both tensors have the same `dtype`.

Examples:

>>> x = tf.constant([0., 0., 0., 0.])
>>> y = tf.constant([-5., -2., 0., 3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>

Note that `minimum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.

>>> x = tf.constant([-5., 0., 0., 0.])
>>> y = tf.constant([-3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -3., -3., -3.], dtype=float32)>

The reduction version of this elementwise operation is `tf.math.reduce_min`

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Minimum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Minimum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Minimum/fuzz.cpp
[GEN] tf.raw_ops.Minimum -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Minimum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Minimum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Minimum`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Minimum` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Minimum/fuzz.cpp
[GEN] tf.raw_ops.Minimum -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            Returns the min of x and y (i.e. x < y ? x : y) element-wise.

Both inputs are number-type tensors (except complex).  `minimum` expects that
both tensors have the same `dtype`.

Examples:

>>> x = tf.constant([0., 0., 0., 0.])
>>> y = tf.constant([-5., -2., 0., 3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>

Note that `minimum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.

>>> x = tf.constant([-5., 0., 0., 0.])
>>> y = tf.constant([-3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -3., -3., -3.], dtype=float32)>

The reduction version of this elementwise operation is `tf.math.reduce_min`

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            Returns the min of x and y (i.e. x < y ? x : y) element-wise.

Both inputs are number-type tensors (except complex).  `minimum` expects that
both tensors have the same `dtype`.

Examples:

>>> x = tf.constant([0., 0., 0., 0.])
>>> y = tf.constant([-5., -2., 0., 3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>

Note that `minimum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.

>>> x = tf.constant([-5., 0., 0., 0.])
>>> y = tf.constant([-3.])
>>> tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -3., -3., -3.], dtype=float32)>

The reduction version of this elementwise operation is `tf.math.reduce_min`

Args:
  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Minimum/fuzz.cpp
[GEN] tf.raw_ops.Minimum -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Minimum`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Minimum functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Minimum/fuzz.cpp
[GEN] tf.raw_ops.IdentityReader -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            A Reader that outputs the queued work as both the key and value.

To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).

Args:
  container: An optional `string`. Defaults to `""`.
    If non-empty, this reader is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this reader is named in the given bucket
    with this shared_name. Otherwise, the node name is used instead.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.IdentityReader`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.IdentityReader` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            A Reader that outputs the queued work as both the key and value.

To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).

Args:
  container: An optional `string`. Defaults to `""`.
    If non-empty, this reader is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this reader is named in the given bucket
    with this shared_name. Otherwise, the node name is used instead.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.IdentityReader`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.IdentityReader` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.IdentityReader/fuzz.cpp
[GEN] tf.raw_ops.IdentityReader -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.IdentityReader`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.IdentityReader` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.IdentityReader`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.IdentityReader` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.IdentityReader/fuzz.cpp
[GEN] tf.raw_ops.IdentityReader -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            A Reader that outputs the queued work as both the key and value.

To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).

Args:
  container: An optional `string`. Defaults to `""`.
    If non-empty, this reader is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this reader is named in the given bucket
    with this shared_name. Otherwise, the node name is used instead.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            A Reader that outputs the queued work as both the key and value.

To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).

Args:
  container: An optional `string`. Defaults to `""`.
    If non-empty, this reader is placed in the given container.
    Otherwise, a default container is used.
  shared_name: An optional `string`. Defaults to `""`.
    If non-empty, this reader is named in the given bucket
    with this shared_name. Otherwise, the node name is used instead.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type mutable `string`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.IdentityReader/fuzz.cpp
[GEN] tf.raw_ops.IdentityReader -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.IdentityReader`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.IdentityReader functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.IdentityReader/fuzz.cpp
[GEN] tf.raw_ops.ScatterMul -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            Multiplies sparse updates into a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions multiply.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to multiply to `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            Multiplies sparse updates into a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions multiply.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to multiply to `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ScatterMul/fuzz.cpp
[GEN] tf.raw_ops.ScatterMul -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterMul`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterMul` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ScatterMul/fuzz.cpp
[GEN] tf.raw_ops.ScatterMul -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            Multiplies sparse updates into a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions multiply.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to multiply to `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            Multiplies sparse updates into a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions multiply.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of updated values to multiply to `ref`.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ScatterMul/fuzz.cpp
[GEN] tf.raw_ops.ScatterMul -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterMul`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterMul functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ScatterMul/fuzz.cpp
[GEN] tf.raw_ops.Placeholder -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            A placeholder op for a value that will be fed into the computation.

N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.

Args:
  dtype: A `tf.DType`. The type of elements in the tensor.
  shape: An optional `tf.TensorShape` or list of `ints`. Defaults to `None`.
    (Optional) The shape of the tensor. If the shape has 0 dimensions, the
    shape is unconstrained.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Placeholder`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Placeholder` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            A placeholder op for a value that will be fed into the computation.

N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.

Args:
  dtype: A `tf.DType`. The type of elements in the tensor.
  shape: An optional `tf.TensorShape` or list of `ints`. Defaults to `None`.
    (Optional) The shape of the tensor. If the shape has 0 dimensions, the
    shape is unconstrained.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Placeholder`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Placeholder` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Placeholder/fuzz.cpp
[GEN] tf.raw_ops.Placeholder -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Placeholder`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Placeholder` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Placeholder`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Placeholder` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Placeholder/fuzz.cpp
[GEN] tf.raw_ops.Placeholder -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            A placeholder op for a value that will be fed into the computation.

N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.

Args:
  dtype: A `tf.DType`. The type of elements in the tensor.
  shape: An optional `tf.TensorShape` or list of `ints`. Defaults to `None`.
    (Optional) The shape of the tensor. If the shape has 0 dimensions, the
    shape is unconstrained.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            A placeholder op for a value that will be fed into the computation.

N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.

Args:
  dtype: A `tf.DType`. The type of elements in the tensor.
  shape: An optional `tf.TensorShape` or list of `ints`. Defaults to `None`.
    (Optional) The shape of the tensor. If the shape has 0 dimensions, the
    shape is unconstrained.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `dtype`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Placeholder/fuzz.cpp
[GEN] tf.raw_ops.Placeholder -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Placeholder`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Placeholder functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Placeholder/fuzz.cpp
[GEN] tf.raw_ops.BarrierTakeMany -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            Takes the given number of completed elements from a barrier.

This operation concatenates completed-element component tensors along
the 0th dimension to make a single component tensor.

Elements come out of the barrier when they are complete, and in the order
in which they were placed into the barrier.  The indices output provides
information about the batch in which each element was originally inserted
into the barrier.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to a barrier.
  num_elements: A `Tensor` of type `int32`.
    A single-element tensor containing the number of elements to
    take.
  component_types: A list of `tf.DTypes` that has length `>= 1`.
    The type of each component in a value.
  allow_small_batch: An optional `bool`. Defaults to `False`.
    Allow to return less than num_elements items if barrier is
    already closed.
  wait_for_incomplete: An optional `bool`. Defaults to `False`.
  timeout_ms: An optional `int`. Defaults to `-1`.
    If the queue is empty, this operation will block for up to
    timeout_ms milliseconds.
    Note: This option is not supported yet.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (indices, keys, values).

  indices: A `Tensor` of type `int64`.
  keys: A `Tensor` of type `string`.
  values: A list of `Tensor` objects of type `component_types`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BarrierTakeMany`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BarrierTakeMany` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            Takes the given number of completed elements from a barrier.

This operation concatenates completed-element component tensors along
the 0th dimension to make a single component tensor.

Elements come out of the barrier when they are complete, and in the order
in which they were placed into the barrier.  The indices output provides
information about the batch in which each element was originally inserted
into the barrier.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to a barrier.
  num_elements: A `Tensor` of type `int32`.
    A single-element tensor containing the number of elements to
    take.
  component_types: A list of `tf.DTypes` that has length `>= 1`.
    The type of each component in a value.
  allow_small_batch: An optional `bool`. Defaults to `False`.
    Allow to return less than num_elements items if barrier is
    already closed.
  wait_for_incomplete: An optional `bool`. Defaults to `False`.
  timeout_ms: An optional `int`. Defaults to `-1`.
    If the queue is empty, this operation will block for up to
    timeout_ms milliseconds.
    Note: This option is not supported yet.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (indices, keys, values).

  indices: A `Tensor` of type `int64`.
  keys: A `Tensor` of type `string`.
  values: A list of `Tensor` objects of type `component_types`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BarrierTakeMany`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BarrierTakeMany` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.BarrierTakeMany/fuzz.cpp
[GEN] tf.raw_ops.BarrierTakeMany -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BarrierTakeMany`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BarrierTakeMany` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BarrierTakeMany`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BarrierTakeMany` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.BarrierTakeMany/fuzz.cpp
[GEN] tf.raw_ops.BarrierTakeMany -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            Takes the given number of completed elements from a barrier.

This operation concatenates completed-element component tensors along
the 0th dimension to make a single component tensor.

Elements come out of the barrier when they are complete, and in the order
in which they were placed into the barrier.  The indices output provides
information about the batch in which each element was originally inserted
into the barrier.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to a barrier.
  num_elements: A `Tensor` of type `int32`.
    A single-element tensor containing the number of elements to
    take.
  component_types: A list of `tf.DTypes` that has length `>= 1`.
    The type of each component in a value.
  allow_small_batch: An optional `bool`. Defaults to `False`.
    Allow to return less than num_elements items if barrier is
    already closed.
  wait_for_incomplete: An optional `bool`. Defaults to `False`.
  timeout_ms: An optional `int`. Defaults to `-1`.
    If the queue is empty, this operation will block for up to
    timeout_ms milliseconds.
    Note: This option is not supported yet.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (indices, keys, values).

  indices: A `Tensor` of type `int64`.
  keys: A `Tensor` of type `string`.
  values: A list of `Tensor` objects of type `component_types`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            Takes the given number of completed elements from a barrier.

This operation concatenates completed-element component tensors along
the 0th dimension to make a single component tensor.

Elements come out of the barrier when they are complete, and in the order
in which they were placed into the barrier.  The indices output provides
information about the batch in which each element was originally inserted
into the barrier.

Args:
  handle: A `Tensor` of type mutable `string`. The handle to a barrier.
  num_elements: A `Tensor` of type `int32`.
    A single-element tensor containing the number of elements to
    take.
  component_types: A list of `tf.DTypes` that has length `>= 1`.
    The type of each component in a value.
  allow_small_batch: An optional `bool`. Defaults to `False`.
    Allow to return less than num_elements items if barrier is
    already closed.
  wait_for_incomplete: An optional `bool`. Defaults to `False`.
  timeout_ms: An optional `int`. Defaults to `-1`.
    If the queue is empty, this operation will block for up to
    timeout_ms milliseconds.
    Note: This option is not supported yet.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (indices, keys, values).

  indices: A `Tensor` of type `int64`.
  keys: A `Tensor` of type `string`.
  values: A list of `Tensor` objects of type `component_types`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.BarrierTakeMany/fuzz.cpp
[GEN] tf.raw_ops.BarrierTakeMany -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BarrierTakeMany`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BarrierTakeMany functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.BarrierTakeMany/fuzz.cpp
[GEN] tf.raw_ops.FloorMod -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            Returns element-wise remainder of division.

This follows Python semantics in that the
result here is consistent with a flooring divide. E.g.
`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.

*NOTE*: `math.floormod` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FloorMod`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FloorMod` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            Returns element-wise remainder of division.

This follows Python semantics in that the
result here is consistent with a flooring divide. E.g.
`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.

*NOTE*: `math.floormod` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FloorMod`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FloorMod` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.FloorMod/fuzz.cpp
[GEN] tf.raw_ops.FloorMod -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FloorMod`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FloorMod` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.FloorMod`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.FloorMod` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.FloorMod/fuzz.cpp
[GEN] tf.raw_ops.FloorMod -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            Returns element-wise remainder of division.

This follows Python semantics in that the
result here is consistent with a flooring divide. E.g.
`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.

*NOTE*: `math.floormod` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            Returns element-wise remainder of division.

This follows Python semantics in that the
result here is consistent with a flooring divide. E.g.
`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.

*NOTE*: `math.floormod` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

Args:
  x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.
  y: A `Tensor`. Must have the same type as `x`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `x`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.FloorMod/fuzz.cpp
[GEN] tf.raw_ops.FloorMod -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.FloorMod`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.FloorMod functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.FloorMod/fuzz.cpp
[GEN] tf.raw_ops.BatchToSpaceND -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            BatchToSpace for N-D tensors of type T.

This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of shape
`block_shape + [batch]`, interleaves these blocks back into the grid defined by
the spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as
the input.  The spatial dimensions of this intermediate result are then
optionally cropped according to `crops` to produce the output.  This is the
reverse of SpaceToBatch.  See below for a precise description.

Args:
  input: A `Tensor`.
    N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,
    where spatial_shape has M dimensions.
  block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D with shape `[M]`, all values must be >= 1.
  crops: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D with shape `[M, 2]`, all values must be >= 0.
      `crops[i] = [crop_start, crop_end]` specifies the amount to crop from input
      dimension `i + 1`, which corresponds to spatial dimension `i`.  It is
      required that
      `crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]`.

    This operation is equivalent to the following steps:

    1. Reshape `input` to `reshaped` of shape:
         [block_shape[0], ..., block_shape[M-1],
          batch / prod(block_shape),
          input_shape[1], ..., input_shape[N-1]]

    2. Permute dimensions of `reshaped` to produce `permuted` of shape
         [batch / prod(block_shape),

          input_shape[1], block_shape[0],
          ...,
          input_shape[M], block_shape[M-1],

          input_shape[M+1], ..., input_shape[N-1]]

    3. Reshape `permuted` to produce `reshaped_permuted` of shape
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0],
          ...,
          input_shape[M] * block_shape[M-1],

          input_shape[M+1],
          ...,
          input_shape[N-1]]

    4. Crop the start and end of dimensions `[1, ..., M]` of
       `reshaped_permuted` according to `crops` to produce the output of shape:
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],
          ...,
          input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],

          input_shape[M+1], ..., input_shape[N-1]]

    Some examples:

    (1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
    ```

    The output tensor has shape `[1, 2, 2, 1]` and value:

    ```
    x = [[[[1], [2]], [[3], [4]]]]
    ```

    (2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]
    ```

    The output tensor has shape `[1, 2, 2, 3]` and value:

    ```
    x = [[[[1, 2, 3], [4, 5, 6]],
          [[7, 8, 9], [10, 11, 12]]]]
    ```

    (3) For the following input of shape `[4, 2, 2, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    x = [[[[1], [3]], [[9], [11]]],
         [[[2], [4]], [[10], [12]]],
         [[[5], [7]], [[13], [15]]],
         [[[6], [8]], [[14], [16]]]]
    ```

    The output tensor has shape `[1, 4, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
         [[5],   [6],  [7],  [8]],
         [[9],  [10], [11],  [12]],
         [[13], [14], [15],  [16]]]]
    ```

    (4) For the following input of shape `[8, 1, 3, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [2, 0]]`:

    ```
    x = [[[[0], [1], [3]]], [[[0], [9], [11]]],
         [[[0], [2], [4]]], [[[0], [10], [12]]],
         [[[0], [5], [7]]], [[[0], [13], [15]]],
         [[[0], [6], [8]]], [[[0], [14], [16]]]]
    ```

    The output tensor has shape `[2, 2, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
          [[5],   [6],  [7],  [8]]],
         [[[9],  [10], [11],  [12]],
          [[13], [14], [15],  [16]]]]
    ```
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BatchToSpaceND`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BatchToSpaceND` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            BatchToSpace for N-D tensors of type T.

This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of shape
`block_shape + [batch]`, interleaves these blocks back into the grid defined by
the spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as
the input.  The spatial dimensions of this intermediate result are then
optionally cropped according to `crops` to produce the output.  This is the
reverse of SpaceToBatch.  See below for a precise description.

Args:
  input: A `Tensor`.
    N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,
    where spatial_shape has M dimensions.
  block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D with shape `[M]`, all values must be >= 1.
  crops: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D with shape `[M, 2]`, all values must be >= 0.
      `crops[i] = [crop_start, crop_end]` specifies the amount to crop from input
      dimension `i + 1`, which corresponds to spatial dimension `i`.  It is
      required that
      `crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]`.

    This operation is equivalent to the following steps:

    1. Reshape `input` to `reshaped` of shape:
         [block_shape[0], ..., block_shape[M-1],
          batch / prod(block_shape),
          input_shape[1], ..., input_shape[N-1]]

    2. Permute dimensions of `reshaped` to produce `permuted` of shape
         [batch / prod(block_shape),

          input_shape[1], block_shape[0],
          ...,
          input_shape[M], block_shape[M-1],

          input_shape[M+1], ..., input_shape[N-1]]

    3. Reshape `permuted` to produce `reshaped_permuted` of shape
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0],
          ...,
          input_shape[M] * block_shape[M-1],

          input_shape[M+1],
          ...,
          input_shape[N-1]]

    4. Crop the start and end of dimensions `[1, ..., M]` of
       `reshaped_permuted` according to `crops` to produce the output of shape:
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],
          ...,
          input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],

          input_shape[M+1], ..., input_shape[N-1]]

    Some examples:

    (1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
    ```

    The output tensor has shape `[1, 2, 2, 1]` and value:

    ```
    x = [[[[1], [2]], [[3], [4]]]]
    ```

    (2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]
    ```

    The output tensor has shape `[1, 2, 2, 3]` and value:

    ```
    x = [[[[1, 2, 3], [4, 5, 6]],
          [[7, 8, 9], [10, 11, 12]]]]
    ```

    (3) For the following input of shape `[4, 2, 2, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    x = [[[[1], [3]], [[9], [11]]],
         [[[2], [4]], [[10], [12]]],
         [[[5], [7]], [[13], [15]]],
         [[[6], [8]], [[14], [16]]]]
    ```

    The output tensor has shape `[1, 4, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
         [[5],   [6],  [7],  [8]],
         [[9],  [10], [11],  [12]],
         [[13], [14], [15],  [16]]]]
    ```

    (4) For the following input of shape `[8, 1, 3, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [2, 0]]`:

    ```
    x = [[[[0], [1], [3]]], [[[0], [9], [11]]],
         [[[0], [2], [4]]], [[[0], [10], [12]]],
         [[[0], [5], [7]]], [[[0], [13], [15]]],
         [[[0], [6], [8]]], [[[0], [14], [16]]]]
    ```

    The output tensor has shape `[2, 2, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
          [[5],   [6],  [7],  [8]]],
         [[[9],  [10], [11],  [12]],
          [[13], [14], [15],  [16]]]]
    ```
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BatchToSpaceND`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BatchToSpaceND` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.BatchToSpaceND/fuzz.cpp
[GEN] tf.raw_ops.BatchToSpaceND -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BatchToSpaceND`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BatchToSpaceND` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.BatchToSpaceND`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.BatchToSpaceND` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.BatchToSpaceND/fuzz.cpp
[GEN] tf.raw_ops.BatchToSpaceND -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            BatchToSpace for N-D tensors of type T.

This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of shape
`block_shape + [batch]`, interleaves these blocks back into the grid defined by
the spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as
the input.  The spatial dimensions of this intermediate result are then
optionally cropped according to `crops` to produce the output.  This is the
reverse of SpaceToBatch.  See below for a precise description.

Args:
  input: A `Tensor`.
    N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,
    where spatial_shape has M dimensions.
  block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D with shape `[M]`, all values must be >= 1.
  crops: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D with shape `[M, 2]`, all values must be >= 0.
      `crops[i] = [crop_start, crop_end]` specifies the amount to crop from input
      dimension `i + 1`, which corresponds to spatial dimension `i`.  It is
      required that
      `crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]`.

    This operation is equivalent to the following steps:

    1. Reshape `input` to `reshaped` of shape:
         [block_shape[0], ..., block_shape[M-1],
          batch / prod(block_shape),
          input_shape[1], ..., input_shape[N-1]]

    2. Permute dimensions of `reshaped` to produce `permuted` of shape
         [batch / prod(block_shape),

          input_shape[1], block_shape[0],
          ...,
          input_shape[M], block_shape[M-1],

          input_shape[M+1], ..., input_shape[N-1]]

    3. Reshape `permuted` to produce `reshaped_permuted` of shape
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0],
          ...,
          input_shape[M] * block_shape[M-1],

          input_shape[M+1],
          ...,
          input_shape[N-1]]

    4. Crop the start and end of dimensions `[1, ..., M]` of
       `reshaped_permuted` according to `crops` to produce the output of shape:
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],
          ...,
          input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],

          input_shape[M+1], ..., input_shape[N-1]]

    Some examples:

    (1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
    ```

    The output tensor has shape `[1, 2, 2, 1]` and value:

    ```
    x = [[[[1], [2]], [[3], [4]]]]
    ```

    (2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]
    ```

    The output tensor has shape `[1, 2, 2, 3]` and value:

    ```
    x = [[[[1, 2, 3], [4, 5, 6]],
          [[7, 8, 9], [10, 11, 12]]]]
    ```

    (3) For the following input of shape `[4, 2, 2, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    x = [[[[1], [3]], [[9], [11]]],
         [[[2], [4]], [[10], [12]]],
         [[[5], [7]], [[13], [15]]],
         [[[6], [8]], [[14], [16]]]]
    ```

    The output tensor has shape `[1, 4, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
         [[5],   [6],  [7],  [8]],
         [[9],  [10], [11],  [12]],
         [[13], [14], [15],  [16]]]]
    ```

    (4) For the following input of shape `[8, 1, 3, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [2, 0]]`:

    ```
    x = [[[[0], [1], [3]]], [[[0], [9], [11]]],
         [[[0], [2], [4]]], [[[0], [10], [12]]],
         [[[0], [5], [7]]], [[[0], [13], [15]]],
         [[[0], [6], [8]]], [[[0], [14], [16]]]]
    ```

    The output tensor has shape `[2, 2, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
          [[5],   [6],  [7],  [8]]],
         [[[9],  [10], [11],  [12]],
          [[13], [14], [15],  [16]]]]
    ```
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            BatchToSpace for N-D tensors of type T.

This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of shape
`block_shape + [batch]`, interleaves these blocks back into the grid defined by
the spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as
the input.  The spatial dimensions of this intermediate result are then
optionally cropped according to `crops` to produce the output.  This is the
reverse of SpaceToBatch.  See below for a precise description.

Args:
  input: A `Tensor`.
    N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,
    where spatial_shape has M dimensions.
  block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    1-D with shape `[M]`, all values must be >= 1.
  crops: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    2-D with shape `[M, 2]`, all values must be >= 0.
      `crops[i] = [crop_start, crop_end]` specifies the amount to crop from input
      dimension `i + 1`, which corresponds to spatial dimension `i`.  It is
      required that
      `crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]`.

    This operation is equivalent to the following steps:

    1. Reshape `input` to `reshaped` of shape:
         [block_shape[0], ..., block_shape[M-1],
          batch / prod(block_shape),
          input_shape[1], ..., input_shape[N-1]]

    2. Permute dimensions of `reshaped` to produce `permuted` of shape
         [batch / prod(block_shape),

          input_shape[1], block_shape[0],
          ...,
          input_shape[M], block_shape[M-1],

          input_shape[M+1], ..., input_shape[N-1]]

    3. Reshape `permuted` to produce `reshaped_permuted` of shape
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0],
          ...,
          input_shape[M] * block_shape[M-1],

          input_shape[M+1],
          ...,
          input_shape[N-1]]

    4. Crop the start and end of dimensions `[1, ..., M]` of
       `reshaped_permuted` according to `crops` to produce the output of shape:
         [batch / prod(block_shape),

          input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],
          ...,
          input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],

          input_shape[M+1], ..., input_shape[N-1]]

    Some examples:

    (1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
    ```

    The output tensor has shape `[1, 2, 2, 1]` and value:

    ```
    x = [[[[1], [2]], [[3], [4]]]]
    ```

    (2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]
    ```

    The output tensor has shape `[1, 2, 2, 3]` and value:

    ```
    x = [[[[1, 2, 3], [4, 5, 6]],
          [[7, 8, 9], [10, 11, 12]]]]
    ```

    (3) For the following input of shape `[4, 2, 2, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [0, 0]]`:

    ```
    x = [[[[1], [3]], [[9], [11]]],
         [[[2], [4]], [[10], [12]]],
         [[[5], [7]], [[13], [15]]],
         [[[6], [8]], [[14], [16]]]]
    ```

    The output tensor has shape `[1, 4, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
         [[5],   [6],  [7],  [8]],
         [[9],  [10], [11],  [12]],
         [[13], [14], [15],  [16]]]]
    ```

    (4) For the following input of shape `[8, 1, 3, 1]`, `block_shape = [2, 2]`, and
        `crops = [[0, 0], [2, 0]]`:

    ```
    x = [[[[0], [1], [3]]], [[[0], [9], [11]]],
         [[[0], [2], [4]]], [[[0], [10], [12]]],
         [[[0], [5], [7]]], [[[0], [13], [15]]],
         [[[0], [6], [8]]], [[[0], [14], [16]]]]
    ```

    The output tensor has shape `[2, 2, 4, 1]` and value:

    ```
    x = [[[[1],   [2],  [3],  [4]],
          [[5],   [6],  [7],  [8]]],
         [[[9],  [10], [11],  [12]],
          [[13], [14], [15],  [16]]]]
    ```
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.BatchToSpaceND/fuzz.cpp
[GEN] tf.raw_ops.BatchToSpaceND -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.BatchToSpaceND`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.BatchToSpaceND functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.BatchToSpaceND/fuzz.cpp
[GEN] tf.raw_ops.ScatterDiv -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            Divides a variable reference by sparse updates.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions divide.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of values that `ref` is divided by.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            Divides a variable reference by sparse updates.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions divide.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of values that `ref` is divided by.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ScatterDiv/fuzz.cpp
[GEN] tf.raw_ops.ScatterDiv -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScatterDiv`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScatterDiv` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ScatterDiv/fuzz.cpp
[GEN] tf.raw_ops.ScatterDiv -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            Divides a variable reference by sparse updates.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions divide.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of values that `ref` is divided by.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            Divides a variable reference by sparse updates.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions divide.

Requires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A tensor of indices into the first dimension of `ref`.
  updates: A `Tensor`. Must have the same type as `ref`.
    A tensor of values that `ref` is divided by.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the operation will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ScatterDiv/fuzz.cpp
[GEN] tf.raw_ops.ScatterDiv -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScatterDiv`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScatterDiv functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ScatterDiv/fuzz.cpp
[GEN] tf.raw_ops.AssignSub -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            Update 'ref' by subtracting 'value' from it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  value: A `Tensor`. Must have the same type as `ref`.
    The value to be subtracted to the variable.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the subtraction will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AssignSub`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AssignSub` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            Update 'ref' by subtracting 'value' from it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  value: A `Tensor`. Must have the same type as `ref`.
    The value to be subtracted to the variable.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the subtraction will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AssignSub`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AssignSub` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.AssignSub/fuzz.cpp
[GEN] tf.raw_ops.AssignSub -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AssignSub`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AssignSub` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.AssignSub`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.AssignSub` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.AssignSub/fuzz.cpp
[GEN] tf.raw_ops.AssignSub -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            Update 'ref' by subtracting 'value' from it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  value: A `Tensor`. Must have the same type as `ref`.
    The value to be subtracted to the variable.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the subtraction will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            Update 'ref' by subtracting 'value' from it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.

Args:
  ref: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a `Variable` node.
  value: A `Tensor`. Must have the same type as `ref`.
    The value to be subtracted to the variable.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, the subtraction will be protected by a lock;
    otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `ref`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.AssignSub/fuzz.cpp
[GEN] tf.raw_ops.AssignSub -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.AssignSub`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.AssignSub functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.AssignSub/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxes -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(180, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(180, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.DrawBoundingBoxes/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxes -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DrawBoundingBoxes`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DrawBoundingBoxes` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.DrawBoundingBoxes/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxes -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(180, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels (height x width) and the bounding
box is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of
the bounding box will be `(40, 10)` to `(180, 50)` (in (x,y) coordinates).

Parts of the bounding box may fall outside the image.

Args:
  images: A `Tensor`. Must be one of the following types: `float32`, `half`.
    4-D with shape `[batch, height, width, depth]`. A batch of images.
  boxes: A `Tensor` of type `float32`.
    3-D with shape `[batch, num_bounding_boxes, 4]` containing bounding
    boxes.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `images`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.DrawBoundingBoxes/fuzz.cpp
[GEN] tf.raw_ops.DrawBoundingBoxes -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DrawBoundingBoxes`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DrawBoundingBoxes functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.DrawBoundingBoxes/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNWithNumSegments -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

Like `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

Like `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseSegmentSqrtNWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNWithNumSegments -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseSegmentSqrtNWithNumSegments` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseSegmentSqrtNWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNWithNumSegments -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

Like `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            Computes the sum along sparse segments of a tensor divided by the sqrt of N.

N is the size of the segment being reduced.

Like `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is
missing, the `output` tensor at that position will be zeroed.

Read
[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)
for an explanation of segments.

Args:
  data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Has same rank as `segment_ids`.
  segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A 1-D tensor. Values should be sorted and can be repeated.
  num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    Should equal the number of distinct segment IDs.
  sparse_gradient: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  A `Tensor`. Has the same type as `data`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseSegmentSqrtNWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseSegmentSqrtNWithNumSegments -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseSegmentSqrtNWithNumSegments`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseSegmentSqrtNWithNumSegments functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseSegmentSqrtNWithNumSegments/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdagradDA -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  gradient_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  gradient_squared_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  gradient_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  gradient_squared_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.SparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdagradDA -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.SparseApplyAdagradDA`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.SparseApplyAdagradDA` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.SparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdagradDA -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  gradient_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  gradient_squared_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            Update entries in '*var' and '*accum' according to the proximal adagrad scheme.

Args:
  var: A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.
    Should be from a Variable().
  gradient_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  gradient_squared_accumulator: A mutable `Tensor`. Must have the same type as `var`.
    Should be from a Variable().
  grad: A `Tensor`. Must have the same type as `var`. The gradient.
  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.
    A vector of indices into the first dimension of var and accum.
  lr: A `Tensor`. Must have the same type as `var`.
    Learning rate. Must be a scalar.
  l1: A `Tensor`. Must have the same type as `var`.
    L1 regularization. Must be a scalar.
  l2: A `Tensor`. Must have the same type as `var`.
    L2 regularization. Must be a scalar.
  global_step: A `Tensor` of type `int64`.
    Training step number. Must be a scalar.
  use_locking: An optional `bool`. Defaults to `False`.
    If True, updating of the var and accum tensors will be protected by
    a lock; otherwise the behavior is undefined, but may exhibit less contention.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `var`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.SparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.SparseApplyAdagradDA -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.SparseApplyAdagradDA`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.SparseApplyAdagradDA functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.SparseApplyAdagradDA/fuzz.cpp
[GEN] tf.raw_ops.ScaleAndTranslate -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  images: A `Tensor`. Must be one of the following types: `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
  size: A `Tensor` of type `int32`.
  scale: A `Tensor` of type `float32`.
  translation: A `Tensor` of type `float32`.
  kernel_type: An optional `string`. Defaults to `"lanczos3"`.
  antialias: An optional `bool`. Defaults to `True`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScaleAndTranslate`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScaleAndTranslate` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  images: A `Tensor`. Must be one of the following types: `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
  size: A `Tensor` of type `int32`.
  scale: A `Tensor` of type `float32`.
  translation: A `Tensor` of type `float32`.
  kernel_type: An optional `string`. Defaults to `"lanczos3"`.
  antialias: An optional `bool`. Defaults to `True`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScaleAndTranslate`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScaleAndTranslate` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.ScaleAndTranslate/fuzz.cpp
[GEN] tf.raw_ops.ScaleAndTranslate -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScaleAndTranslate`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScaleAndTranslate` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.ScaleAndTranslate`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.ScaleAndTranslate` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.ScaleAndTranslate/fuzz.cpp
[GEN] tf.raw_ops.ScaleAndTranslate -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  images: A `Tensor`. Must be one of the following types: `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
  size: A `Tensor` of type `int32`.
  scale: A `Tensor` of type `float32`.
  translation: A `Tensor` of type `float32`.
  kernel_type: An optional `string`. Defaults to `"lanczos3"`.
  antialias: An optional `bool`. Defaults to `True`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            TODO: add doc.

Args:
  images: A `Tensor`. Must be one of the following types: `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
  size: A `Tensor` of type `int32`.
  scale: A `Tensor` of type `float32`.
  translation: A `Tensor` of type `float32`.
  kernel_type: An optional `string`. Defaults to `"lanczos3"`.
  antialias: An optional `bool`. Defaults to `True`.
  name: A name for the operation (optional).

Returns:
  A `Tensor` of type `float32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.ScaleAndTranslate/fuzz.cpp
[GEN] tf.raw_ops.ScaleAndTranslate -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.ScaleAndTranslate`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.ScaleAndTranslate functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.ScaleAndTranslate/fuzz.cpp
[GEN] tf.raw_ops.CombinedNonMaxSuppression -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            Greedily selects a subset of bounding boxes in descending order of score,

This operation performs non_max_suppression on the inputs per batch, across
all classes.
Prunes away boxes that have high intersection-over-union (IOU) overlap
with previously selected boxes.  Bounding boxes are supplied as
[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any
diagonal pair of box corners and the coordinates can be provided as normalized
(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm
is agnostic to where the origin is in the coordinate system. Also note that
this algorithm is invariant to orthogonal transformations and translations
of the coordinate system; thus translating or reflections of the coordinate
system result in the same boxes being selected by the algorithm.
The output of this operation is the final boxes, scores and classes tensor
returned after performing non_max_suppression.

Args:
  boxes: A `Tensor` of type `float32`.
    A 4-D float tensor of shape `[batch_size, num_boxes, q, 4]`. If `q` is 1 then
    same boxes are used for all classes otherwise, if `q` is equal to number of
    classes, class-specific boxes are used.
  scores: A `Tensor` of type `float32`.
    A 3-D float tensor of shape `[batch_size, num_boxes, num_classes]`
    representing a single score corresponding to each box (each row of boxes).
  max_output_size_per_class: A `Tensor` of type `int32`.
    A scalar integer tensor representing the maximum number of
    boxes to be selected by non max suppression per class
  max_total_size: A `Tensor` of type `int32`.
    An int32 scalar representing the maximum number of boxes retained over all
    classes. Note that setting this value to a large number may result in OOM error
    depending on the system workload.
  iou_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding whether
    boxes overlap too much with respect to IOU.
  score_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding when to remove
    boxes based on score.
  pad_per_class: An optional `bool`. Defaults to `False`.
    If false, the output nmsed boxes, scores and classes
    are padded/clipped to `max_total_size`. If true, the
    output nmsed boxes, scores and classes are padded to be of length
    `max_size_per_class`*`num_classes`, unless it exceeds `max_total_size` in
    which case it is clipped to `max_total_size`. Defaults to false.
  clip_boxes: An optional `bool`. Defaults to `True`.
    If true, assume the box coordinates are between [0, 1] and clip the output boxes
    if they fall beyond [0, 1]. If false, do not do clipping and output the box
    coordinates as it is.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections).

  nmsed_boxes: A `Tensor` of type `float32`.
  nmsed_scores: A `Tensor` of type `float32`.
  nmsed_classes: A `Tensor` of type `float32`.
  valid_detections: A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CombinedNonMaxSuppression`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CombinedNonMaxSuppression` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            Greedily selects a subset of bounding boxes in descending order of score,

This operation performs non_max_suppression on the inputs per batch, across
all classes.
Prunes away boxes that have high intersection-over-union (IOU) overlap
with previously selected boxes.  Bounding boxes are supplied as
[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any
diagonal pair of box corners and the coordinates can be provided as normalized
(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm
is agnostic to where the origin is in the coordinate system. Also note that
this algorithm is invariant to orthogonal transformations and translations
of the coordinate system; thus translating or reflections of the coordinate
system result in the same boxes being selected by the algorithm.
The output of this operation is the final boxes, scores and classes tensor
returned after performing non_max_suppression.

Args:
  boxes: A `Tensor` of type `float32`.
    A 4-D float tensor of shape `[batch_size, num_boxes, q, 4]`. If `q` is 1 then
    same boxes are used for all classes otherwise, if `q` is equal to number of
    classes, class-specific boxes are used.
  scores: A `Tensor` of type `float32`.
    A 3-D float tensor of shape `[batch_size, num_boxes, num_classes]`
    representing a single score corresponding to each box (each row of boxes).
  max_output_size_per_class: A `Tensor` of type `int32`.
    A scalar integer tensor representing the maximum number of
    boxes to be selected by non max suppression per class
  max_total_size: A `Tensor` of type `int32`.
    An int32 scalar representing the maximum number of boxes retained over all
    classes. Note that setting this value to a large number may result in OOM error
    depending on the system workload.
  iou_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding whether
    boxes overlap too much with respect to IOU.
  score_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding when to remove
    boxes based on score.
  pad_per_class: An optional `bool`. Defaults to `False`.
    If false, the output nmsed boxes, scores and classes
    are padded/clipped to `max_total_size`. If true, the
    output nmsed boxes, scores and classes are padded to be of length
    `max_size_per_class`*`num_classes`, unless it exceeds `max_total_size` in
    which case it is clipped to `max_total_size`. Defaults to false.
  clip_boxes: An optional `bool`. Defaults to `True`.
    If true, assume the box coordinates are between [0, 1] and clip the output boxes
    if they fall beyond [0, 1]. If false, do not do clipping and output the box
    coordinates as it is.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections).

  nmsed_boxes: A `Tensor` of type `float32`.
  nmsed_scores: A `Tensor` of type `float32`.
  nmsed_classes: A `Tensor` of type `float32`.
  valid_detections: A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CombinedNonMaxSuppression`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CombinedNonMaxSuppression` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.CombinedNonMaxSuppression/fuzz.cpp
[GEN] tf.raw_ops.CombinedNonMaxSuppression -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CombinedNonMaxSuppression`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CombinedNonMaxSuppression` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.CombinedNonMaxSuppression`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.CombinedNonMaxSuppression` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.CombinedNonMaxSuppression/fuzz.cpp
[GEN] tf.raw_ops.CombinedNonMaxSuppression -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            Greedily selects a subset of bounding boxes in descending order of score,

This operation performs non_max_suppression on the inputs per batch, across
all classes.
Prunes away boxes that have high intersection-over-union (IOU) overlap
with previously selected boxes.  Bounding boxes are supplied as
[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any
diagonal pair of box corners and the coordinates can be provided as normalized
(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm
is agnostic to where the origin is in the coordinate system. Also note that
this algorithm is invariant to orthogonal transformations and translations
of the coordinate system; thus translating or reflections of the coordinate
system result in the same boxes being selected by the algorithm.
The output of this operation is the final boxes, scores and classes tensor
returned after performing non_max_suppression.

Args:
  boxes: A `Tensor` of type `float32`.
    A 4-D float tensor of shape `[batch_size, num_boxes, q, 4]`. If `q` is 1 then
    same boxes are used for all classes otherwise, if `q` is equal to number of
    classes, class-specific boxes are used.
  scores: A `Tensor` of type `float32`.
    A 3-D float tensor of shape `[batch_size, num_boxes, num_classes]`
    representing a single score corresponding to each box (each row of boxes).
  max_output_size_per_class: A `Tensor` of type `int32`.
    A scalar integer tensor representing the maximum number of
    boxes to be selected by non max suppression per class
  max_total_size: A `Tensor` of type `int32`.
    An int32 scalar representing the maximum number of boxes retained over all
    classes. Note that setting this value to a large number may result in OOM error
    depending on the system workload.
  iou_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding whether
    boxes overlap too much with respect to IOU.
  score_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding when to remove
    boxes based on score.
  pad_per_class: An optional `bool`. Defaults to `False`.
    If false, the output nmsed boxes, scores and classes
    are padded/clipped to `max_total_size`. If true, the
    output nmsed boxes, scores and classes are padded to be of length
    `max_size_per_class`*`num_classes`, unless it exceeds `max_total_size` in
    which case it is clipped to `max_total_size`. Defaults to false.
  clip_boxes: An optional `bool`. Defaults to `True`.
    If true, assume the box coordinates are between [0, 1] and clip the output boxes
    if they fall beyond [0, 1]. If false, do not do clipping and output the box
    coordinates as it is.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections).

  nmsed_boxes: A `Tensor` of type `float32`.
  nmsed_scores: A `Tensor` of type `float32`.
  nmsed_classes: A `Tensor` of type `float32`.
  valid_detections: A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            Greedily selects a subset of bounding boxes in descending order of score,

This operation performs non_max_suppression on the inputs per batch, across
all classes.
Prunes away boxes that have high intersection-over-union (IOU) overlap
with previously selected boxes.  Bounding boxes are supplied as
[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any
diagonal pair of box corners and the coordinates can be provided as normalized
(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm
is agnostic to where the origin is in the coordinate system. Also note that
this algorithm is invariant to orthogonal transformations and translations
of the coordinate system; thus translating or reflections of the coordinate
system result in the same boxes being selected by the algorithm.
The output of this operation is the final boxes, scores and classes tensor
returned after performing non_max_suppression.

Args:
  boxes: A `Tensor` of type `float32`.
    A 4-D float tensor of shape `[batch_size, num_boxes, q, 4]`. If `q` is 1 then
    same boxes are used for all classes otherwise, if `q` is equal to number of
    classes, class-specific boxes are used.
  scores: A `Tensor` of type `float32`.
    A 3-D float tensor of shape `[batch_size, num_boxes, num_classes]`
    representing a single score corresponding to each box (each row of boxes).
  max_output_size_per_class: A `Tensor` of type `int32`.
    A scalar integer tensor representing the maximum number of
    boxes to be selected by non max suppression per class
  max_total_size: A `Tensor` of type `int32`.
    An int32 scalar representing the maximum number of boxes retained over all
    classes. Note that setting this value to a large number may result in OOM error
    depending on the system workload.
  iou_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding whether
    boxes overlap too much with respect to IOU.
  score_threshold: A `Tensor` of type `float32`.
    A 0-D float tensor representing the threshold for deciding when to remove
    boxes based on score.
  pad_per_class: An optional `bool`. Defaults to `False`.
    If false, the output nmsed boxes, scores and classes
    are padded/clipped to `max_total_size`. If true, the
    output nmsed boxes, scores and classes are padded to be of length
    `max_size_per_class`*`num_classes`, unless it exceeds `max_total_size` in
    which case it is clipped to `max_total_size`. Defaults to false.
  clip_boxes: An optional `bool`. Defaults to `True`.
    If true, assume the box coordinates are between [0, 1] and clip the output boxes
    if they fall beyond [0, 1]. If false, do not do clipping and output the box
    coordinates as it is.
  name: A name for the operation (optional).

Returns:
  A tuple of `Tensor` objects (nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections).

  nmsed_boxes: A `Tensor` of type `float32`.
  nmsed_scores: A `Tensor` of type `float32`.
  nmsed_classes: A `Tensor` of type `float32`.
  valid_detections: A `Tensor` of type `int32`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.CombinedNonMaxSuppression/fuzz.cpp
[GEN] tf.raw_ops.CombinedNonMaxSuppression -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.CombinedNonMaxSuppression`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.CombinedNonMaxSuppression functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.CombinedNonMaxSuppression/fuzz.cpp
[GEN] tf.raw_ops.Abort -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            Raise a exception to abort the process when called.

If exit_without_error is true, the process will exit normally,
otherwise it will exit with a SIGABORT signal.

Returns nothing but an exception.

Args:
  error_msg: An optional `string`. Defaults to `""`.
    A string which is the message associated with the exception.
  exit_without_error: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Abort`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Abort` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            Raise a exception to abort the process when called.

If exit_without_error is true, the process will exit normally,
otherwise it will exit with a SIGABORT signal.

Returns nothing but an exception.

Args:
  error_msg: An optional `string`. Defaults to `""`.
    A string which is the message associated with the exception.
  exit_without_error: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Abort`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Abort` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.Abort/fuzz.cpp
[GEN] tf.raw_ops.Abort -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Abort`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Abort` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.Abort`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.Abort` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.Abort/fuzz.cpp
[GEN] tf.raw_ops.Abort -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            Raise a exception to abort the process when called.

If exit_without_error is true, the process will exit normally,
otherwise it will exit with a SIGABORT signal.

Returns nothing but an exception.

Args:
  error_msg: An optional `string`. Defaults to `""`.
    A string which is the message associated with the exception.
  exit_without_error: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            Raise a exception to abort the process when called.

If exit_without_error is true, the process will exit normally,
otherwise it will exit with a SIGABORT signal.

Returns nothing but an exception.

Args:
  error_msg: An optional `string`. Defaults to `""`.
    A string which is the message associated with the exception.
  exit_without_error: An optional `bool`. Defaults to `False`.
  name: A name for the operation (optional).

Returns:
  The created Operation.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.Abort/fuzz.cpp
[GEN] tf.raw_ops.Abort -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.Abort`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.Abort functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.Abort/fuzz.cpp
[GEN] tf.raw_ops.DebugGradientRefIdentity -> original

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            Identity op for gradient debugging.

This op is hidden from public in Python. It is used by TensorFlow Debugger to
register gradient tensors for gradient debugging.
This op operates on reference-type tensors.

Args:
  input: A mutable `Tensor`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DebugGradientRefIdentity`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DebugGradientRefIdentity` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            Identity op for gradient debugging.

This op is hidden from public in Python. It is used by TensorFlow Debugger to
register gradient tensors for gradient debugging.
This op operates on reference-type tensors.

Args:
  input: A mutable `Tensor`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DebugGradientRefIdentity`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DebugGradientRefIdentity` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./original/tf.raw_ops.DebugGradientRefIdentity/fuzz.cpp
[GEN] tf.raw_ops.DebugGradientRefIdentity -> no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DebugGradientRefIdentity`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DebugGradientRefIdentity` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: Note:
1. If you want to use
```cpp
template <typename T>
void fillTensorWithData(tensorflow::Tensor& tensor, const uint8_t* data,
                        size_t& offset, size_t total_size) {
  auto flat = tensor.flat<T>();
  const size_t num_elements = flat.size();
  const size_t element_size = sizeof(T);

  for (size_t i = 0; i < num_elements; ++i) {
    if (offset + element_size <= total_size) {
      T value;
      std::memcpy(&value, data + offset, element_size);
      offset += element_size;
      flat(i) = value;
    } else {
      flat(i) = T{};
    }
  }
}

void fillTensorWithDataByType(tensorflow::Tensor& tensor,
                              tensorflow::DataType dtype, const uint8_t* data,
                              size_t& offset, size_t total_size) {
  switch (dtype) {
    case tensorflow::DT_FLOAT:
      fillTensorWithData<float>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_DOUBLE:
      fillTensorWithData<double>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT32:
      fillTensorWithData<int32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT8:
      fillTensorWithData<uint8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT16:
      fillTensorWithData<int16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT8:
      fillTensorWithData<int8_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_INT64:
      fillTensorWithData<int64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BOOL:
      fillTensorWithData<bool>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT16:
      fillTensorWithData<uint16_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT32:
      fillTensorWithData<uint32_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_UINT64:
      fillTensorWithData<uint64_t>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_BFLOAT16:
      fillTensorWithData<tensorflow::bfloat16>(tensor, data, offset,
                                               total_size);
      break;
    case tensorflow::DT_HALF:
      fillTensorWithData<Eigen::half>(tensor, data, offset, total_size);
      break;
    case tensorflow::DT_COMPLEX64:
      fillTensorWithData<tensorflow::complex64>(tensor, data, offset,
                                                total_size);
      break;
    case tensorflow::DT_COMPLEX128:
      fillTensorWithData<tensorflow::complex128>(tensor, data, offset,
                                                 total_size);
      break;

    default:
      return 0;
  }
}
```
to fill the tensor, make sure that you provide the correct data type for each datatype instead of using float for most. For example, if you have a tensor of type bfloat16, you should use `bfloat16` instead of `float`. Or, tensor.flat() will crash. And for special data types like `DT_STRING`, you need to treat them separately. 

2. when fill tensor with data, after memory copy, prevent overflow.
3. Try to print the tensor and everything you feed into the api.
4. Do not forget to add namespace tensorflow:: to some functions.
    
    
    
    **Instructions for `fuzz.cpp` content:**

    1.  **Complete `LLVMFuzzerTestOneInput`:** Implement the `LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)` function.
    2.  **Tensor Creation:**
        * Parse data type. Strictly follow the data type specified in the document. For example:
        ```cpp
            tensorflow::DataType parseDataType(uint8_t selector) {
    // You need to abandon some data types based on the document to avoid invalid inputs
  tensorflow::DataType dtype; 
  switch (selector % 23) {  
    case 0:
      dtype = DT_FLOAT;
      break;
    case 1:
      dtype = DT_DOUBLE;
      break;
    case 2:
      dtype = DT_INT32;
      break;
    case 3:
      dtype = DT_UINT8;
      break;
    case 4:
      dtype = DT_INT16;
      break;
    case 5:
      dtype = DT_INT8;
      break;
    case 6:
      dtype = DT_STRING;
      break;
    case 7:
      dtype = DT_COMPLEX64; // float_complex
      break;
    case 8:
      dtype = DT_INT64;
      break;
    case 9:
      dtype = DT_BOOL;
      break;
    case 10:
      dtype = DT_QINT8; // quantized_int8
      break;
    case 11:
      dtype = DT_QUINT8; // quantized_uint8
      break;
    case 12:
      dtype = DT_QINT32; // quantized_int32
      break;
    case 13:
      dtype = DT_BFLOAT16; // bfloat16
      break;
    case 14:
      dtype = DT_QINT16; // quantized_int16
      break;
    case 15:
      dtype = DT_QUINT16; // quantized_uint16
      break;
    case 16:
      dtype = DT_UINT16;
      break;
    case 17:
      dtype = DT_COMPLEX128; // double_complex
      break;
    case 18:
      dtype = DT_HALF; // float16
      break;
    case 19:
      dtype = DT_UINT32;
      break;
    case 20:
      dtype = DT_UINT64;
      break;
  }

  return dtype;
}
        ```
        * determine the rank
        ```cpp
            uint8_t parseRank(uint8_t byte)
{
    constexpr uint8_t range = MAX_RANK - MIN_RANK + 1;
    uint8_t rank = byte % range + MIN_RANK;
    return rank;
}
        ```
        * determine the shape
        ```cpp
            std::vector<int64_t> parseShape(const uint8_t* data, size_t& offset, size_t total_size, uint8_t rank) {
    if (rank == 0) {
        return {};
    }

    std::vector<int64_t> shape;
    shape.reserve(rank);
    const auto sizeof_dim = sizeof(int64_t);

    for (uint8_t i = 0; i < rank; ++i) {
        if (offset + sizeof_dim <= total_size) {
            int64_t dim_val;
            std::memcpy(&dim_val, data + offset, sizeof_dim);
            offset += sizeof_dim;
            
            dim_val = MIN_TENSOR_SHAPE_DIMS_TF +
                    static_cast<int64_t>((static_cast<uint64_t>(std::abs(dim_val)) %
                                        static_cast<uint64_t>(MAX_TENSOR_SHAPE_DIMS_TF - MIN_TENSOR_SHAPE_DIMS_TF + 1)));

            shape.push_back(dim_val);
        } else {
             shape.push_back(1);
        }
    }

    return shape;
}
        ```

        * Create input tensor(s) for the `tf.raw_ops.DebugGradientRefIdentity`. 
    3.  **Operation Application:** Apply the `tf.raw_ops.DebugGradientRefIdentity` operation to the created tensor(s).
    4.  **Self-Contained `fuzz.cpp`:** The generated code should be the complete content for `fuzz.cpp`.
    5.  **No Comments:** Remove ALL C-style (`/* ... */`, `// ...`) and C++-style comments.




    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_doc/tf.raw_ops.DebugGradientRefIdentity/fuzz.cpp
[GEN] tf.raw_ops.DebugGradientRefIdentity -> no_helper

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            Identity op for gradient debugging.

This op is hidden from public in Python. It is used by TensorFlow Debugger to
register gradient tensors for gradient debugging.
This op operates on reference-type tensors.

Args:
  input: A mutable `Tensor`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            Identity op for gradient debugging.

This op is hidden from public in Python. It is used by TensorFlow Debugger to
register gradient tensors for gradient debugging.
This op operates on reference-type tensors.

Args:
  input: A mutable `Tensor`.
  name: A name for the operation (optional).

Returns:
  A mutable `Tensor`. Has the same type as `input`.

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper/tf.raw_ops.DebugGradientRefIdentity/fuzz.cpp
[GEN] tf.raw_ops.DebugGradientRefIdentity -> no_helper_no_doc

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    

    I need to write a C++ testharness for the TensorFlow C++ frontend operation `tf.raw_ops.DebugGradientRefIdentity`.

    The document of the API is as follows:
            

    The testharness will be compiled with libFuzzer.
    Your primary goal is to generate C++ code for the `fuzz.cpp` file.

    Please complete the implementation with C++ code that properly tests the tf.raw_ops.DebugGradientRefIdentity functionality. Answer with 

    ```cpp
    ```

    and only the full C++ code that can be compiled. Do not include any other text or explanations.

    Note: 
    
    
    
    
        Here is the skeleton of the file:

    ```fuzz.cpp
    #include <cstdint>
#include <iostream>
#include <cstring>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <tensorflow/core/framework/types.h>
extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
    try {
        size_t offset = 0;
    } catch (const std::exception& e) {
        // print Exception to stderr, do not remove this
        std::cout << "Exception caught: " << e.what() << std::endl;
        return -1;
    }
    return 0;
}

    ```
    
[OK] Wrote ./no_helper_no_doc/tf.raw_ops.DebugGradientRefIdentity/fuzz.cpp
